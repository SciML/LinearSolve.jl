var documenterSearchIndex = {"docs":
[{"location":"basics/CachingAPI/#Caching-Interface-API-Functions","page":"Caching Interface API Functions","title":"Caching Interface API Functions","text":"","category":"section"},{"location":"basics/CachingAPI/","page":"Caching Interface API Functions","title":"Caching Interface API Functions","text":"LinearSolve.set_A\r\nLinearSolve.set_b\r\nLinearSolve.set_u\r\nLinearSolve.set_p\r\nLinearSolve.set_prec","category":"page"},{"location":"basics/CachingAPI/#LinearSolve.set_A","page":"Caching Interface API Functions","title":"LinearSolve.set_A","text":"set_A(cache, A)\n\n\n\n\n\n\n","category":"function"},{"location":"basics/CachingAPI/#LinearSolve.set_b","page":"Caching Interface API Functions","title":"LinearSolve.set_b","text":"set_b(cache, b)\n\n\n\n\n\n\n","category":"function"},{"location":"basics/CachingAPI/#LinearSolve.set_u","page":"Caching Interface API Functions","title":"LinearSolve.set_u","text":"set_u(cache, u)\n\n\n\n\n\n\n","category":"function"},{"location":"basics/CachingAPI/#LinearSolve.set_p","page":"Caching Interface API Functions","title":"LinearSolve.set_p","text":"set_p(cache, p)\n\n\n\n\n\n\n","category":"function"},{"location":"basics/CachingAPI/#LinearSolve.set_prec","page":"Caching Interface API Functions","title":"LinearSolve.set_prec","text":"set_prec(cache, Pl, Pr)\n\n\n\n\n\n\n","category":"function"},{"location":"basics/LinearProblem/#Linear-Problems","page":"Linear Problems","title":"Linear Problems","text":"","category":"section"},{"location":"basics/LinearProblem/","page":"Linear Problems","title":"Linear Problems","text":"LinearProblem","category":"page"},{"location":"basics/LinearProblem/#SciMLBase.LinearProblem","page":"Linear Problems","title":"SciMLBase.LinearProblem","text":"Defines a linear system problem. Documentation Page: https://docs.sciml.ai/LinearSolve/stable/basics/LinearProblem/\n\nMathematical Specification of a Linear Problem\n\nConcrete LinearProblem\n\nTo define a LinearProblem, you simply need to give the AbstractMatrix A and an AbstractVector b which defines the linear system:\n\nAu = b\n\nMatrix-Free LinearProblem\n\nFor matrix-free versions, the specification of the problem is given by an operator A(u,p,t) which computes A*u, or in-place as A(du,u,p,t). These are specified via the AbstractSciMLOperator interface. For more details, see the SciMLBase Documentation.\n\nNote that matrix-free versions of LinearProblem definitions are not compatible with all solvers. To check a solver for compatibility, use the function xxxxx.\n\nProblem Type\n\nConstructors\n\nOptionally, an initial guess u₀ can be supplied which is used for iterative methods.\n\nLinearProblem{isinplace}(A,x,p=NullParameters();u0=nothing,kwargs...)\nLinearProblem(f::AbstractDiffEqOperator,u0,p=NullParameters();u0=nothing,kwargs...)\n\nisinplace optionally sets whether the function is in-place or not, i.e. whether the solvers are allowed to mutate. By default this is true for AbstractMatrix, and for AbstractSciMLOperators it matches the choice of the operator definition.\n\nParameters are optional, and if not given, then a NullParameters() singleton will be used, which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers.\n\nFields\n\nA: The representation of the linear operator.\nb: The right-hand side of the linear system.\np: The parameters for the problem. Defaults to NullParameters. Currently unused.\nu0: The initial condition used by iterative solvers.\nkwargs: The keyword arguments passed on to the solvers.\n\n\n\n","category":"type"},{"location":"advanced/custom/#Passing-in-a-Custom-Linear-Solver","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"","category":"section"},{"location":"advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"Julia users are building a wide variety of applications in the SciML ecosystem, often requiring problem-specific handling of their linear solves. As existing solvers in LinearSolve.jl may not be optimally suited for novel applications, it is essential for the linear solve interface to be easily extendable by users. To that end, the linear solve algorithm LinearSolveFunction() accepts a user-defined function for handling the solve. A user can pass in their custom linear solve function, say my_linsolve, to LinearSolveFunction(). A contrived example of solving a linear system with a custom solver is below.","category":"page"},{"location":"advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"using LinearSolve, LinearAlgebra\n\nfunction my_linsolve(A,b,u,p,newA,Pl,Pr,solverdata;verbose=true, kwargs...)\n    if verbose == true\n        println(\"solving Ax=b\")\n    end\n    u = A \\ b\n    return u\nend\n\nprob = LinearProblem(Diagonal(rand(4)), rand(4))\nalg  = LinearSolveFunction(my_linsolve)\nsol  = solve(prob, alg)\nsol.u","category":"page"},{"location":"advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"The inputs to the function are as follows:","category":"page"},{"location":"advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"A, the linear operator\nb, the right-hand-side\nu, the solution initialized as zero(b),\np, a set of parameters\nnewA, a Bool which is true if A has been modified since last solve\nPl, left-preconditioner\nPr, right-preconditioner\nsolverdata, solver cache set to nothing if solver hasn't been initialized\nkwargs, standard SciML keyword arguments such as verbose, maxiters, abstol, reltol","category":"page"},{"location":"advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"The function my_linsolve must accept the above specified arguments, and return the solution, u. As memory for u is already allocated, the user may choose to modify u in place as follows:","category":"page"},{"location":"advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"function my_linsolve!(A,b,u,p,newA,Pl,Pr,solverdata;verbose=true, kwargs...)\n    if verbose == true\n        println(\"solving Ax=b\")\n    end\n    u .= A \\ b # in place\n    return u\nend\n\nalg  = LinearSolveFunction(my_linsolve!)\nsol  = solve(prob, alg)\nsol.u","category":"page"},{"location":"basics/FAQ/#Frequently-Asked-Questions","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"","category":"section"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Ask more questions.","category":"page"},{"location":"basics/FAQ/#How-is-LinearSolve.jl-compared-to-just-using-normal-\\,-i.e.-A\\b?","page":"Frequently Asked Questions","title":"How is LinearSolve.jl compared to just using normal \\, i.e. A\\b?","text":"","category":"section"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Check out this video from JuliaCon 2022 which goes into detail on how and why LinearSolve.jl is able to be a more general and efficient interface.","category":"page"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Note that if \\ is good enough for you, great! We still tend to use \\ in the REPL all of the time! However, if you're building a package, you may want to consider using LinearSolve.jl for the improved efficiency and ability to choose solvers.","category":"page"},{"location":"basics/FAQ/#Python's-NumPy/SciPy-just-calls-fast-Fortran/C-code,-why-would-LinearSolve.jl-be-any-better?","page":"Frequently Asked Questions","title":"Python's NumPy/SciPy just calls fast Fortran/C code, why would LinearSolve.jl be any better?","text":"","category":"section"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"This is addressed in the JuliaCon 2022 video. This happens in a few ways:","category":"page"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"The Fortran/C code that NumPy/SciPy uses is actually slow. It's OpenBLAS, a library developed in part by the Julia Lab back in 2012 as a fast open source BLAS implementation. Many open source environments now use this build, including many R distributions. However, the Julia Lab has greatly improved its ability to generate optimized SIMD in platform-specific ways. This, and improved multithreading support (OpenBLAS's multithreading is rather slow), has led to pure Julia-based BLAS implementations which the lab now works on. This includes RecursiveFactorization.jl which generally outperforms OpenBLAS by 2x-10x depending on the platform. It even outperforms MKL for small matrices (<100). LinearSolve.jl uses RecursiveFactorization.jl by default sometimes, but switches to BLAS when it would be faster (in a platform and matrix-specific way).\nStandard approaches to handling linear solves re-allocate the pivoting vector each time. This leads to GC pauses that can slow down calculations. LinearSolve.jl has proper caches for fully preallocated no-GC workflows.\nLinearSolve.jl makes a lot of other optimizations, like factorization reuse and symbolic factorization reuse, automatic. Many of these optimizations are not even possible from the high-level APIs of things like Python's major libraries and MATLAB.\nLinearSolve.jl has a much more extensive set of sparse matrix solvers, which is why you see a major difference (2x-10x) for sparse matrices. Which sparse matrix solver between KLU, UMFPACK, Pardiso, etc. is optimal depends a lot on matrix sizes, sparsity patterns, and threading overheads. LinearSolve.jl's heuristics handle these kinds of issues.","category":"page"},{"location":"basics/FAQ/#How-do-I-use-IterativeSolvers-solvers-with-a-weighted-tolerance-vector?","page":"Frequently Asked Questions","title":"How do I use IterativeSolvers solvers with a weighted tolerance vector?","text":"","category":"section"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"IterativeSolvers.jl computes the norm after the application of the left precondtioner Pl. Thus in order to use a vector tolerance weights, one can mathematically hack the system via the following formulation:","category":"page"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"using LinearSolve, LinearAlgebra\n\nn = 2\nA = rand(n,n)\nb = rand(n)\n\nweights = [1e-1, 1]\nPl = LinearSolve.InvPreconditioner(Diagonal(weights))\nPr = Diagonal(weights)\n\n\nprob = LinearProblem(A,b)\nsol = solve(prob,IterativeSolversJL_GMRES(),Pl=Pl,Pr=Pr)\n\nsol.u","category":"page"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"If you want to use a \"real\" preconditioner under the norm weights, then one can use ComposePreconditioner to apply the preconditioner after the application of the weights like as follows:","category":"page"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"using LinearSolve, LinearAlgebra\n\nn = 4\nA = rand(n,n)\nb = rand(n)\n\nweights = rand(n)\nrealprec = lu(rand(n,n)) # some random preconditioner\nPl = LinearSolve.ComposePreconditioner(LinearSolve.InvPreconditioner(Diagonal(weights)),realprec)\nPr = Diagonal(weights)\n\nprob = LinearProblem(A,b)\nsol = solve(prob,IterativeSolversJL_GMRES(),Pl=Pl,Pr=Pr)","category":"page"},{"location":"advanced/developing/#Developing-New-Linear-Solvers","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"","category":"section"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"Developing new or custom linear solvers for the SciML interface can be done in one of two ways:","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"You can either create a completely new set of dispatches for init and solve.\nYou can extend LinearSolve.jl's internal mechanisms.","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"For developer ease, we highly recommend (2) as that will automatically make the caching API work. Thus this is the documentation for how to do that.","category":"page"},{"location":"advanced/developing/#Developing-New-Linear-Solvers-with-LinearSolve.jl-Primitives","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers with LinearSolve.jl Primitives","text":"","category":"section"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"Let's create a new wrapper for a simple LU-factorization which uses only the basic machinery. A simplified version is:","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"struct MyLUFactorization{P} <: SciMLBase.AbstractLinearAlgorithm end\r\n\r\ninit_cacheval(alg::MyLUFactorization, A, b, u, Pl, Pr, maxiters, abstol, reltol, verbose) = lu!(convert(AbstractMatrix,A))\r\n\r\nfunction SciMLBase.solve(cache::LinearCache, alg::MyLUFactorization; kwargs...)\r\n    if cache.isfresh\r\n        A = convert(AbstractMatrix,A)\r\n        fact = lu!(A)\r\n        cache = set_cacheval(cache, fact)\r\n    end\r\n    y = ldiv!(cache.u, cache.cacheval, cache.b)\r\n    SciMLBase.build_linear_solution(alg,y,nothing,cache)\r\nend","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"The way this works is as follows. LinearSolve.jl has a LinearCache that everything shares (this is what gives most of the ease of use). However, many algorithms need to cache their own things, and so there's one value cacheval that is for the algorithms to modify. The function:","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"init_cacheval(alg::MyLUFactorization, A, b, u, Pl, Pr, maxiters, abstol, reltol, verbose)","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"is what is called at init time to create the first cacheval. Note that this should match the type of the cache later used in solve as many algorithms, like those in OrdinaryDiffEq.jl, expect type-groundedness in the linear solver definitions. While there are cheaper ways to obtain this type for LU factorizations (specifically, ArrayInterfaceCore.lu_instance(A)), for a demonstration this just performs an LU-factorization to get an LU{T, Matrix{T}} which it puts into the cacheval so its typed for future use.","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"After the init_cacheval, the only thing left to do is to define SciMLBase.solve(cache::LinearCache, alg::MyLUFactorization). Many algorithms may use a lazy matrix-free representation of the operator A. Thus if the algorithm requires a concrete matrix, like LU-factorization does, the algorithm should convert(AbstractMatrix,cache.A). The flag cache.isfresh states whether A has changed since the last solve. Since we only need to factorize when A is new, the factorization part of the algorithm is done in a if cache.isfresh. cache = set_cacheval(cache, fact) puts the new factorization into the cache so it's updated for future solves. Then y = ldiv!(cache.u, cache.cacheval, cache.b) performs the solve and a linear solution is returned via SciMLBase.build_linear_solution(alg,y,nothing,cache).","category":"page"},{"location":"basics/Preconditioners/#prec","page":"Preconditioners","title":"Preconditioners","text":"","category":"section"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"Many linear solvers can be accelerated by using what is known as a preconditioner, an approximation to the matrix inverse action which is cheap to evaluate. These can improve the numerical conditioning of the solver process and in turn improve the performance. LinearSolve.jl provides an interface for the definition of preconditioners which works with the wrapped packages.","category":"page"},{"location":"basics/Preconditioners/#Using-Preconditioners","page":"Preconditioners","title":"Using Preconditioners","text":"","category":"section"},{"location":"basics/Preconditioners/#Mathematical-Definition","page":"Preconditioners","title":"Mathematical Definition","text":"","category":"section"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"Preconditioners are specified in the keyword arguments to init or solve: Pl for left and Pr for right preconditioner, respectively. The right preconditioner, P_r transforms the linear system Au = b into the form:","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"AP_r^-1(P_r u) = AP_r^-1y = b","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"which is solved for y, and then P_r u = y is solved for u. The left preconditioner, P_l, transforms the linear system into the form:","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"P_l^-1(Au - b) = 0","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"A two-sided preconditioned system is of the form:","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"P_l A P_r^-1 (P_r u) = P_l b","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"By default, if no preconditioner is given the preconditioner is assumed to be the identity I.","category":"page"},{"location":"basics/Preconditioners/#Using-Preconditioners-2","page":"Preconditioners","title":"Using Preconditioners","text":"","category":"section"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"In the following, we will use the DiagonalPreconditioner to define a two-sided preconditioned system which first divides by some random numbers and then multiplies by the same values. This is commonly used in the case where if, instead of random, s is an approximation to the eigenvalues of a system.","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"using LinearSolve, LinearAlgebra\r\nn = 4\r\ns = rand(n)\r\nPl = Diagonal(s)\r\n\r\nA = rand(n,n)\r\nb = rand(n)\r\n\r\nprob = LinearProblem(A,b)\r\nsol = solve(prob,IterativeSolversJL_GMRES(),Pl=Pl)\r\nsol.u","category":"page"},{"location":"basics/Preconditioners/#Preconditioner-Interface","page":"Preconditioners","title":"Preconditioner Interface","text":"","category":"section"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"To define a new preconditioner you define a Julia type which satisfies the following interface:","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"Base.eltype(::Preconditioner) (Required only for Krylov.jl)\nLinearAlgebra.ldiv!(::AbstractVector,::Preconditioner,::AbstractVector) and LinearAlgebra.ldiv!(::Preconditioner,::AbstractVector)","category":"page"},{"location":"basics/Preconditioners/#Curated-List-of-Pre-Defined-Preconditioners","page":"Preconditioners","title":"Curated List of Pre-Defined Preconditioners","text":"","category":"section"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"The following preconditioners match the interface of LinearSolve.jl.","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"LinearSolve.ComposePreconditioner(prec1,prec2): composes the preconditioners to apply prec1 before prec2.\nLinearSolve.InvPreconditioner(prec): inverts mul! and ldiv! in a preconditioner definition as a lazy inverse.\nLinearAlgera.Diagonal(s::Union{Number,AbstractVector}): the lazy Diagonal matrix type of Base.LinearAlgebra. Used for efficient construction of a diagonal preconditioner.\nOther Base.LinearAlgera types: all define the full Preconditioner interface.\nIncompleteLU.ilu: an implementation of the incomplete LU-factorization preconditioner. This requires A as a SparseMatrixCSC.\nPreconditioners.CholeskyPreconditioner(A, i): An incomplete Cholesky preconditioner with cut-off level i. Requires A as a AbstractMatrix and positive semi-definite.\nAlgebraicMultiGrid: Implementations of the algebraic multigrid method. Must be converted to a preconditioner via AlgebraicMultiGrid.aspreconditioner(AlgebraicMultiGrid.precmethod(A)). Requires A as a AbstractMatrix. Provides the following methods:\nAlgebraicMultiGrid.ruge_stuben(A)\nAlgebraicMultiGrid.smoothed_aggregation(A)\nPyAMG: Implementations of the algebraic multigrid method. Must be converted to a preconditioner via PyAMG.aspreconditioner(PyAMG.precmethod(A)). Requires A as a AbstractMatrix. Provides the following methods:\nPyAMG.RugeStubenSolver(A)\nPyAMG.SmoothedAggregationSolver(A)\nILUZero.ILU0Precon(A::SparseMatrixCSC{T,N}, b_type = T): An incomplete LU implementation. Requires A as a SparseMatrixCSC.\nLimitedLDLFactorizations.lldl: A limited-memory LDLᵀ factorization for symmetric matrices. Requires A as a SparseMatrixCSC. Applying F = lldl(A); F.D .= abs.(F.D) before usage as a preconditioner makes the preconditioner symmetric postive definite and thus is required for Krylov methods which are specialized for symmetric linear systems.\nRandomizedPreconditioners.NystromPreconditioner A randomized sketching method for positive semidefinite matrices A. Builds a preconditioner P  A + μ*I for the system (A + μ*I)x = b","category":"page"},{"location":"tutorials/linear/#Solving-Linear-Systems-in-Julia","page":"Solving Linear Systems in Julia","title":"Solving Linear Systems in Julia","text":"","category":"section"},{"location":"tutorials/linear/","page":"Solving Linear Systems in Julia","title":"Solving Linear Systems in Julia","text":"A linear system Au=b is specified by defining an AbstractMatrix A, or by providing a matrix-free operator for performing A*x operations via the function A(u,p,t) out-of-place and A(du,u,p,t) for in-place. For the sake of simplicity, this tutorial will only showcase concrete matrices.","category":"page"},{"location":"tutorials/linear/","page":"Solving Linear Systems in Julia","title":"Solving Linear Systems in Julia","text":"The following defines a matrix and a LinearProblem which is subsequently solved by the default linear solver.","category":"page"},{"location":"tutorials/linear/","page":"Solving Linear Systems in Julia","title":"Solving Linear Systems in Julia","text":"using LinearSolve\n\nA = rand(4,4)\nb = rand(4)\nprob = LinearProblem(A, b)\nsol = solve(prob)\nsol.u","category":"page"},{"location":"tutorials/linear/","page":"Solving Linear Systems in Julia","title":"Solving Linear Systems in Julia","text":"Note that solve(prob) is equivalent to solve(prob,nothing) where nothing denotes the choice of the default linear solver. This is equivalent to the Julia built-in A\\b, where the solution is recovered via sol.u. The power of this package comes into play when changing the algorithms. For example, Krylov.jl has some nice methods like GMRES which can be faster in some cases. With LinearSolve.jl, there is one interface and changing linear solvers is simply the switch of the algorithm choice:","category":"page"},{"location":"tutorials/linear/","page":"Solving Linear Systems in Julia","title":"Solving Linear Systems in Julia","text":"sol = solve(prob,KrylovJL_GMRES())\nsol.u","category":"page"},{"location":"tutorials/linear/","page":"Solving Linear Systems in Julia","title":"Solving Linear Systems in Julia","text":"Thus a package which uses LinearSolve.jl simply needs to allow the user to pass in an algorithm struct and all wrapped linear solvers are immediately available as tweaks to the general algorithm.","category":"page"},{"location":"basics/common_solver_opts/#Common-Solver-Options-(Keyword-Arguments-for-Solve)","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"While many algorithms have specific arguments within their constructor, the keyword arguments for solve are common across all of the algorithms in order to give composability. These are also the options taken at init time. The following are the options these algorithms take, along with their defaults.","category":"page"},{"location":"basics/common_solver_opts/#General-Controls","page":"Common Solver Options (Keyword Arguments for Solve)","title":"General Controls","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"alias_A: Whether to alias the matrix A or use a copy by default. When true, algorithms like LU-factorization can be faster by reusing the memory via lu!, but care must be taken as the original input will be modified. Default is false.\nalias_b: Whether to alias the matrix b or use a copy by default. When true, algorithms can write and change b upon usage. Care must be taken as the original input will be modified. Default is false.\nverbose: Whether to print extra information. Defaults to false.","category":"page"},{"location":"basics/common_solver_opts/#Iterative-Solver-Controls","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Iterative Solver Controls","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"Error controls are not used by all algorithms. Specifically, direct solves always solve completely. Error controls only apply to iterative solvers.","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"abstol: The absolute tolerance. Defaults to √(eps(eltype(A)))\nreltol: The relative tolerance. Defaults to √(eps(eltype(A)))\nmaxiters: The number of iterations allowed. Defaults to length(prob.b)\nPl,Pr: The left and right preconditioners respectively. For more information see the Preconditioners page.","category":"page"},{"location":"solvers/solvers/#linearsystemsolvers","page":"Linear System Solvers","title":"Linear System Solvers","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"solve(prob::LinearProlem,alg;kwargs)","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Solves for Au=b in the problem defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.","category":"page"},{"location":"solvers/solvers/#Recommended-Methods","page":"Linear System Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"The default algorithm nothing is good for choosing an algorithm that will work, but one may need to change this to receive more performance or precision. If more precision is necessary, QRFactorization() and SVDFactorization() are the best choices, with SVD being the slowest but most precise.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"For efficiency, RFLUFactorization is the fastest for dense LU-factorizations. FastLUFactorization will be faster than LUFactorization which is the Base.LinearAlgebra (\\ default) implementation of LU factorization. SimpleLUFactorization will be fast on very small matrices.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"For sparse LU-factorizations, KLUFactorization if there is less structure to the sparsity pattern and UMFPACKFactorization if there is more structure. Pardiso.jl's methods are also known to be very efficient sparse linear solvers.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"As sparse matrices get larger, iterative solvers tend to get more efficient than factorization methods if a lower tolerance of the solution is required.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Krylov.jl generally outperforms IterativeSolvers.jl and KrylovKit.jl, and is compatible with CPUs and GPUs, and thus is the generally preferred form for Krylov methods.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Finally, a user can pass a custom function for handling the linear solve using LinearSolveFunction() if existing solvers are not optimally suited for their application. The interface is detailed here","category":"page"},{"location":"solvers/solvers/#Full-List-of-Methods","page":"Linear System Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"solvers/solvers/#RecursiveFactorization.jl","page":"Linear System Solvers","title":"RecursiveFactorization.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"RFLUFactorization(): a fast pure Julia LU-factorization implementation using RecursiveFactorization.jl. This is by far the fastest LU-factorization implementation, usually outperforming OpenBLAS and MKL, but currently optimized only for Base Array with Float32 or Float64.  Additional optimization for  complex matrices is in the works.","category":"page"},{"location":"solvers/solvers/#Base.LinearAlgebra","page":"Linear System Solvers","title":"Base.LinearAlgebra","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"These overloads tend to work for many array types, such as CuArrays for GPU-accelerated solving, using the overloads provided by the respective packages. Given that this can be customized per-package, details given below describe a subset of important arrays (Matrix, SparseMatrixCSC, CuMatrix, etc.)","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"LUFactorization(pivot=LinearAlgebra.RowMaximum()): Julia's built in lu.\nOn dense matrices this uses the current BLAS implementation of the user's computer which by default is OpenBLAS but will use MKL if the user does using MKL in their system.\nOn sparse matrices this will use UMFPACK from SuiteSparse. Note that this will not cache the symbolic factorization.\nOn CuMatrix it will use a CUDA-accelerated LU from CuSolver.\nOn BandedMatrix and BlockBandedMatrix it will use a banded LU.\nQRFactorization(pivot=LinearAlgebra.NoPivot(),blocksize=16): Julia's built in qr.\nOn dense matrices this uses the current BLAS implementation of the user's computer which by default is OpenBLAS but will use MKL if the user does using MKL in their system.\nOn sparse matrices this will use SPQR from SuiteSparse\nOn CuMatrix it will use a CUDA-accelerated QR from CuSolver.\nOn BandedMatrix and BlockBandedMatrix it will use a banded QR.\nSVDFactorization(full=false,alg=LinearAlgebra.DivideAndConquer()): Julia's built in svd.\nOn dense matrices this uses the current BLAS implementation of the user's computer which by default is OpenBLAS but will use MKL if the user does using MKL in their system.\nGenericFactorization(fact_alg): Constructs a linear solver from a generic factorization algorithm fact_alg which complies with the Base.LinearAlgebra factorization API. Quoting from Base:\nIf A is upper or lower triangular (or diagonal), no factorization of A is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used. For rectangular A the result is the minimum-norm least squares solution computed by a pivoted QR factorization of A and a rank estimate of A based on the R factor. When A is sparse, a similar polyalgorithm is used. For indefinite matrices, the LDLt factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.","category":"page"},{"location":"solvers/solvers/#LinearSolve.jl","page":"Linear System Solvers","title":"LinearSolve.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"LinearSolve.jl contains some linear solvers built in.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"SimpleLUFactorization: a simple LU-factorization implementation without BLAS. Fast for small matrices.","category":"page"},{"location":"solvers/solvers/#FastLapackInterface.jl","page":"Linear System Solvers","title":"FastLapackInterface.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"FastLapackInterface.jl is a package that allows for a lower-level interface to the LAPACK calls to allow for preallocating workspaces to decrease the overhead of the wrappers. LinearSolve.jl provides a wrapper to these routines in a way where an initialized solver has a non-allocating LU factorization. In theory, this post-initialized solve should always be faster than the Base.LinearAlgebra version.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"FastLUFactorization the FastLapackInterface version of the LU factorizaiton. Notably, this version does not allow for choice of pivoting method.\nFastQRFactorization(pivot=NoPivot(),blocksize=32), the FastLapackInterface version of the QR factorizaiton.","category":"page"},{"location":"solvers/solvers/#SuiteSparse.jl","page":"Linear System Solvers","title":"SuiteSparse.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"By default, the SuiteSparse.jl are implemented for efficiency by caching the symbolic factorization. I.e. if set_A is used, it is expected that the new A has the same sparsity pattern as the previous A. If this algorithm is to be used in a context where that assumption does not hold, set reuse_symbolic=false.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"KLUFactorization(;reuse_symbolic=true): A fast sparse LU-factorization which specializes on sparsity patterns with \"less structure\".\nUMFPACKFactorization(;reuse_symbolic=true): A fast sparse multithreaded LU-factorization which specializes on sparsity patterns that are more structured.","category":"page"},{"location":"solvers/solvers/#Pardiso.jl","page":"Linear System Solvers","title":"Pardiso.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing this solver requires adding the package LinearSolvePardiso.jl","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"The following algorithms are pre-specified:","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"MKLPardisoFactorize(;kwargs...): A sparse factorization method.\nMKLPardisoIterate(;kwargs...): A mixed factorization+iterative method.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Those algorithms are defined via:","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"MKLPardisoFactorize(;kwargs...) = PardisoJL(;fact_phase=Pardiso.NUM_FACT,\n                                             solve_phase=Pardiso.SOLVE_ITERATIVE_REFINE,\n                                             kwargs...)\nMKLPardisoIterate(;kwargs...) = PardisoJL(;solve_phase=Pardiso.NUM_FACT_SOLVE_REFINE,\n                                           kwargs...)","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"The full set of keyword arguments for PardisoJL are:","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Base.@kwdef struct PardisoJL <: SciMLLinearSolveAlgorithm\n    nprocs::Union{Int, Nothing} = nothing\n    solver_type::Union{Int, Pardiso.Solver, Nothing} = nothing\n    matrix_type::Union{Int, Pardiso.MatrixType, Nothing} = nothing\n    fact_phase::Union{Int, Pardiso.Phase, Nothing} = nothing\n    solve_phase::Union{Int, Pardiso.Phase, Nothing} = nothing\n    release_phase::Union{Int, Nothing} = nothing\n    iparm::Union{Vector{Tuple{Int,Int}}, Nothing} = nothing\n    dparm::Union{Vector{Tuple{Int,Int}}, Nothing} = nothing\nend","category":"page"},{"location":"solvers/solvers/#CUDA.jl","page":"Linear System Solvers","title":"CUDA.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Note that CuArrays are supported by GenericFactorization in the \"normal\" way. The following are non-standard GPU factorization routines.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing this solver requires adding the package LinearSolveCUDA.jl","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"CudaOffloadFactorization(): An offloading technique used to GPU-accelerate CPU-based computations. Requires a sufficiently large A to overcome the data transfer costs.","category":"page"},{"location":"solvers/solvers/#IterativeSolvers.jl","page":"Linear System Solvers","title":"IterativeSolvers.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"IterativeSolversJL_CG(args...;kwargs...): A generic CG implementation\nIterativeSolversJL_GMRES(args...;kwargs...): A generic GMRES implementation\nIterativeSolversJL_BICGSTAB(args...;kwargs...): A generic BICGSTAB implementation\nIterativeSolversJL_MINRES(args...;kwargs...): A generic MINRES implementation","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"The general algorithm is:","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"IterativeSolversJL(args...;\n                   generate_iterator = IterativeSolvers.gmres_iterable!,\n                   Pl=nothing, Pr=nothing,\n                   gmres_restart=0, kwargs...)","category":"page"},{"location":"solvers/solvers/#Krylov.jl","page":"Linear System Solvers","title":"Krylov.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"KrylovJL_CG(args...;kwargs...): A generic CG implementation for Hermitian and positive definite linear systems\nKrylovJL_MINRES(args...;kwargs...): A generic MINRES implementation for Hermitian linear systems\nKrylovJL_GMRES(args...;kwargs...): A generic GMRES implementation for square non-Hermitian linear systems\nKrylovJL_BICGSTAB(args...;kwargs...): A generic BICGSTAB implementation for square non-Hermitian linear systems\nKrylovJL_LSMR(args...;kwargs...): A generic LSMR implementation for least-squares problems\nKrylovJL_CRAIGMR(args...;kwargs...): A generic CRAIGMR implementation for least-norm problems","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"The general algorithm is:","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"KrylovJL(args...; KrylovAlg = Krylov.gmres!,\n                  Pl=nothing, Pr=nothing,\n                  gmres_restart=0, window=0,\n                  kwargs...)","category":"page"},{"location":"solvers/solvers/#KrylovKit.jl","page":"Linear System Solvers","title":"KrylovKit.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"KrylovKitJL_CG(args...;kwargs...): A generic CG implementation\nKrylovKitJL_GMRES(args...;kwargs...): A generic GMRES implementation","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"The general algorithm is:","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"function KrylovKitJL(args...;\n                     KrylovAlg = KrylovKit.GMRES, gmres_restart = 0,\n                     kwargs...)","category":"page"},{"location":"#LinearSolve.jl:-High-Performance-Unified-Linear-Solvers","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"","category":"section"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"LinearSolve.jl is a unified interface for the linear solving packages of Julia. It interfaces with other packages of the Julia ecosystem to make it easy to test alternative solver packages and pass small types to control algorithm swapping. It also interfaces with the ModelingToolkit.jl world of symbolic modeling to allow for automatically generating high-performance code.","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"Performance is key: the current methods are made to be highly performant on scalar and statically sized small problems, with options for large-scale systems. If you run into any performance issues, please file an issue.","category":"page"},{"location":"#Installation","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"Installation","text":"","category":"section"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"To install LinearSolve.jl, use the Julia package manager:","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"using Pkg\nPkg.add(\"LinearSolve\")","category":"page"},{"location":"#Contributing","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"Contributing","text":"","category":"section"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nSee the SciML Style Guide for common coding practices and other style decisions.\nThere are a few community forums:\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Slack\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Zulip\nOn the Julia Discourse forums\nSee also SciML Community page","category":"page"},{"location":"#Roadmap","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"Roadmap","text":"","category":"section"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"Wrappers for every linear solver in the Julia language is on the roadmap. If there are any important ones that are missing that you would like to see added, please open an issue. The current algorithms should support automatic differentiation. Pre-defined preconditioners would be a welcome addition.","category":"page"},{"location":"#Reproducibility","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"</details>","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"</details>","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"using Pkg # hide\nPkg.status(;mode = PKGMODE_MANIFEST) # hide","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"</details>","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"You can also download the \n<a href=\"","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"using TOML\nversion = TOML.parse(read(\"../../Project.toml\",String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\",String))[\"name\"]\nlink = \"https://github.com/SciML/\"*name*\".jl/tree/gh-pages/v\"*version*\"/assets/Manifest.toml\"","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"\">manifest</a> file and the\n<a href=\"","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"using TOML\nversion = TOML.parse(read(\"../../Project.toml\",String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\",String))[\"name\"]\nlink = \"https://github.com/SciML/\"*name*\".jl/tree/gh-pages/v\"*version*\"/assets/Project.toml\"","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"\">project</a> file.","category":"page"},{"location":"tutorials/caching_interface/#Linear-Solve-with-Caching-Interface","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"","category":"section"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"In many cases one may want to cache information that is reused between different linear solves. For example, if one is going to perform:","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"A\\b1\nA\\b2","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"then it would be more efficient to LU-factorize one time and reuse the factorization:","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"lu!(A)\nA\\b1\nA\\b2","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"LinearSolve.jl's caching interface automates this process to use the most efficient means of solving and resolving linear systems. To do this with LinearSolve.jl, you simply init a cache, solve, replace b, and solve again. This looks like:","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"using LinearSolve\n\nn = 4\nA = rand(n,n)\nb1 = rand(n); b2 = rand(n)\nprob = LinearProblem(A, b1)\n\nlinsolve = init(prob)\nsol1 = solve(linsolve)\n\nsol1.u","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"linsolve = LinearSolve.set_b(sol1.cache,b2)\nsol2 = solve(linsolve)\n\nsol2.u","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"Then refactorization will occur when a new A is given:","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"A2 = rand(n,n)\nlinsolve = LinearSolve.set_A(sol2.cache,A2)\nsol3 = solve(linsolve)\n\nsol3.u","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"The factorization occurs on the first solve, and it stores the factorization in the cache. You can retrieve this cache via sol.cache, which is the same object as the init but updated to know not to re-solve the factorization.","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"The advantage of course with using LinearSolve.jl in this form is that it is efficient while being agnostic to the linear solver. One can easily swap in iterative solvers, sparse solvers, etc. and it will do all of the tricks like caching symbolic factorizations if the sparsity pattern is unchanged.","category":"page"}]
}
