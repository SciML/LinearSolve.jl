<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Linear System Solvers · LinearSolve.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://docs.sciml.ai/LinearSolve/stable/solvers/solvers/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="LinearSolve.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">LinearSolve.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">LinearSolve.jl: High-Performance Unified Linear Solvers</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/linear/">Solving Linear Systems in Julia</a></li><li><a class="tocitem" href="../../tutorials/caching_interface/">Linear Solve with Caching Interface</a></li></ul></li><li><span class="tocitem">Basics</span><ul><li><a class="tocitem" href="../../basics/LinearProblem/">Linear Problems</a></li><li><a class="tocitem" href="../../basics/common_solver_opts/">Common Solver Options (Keyword Arguments for Solve)</a></li><li><a class="tocitem" href="../../basics/CachingAPI/">Caching Interface API Functions</a></li><li><a class="tocitem" href="../../basics/Preconditioners/">Preconditioners</a></li><li><a class="tocitem" href="../../basics/FAQ/">Frequently Asked Questions</a></li></ul></li><li><span class="tocitem">Solvers</span><ul><li class="is-active"><a class="tocitem" href>Linear System Solvers</a><ul class="internal"><li><a class="tocitem" href="#Recommended-Methods"><span>Recommended Methods</span></a></li><li><a class="tocitem" href="#Full-List-of-Methods"><span>Full List of Methods</span></a></li></ul></li></ul></li><li><span class="tocitem">Advanced</span><ul><li><a class="tocitem" href="../../advanced/developing/">Developing New Linear Solvers</a></li><li><a class="tocitem" href="../../advanced/custom/">Passing in a Custom Linear Solver</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Solvers</a></li><li class="is-active"><a href>Linear System Solvers</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Linear System Solvers</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/SciML/LinearSolve.jl/blob/main/docs/src/solvers/solvers.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="linearsystemsolvers"><a class="docs-heading-anchor" href="#linearsystemsolvers">Linear System Solvers</a><a id="linearsystemsolvers-1"></a><a class="docs-heading-anchor-permalink" href="#linearsystemsolvers" title="Permalink"></a></h1><p><code>solve(prob::LinearProlem,alg;kwargs)</code></p><p>Solves for <span>$Au=b$</span> in the problem defined by <code>prob</code> using the algorithm <code>alg</code>. If no algorithm is given, a default algorithm will be chosen.</p><h2 id="Recommended-Methods"><a class="docs-heading-anchor" href="#Recommended-Methods">Recommended Methods</a><a id="Recommended-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Recommended-Methods" title="Permalink"></a></h2><p>The default algorithm <code>nothing</code> is good for picking an algorithm that will work, but one may need to change this to receive more performance or precision. If more precision is necessary, <code>QRFactorization()</code> and <code>SVDFactorization()</code> are the best choices, with SVD being the slowest but most precise.</p><p>For efficiency, <code>RFLUFactorization</code> is the fastest for dense LU-factorizations. <code>FastLUFactorization</code> will be faster than <code>LUFactorization</code> which is the Base.LinearAlgebra (<code>\</code> default) implementation of LU factorization. <code>SimpleLUFactorization</code> will be fast on very small matrices.</p><p>For sparse LU-factorizations, <code>KLUFactorization</code> if there is less structure to the sparsity pattern and <code>UMFPACKFactorization</code> if there is more structure. Pardiso.jl&#39;s methods are also known to be very efficient sparse linear solvers.</p><p>While these sparse factorizations are based on implementations in other languages, and therefore constrained to standard number types (<code>Float64</code>,  <code>Float32</code> and their complex counterparts),  <code>SparspakFactorization</code> is able to handle general number types, e.g. defined by <code>ForwardDiff.jl</code>, <code>MultiFloats.jl</code>, or <code>IntervalArithmetics.jl</code>.</p><p>As sparse matrices get larger, iterative solvers tend to get more efficient than factorization methods if a lower tolerance of the solution is required.</p><p>Krylov.jl generally outperforms IterativeSolvers.jl and KrylovKit.jl, and is compatible with CPUs and GPUs, and thus is the generally preferred form for Krylov methods.</p><p>Finally, a user can pass a custom function for handling the linear solve using <code>LinearSolveFunction()</code> if existing solvers are not optimally suited for their application. The interface is detailed <a href="#passing-in-a-custom-linear-solver">here</a>.</p><h2 id="Full-List-of-Methods"><a class="docs-heading-anchor" href="#Full-List-of-Methods">Full List of Methods</a><a id="Full-List-of-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Full-List-of-Methods" title="Permalink"></a></h2><h3 id="RecursiveFactorization.jl"><a class="docs-heading-anchor" href="#RecursiveFactorization.jl">RecursiveFactorization.jl</a><a id="RecursiveFactorization.jl-1"></a><a class="docs-heading-anchor-permalink" href="#RecursiveFactorization.jl" title="Permalink"></a></h3><ul><li><code>RFLUFactorization()</code>: a fast pure Julia LU-factorization implementation using RecursiveFactorization.jl. This is by far the fastest LU-factorization implementation, usually outperforming OpenBLAS and MKL, but currently optimized only for Base <code>Array</code> with <code>Float32</code> or <code>Float64</code>.  Additional optimization for complex matrices is in the works.</li></ul><h3 id="Base.LinearAlgebra"><a class="docs-heading-anchor" href="#Base.LinearAlgebra">Base.LinearAlgebra</a><a id="Base.LinearAlgebra-1"></a><a class="docs-heading-anchor-permalink" href="#Base.LinearAlgebra" title="Permalink"></a></h3><p>These overloads tend to work for many array types, such as <code>CuArrays</code> for GPU-accelerated solving, using the overloads provided by the respective packages. Given that this can be customized per-package, details given below describe a subset of important arrays (<code>Matrix</code>, <code>SparseMatrixCSC</code>, <code>CuMatrix</code>, etc.)</p><ul><li><p><code>LUFactorization(pivot=LinearAlgebra.RowMaximum())</code>: Julia&#39;s built in <code>lu</code>.</p><ul><li>On dense matrices, this uses the current BLAS implementation of the user&#39;s computer, which by default is OpenBLAS but will use MKL if the user does <code>using MKL</code> in their system.</li><li>On sparse matrices, this will use UMFPACK from SuiteSparse. Note that this will not cache the symbolic factorization.</li><li>On CuMatrix, it will use a CUDA-accelerated LU from CuSolver.</li><li>On BandedMatrix and BlockBandedMatrix, it will use a banded LU.</li></ul></li><li><p><code>QRFactorization(pivot=LinearAlgebra.NoPivot(),blocksize=16)</code>: Julia&#39;s built in <code>qr</code>.</p><ul><li>On dense matrices, this uses the current BLAS implementation of the user&#39;s computer which by default is OpenBLAS but will use MKL if the user does <code>using MKL</code> in their system.</li><li>On sparse matrices, this will use SPQR from SuiteSparse</li><li>On CuMatrix, it will use a CUDA-accelerated QR from CuSolver.</li><li>On BandedMatrix and BlockBandedMatrix, it will use a banded QR.</li></ul></li><li><p><code>SVDFactorization(full=false,alg=LinearAlgebra.DivideAndConquer())</code>: Julia&#39;s built in <code>svd</code>.</p><ul><li>On dense matrices, this uses the current BLAS implementation of the user&#39;s computer which by default is OpenBLAS but will use MKL if the user does <code>using MKL</code> in their system.</li></ul></li><li><p><code>GenericFactorization(;fact_alg=LinearAlgebra.factorize())</code>: Constructs a linear solver from a generic factorization algorithm <code>fact_alg</code> which complies with the Base.LinearAlgebra factorization API. Quoting from Base:</p><ul><li>If <code>A</code> is upper or lower triangular (or diagonal), no factorization of <code>A</code> is required. The system is then solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used. For rectangular <code>A</code> the result is the minimum-norm least squares solution computed by a pivoted QR factorization of <code>A</code> and a rank estimate of <code>A</code> based on the R factor. When <code>A</code> is sparse, a similar polyalgorithm is used. For indefinite matrices, the <code>LDLt</code> factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.</li></ul></li></ul><h3 id="LinearSolve.jl"><a class="docs-heading-anchor" href="#LinearSolve.jl">LinearSolve.jl</a><a id="LinearSolve.jl-1"></a><a class="docs-heading-anchor-permalink" href="#LinearSolve.jl" title="Permalink"></a></h3><p>LinearSolve.jl contains some linear solvers built in.</p><ul><li><code>SimpleLUFactorization</code>: a simple LU-factorization implementation without BLAS. Fast for small matrices.</li><li><code>DiagonalFactorization</code>: a special implementation only for solving <code>Diagonal</code> matrices fast.</li></ul><h3 id="FastLapackInterface.jl"><a class="docs-heading-anchor" href="#FastLapackInterface.jl">FastLapackInterface.jl</a><a id="FastLapackInterface.jl-1"></a><a class="docs-heading-anchor-permalink" href="#FastLapackInterface.jl" title="Permalink"></a></h3><p>FastLapackInterface.jl is a package that allows for a lower-level interface to the LAPACK calls to allow for preallocating workspaces to decrease the overhead of the wrappers. LinearSolve.jl provides a wrapper to these routines in a way where an initialized solver has a non-allocating LU factorization. In theory, this post-initialized solve should always be faster than the Base.LinearAlgebra version.</p><ul><li><code>FastLUFactorization</code> the <code>FastLapackInterface</code> version of the LU factorization. Notably, this version does not allow for choice of pivoting method.</li><li><code>FastQRFactorization(pivot=NoPivot(),blocksize=32)</code>, the <code>FastLapackInterface</code> version of the QR factorization.</li></ul><h3 id="SuiteSparse.jl"><a class="docs-heading-anchor" href="#SuiteSparse.jl">SuiteSparse.jl</a><a id="SuiteSparse.jl-1"></a><a class="docs-heading-anchor-permalink" href="#SuiteSparse.jl" title="Permalink"></a></h3><p>By default, the SuiteSparse.jl are implemented for efficiency by caching the symbolic factorization. I.e., if <code>set_A</code> is used, it is expected that the new <code>A</code> has the same sparsity pattern as the previous <code>A</code>. If this algorithm is to be used in a context where that assumption does not hold, set <code>reuse_symbolic=false</code>.</p><ul><li><code>KLUFactorization(;reuse_symbolic=true)</code>: A fast sparse LU-factorization which specializes on sparsity patterns with “less structure”.</li><li><code>UMFPACKFactorization(;reuse_symbolic=true)</code>: A fast sparse multithreaded LU-factorization which specializes on sparsity patterns with “more structure”.</li></ul><h3 id="Pardiso.jl"><a class="docs-heading-anchor" href="#Pardiso.jl">Pardiso.jl</a><a id="Pardiso.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Pardiso.jl" title="Permalink"></a></h3><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Using this solver requires adding the package LinearSolvePardiso.jl</p></div></div><p>The following algorithms are pre-specified:</p><ul><li><code>MKLPardisoFactorize(;kwargs...)</code>: A sparse factorization method.</li><li><code>MKLPardisoIterate(;kwargs...)</code>: A mixed factorization+iterative method.</li></ul><p>Those algorithms are defined via:</p><pre><code class="language-julia hljs">function MKLPardisoFactorize(; kwargs...)
    PardisoJL(; fact_phase = Pardiso.NUM_FACT,
              solve_phase = Pardiso.SOLVE_ITERATIVE_REFINE,
              kwargs...)
end
function MKLPardisoIterate(; kwargs...)
    PardisoJL(; solve_phase = Pardiso.NUM_FACT_SOLVE_REFINE,
              kwargs...)
end</code></pre><p>The full set of keyword arguments for <code>PardisoJL</code> are:</p><pre><code class="language-julia hljs">Base.@kwdef struct PardisoJL &lt;: SciMLLinearSolveAlgorithm
    nprocs::Union{Int, Nothing} = nothing
    solver_type::Union{Int, Pardiso.Solver, Nothing} = nothing
    matrix_type::Union{Int, Pardiso.MatrixType, Nothing} = nothing
    fact_phase::Union{Int, Pardiso.Phase, Nothing} = nothing
    solve_phase::Union{Int, Pardiso.Phase, Nothing} = nothing
    release_phase::Union{Int, Nothing} = nothing
    iparm::Union{Vector{Tuple{Int, Int}}, Nothing} = nothing
    dparm::Union{Vector{Tuple{Int, Int}}, Nothing} = nothing
end</code></pre><h3 id="Sparspak.jl"><a class="docs-heading-anchor" href="#Sparspak.jl">Sparspak.jl</a><a id="Sparspak.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Sparspak.jl" title="Permalink"></a></h3><p>This is the translation of the well-known sparse matrix software Sparspak (Waterloo Sparse Matrix Package), solving large sparse systems of linear algebraic equations. Sparspak is composed of the subroutines from the book &quot;Computer Solution of Large Sparse Positive Definite Systems&quot; by Alan George and Joseph Liu. Originally written in Fortran 77, later rewritten in Fortran 90. Here is the software translated into Julia. The Julia rewrite is released  under the MIT license with an express permission from the authors of the Fortran package. The package uses mutiple dispatch to route around standard BLAS routines in the case e.g. of arbitrary-precision floating point numbers or ForwardDiff.Dual. This e.g. allows for Automatic Differentiation (AD) of a sparse-matrix solve.</p><h3 id="CUDA.jl"><a class="docs-heading-anchor" href="#CUDA.jl">CUDA.jl</a><a id="CUDA.jl-1"></a><a class="docs-heading-anchor-permalink" href="#CUDA.jl" title="Permalink"></a></h3><p>Note that <code>CuArrays</code> are supported by <code>GenericFactorization</code> in the “normal” way. The following are non-standard GPU factorization routines.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Using this solver requires adding the package LinearSolveCUDA.jl</p></div></div><ul><li><code>CudaOffloadFactorization()</code>: An offloading technique used to GPU-accelerate CPU-based computations. Requires a sufficiently large <code>A</code> to overcome the data transfer costs.</li></ul><h3 id="IterativeSolvers.jl"><a class="docs-heading-anchor" href="#IterativeSolvers.jl">IterativeSolvers.jl</a><a id="IterativeSolvers.jl-1"></a><a class="docs-heading-anchor-permalink" href="#IterativeSolvers.jl" title="Permalink"></a></h3><ul><li><code>IterativeSolversJL_CG(args...;kwargs...)</code>: A generic CG implementation</li><li><code>IterativeSolversJL_GMRES(args...;kwargs...)</code>: A generic GMRES implementation</li><li><code>IterativeSolversJL_BICGSTAB(args...;kwargs...)</code>: A generic BICGSTAB implementation</li><li><code>IterativeSolversJL_MINRES(args...;kwargs...)</code>: A generic MINRES implementation</li></ul><p>The general algorithm is:</p><pre><code class="language-julia hljs">IterativeSolversJL(args...;
                   generate_iterator = IterativeSolvers.gmres_iterable!,
                   Pl = nothing, Pr = nothing,
                   gmres_restart = 0, kwargs...)</code></pre><h3 id="Krylov.jl"><a class="docs-heading-anchor" href="#Krylov.jl">Krylov.jl</a><a id="Krylov.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Krylov.jl" title="Permalink"></a></h3><ul><li><code>KrylovJL_CG(args...;kwargs...)</code>: A generic CG implementation for Hermitian and positive definite linear systems</li><li><code>KrylovJL_MINRES(args...;kwargs...)</code>: A generic MINRES implementation for Hermitian linear systems</li><li><code>KrylovJL_GMRES(args...;kwargs...)</code>: A generic GMRES implementation for square non-Hermitian linear systems</li><li><code>KrylovJL_BICGSTAB(args...;kwargs...)</code>: A generic BICGSTAB implementation for square non-Hermitian linear systems</li><li><code>KrylovJL_LSMR(args...;kwargs...)</code>: A generic LSMR implementation for least-squares problems</li><li><code>KrylovJL_CRAIGMR(args...;kwargs...)</code>: A generic CRAIGMR implementation for least-norm problems</li></ul><p>The general algorithm is:</p><pre><code class="language-julia hljs">KrylovJL(args...; KrylovAlg = Krylov.gmres!,
         Pl = nothing, Pr = nothing,
         gmres_restart = 0, window = 0,
         kwargs...)</code></pre><h3 id="KrylovKit.jl"><a class="docs-heading-anchor" href="#KrylovKit.jl">KrylovKit.jl</a><a id="KrylovKit.jl-1"></a><a class="docs-heading-anchor-permalink" href="#KrylovKit.jl" title="Permalink"></a></h3><ul><li><code>KrylovKitJL_CG(args...;kwargs...)</code>: A generic CG implementation</li><li><code>KrylovKitJL_GMRES(args...;kwargs...)</code>: A generic GMRES implementation</li></ul><p>The general algorithm is:</p><pre><code class="language-julia hljs">KrylovKitJL(args...;
            KrylovAlg = KrylovKit.GMRES, gmres_restart = 0,
            kwargs...)</code></pre><h3 id="HYPRE.jl"><a class="docs-heading-anchor" href="#HYPRE.jl">HYPRE.jl</a><a id="HYPRE.jl-1"></a><a class="docs-heading-anchor-permalink" href="#HYPRE.jl" title="Permalink"></a></h3><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Using HYPRE solvers requires Julia version 1.9 or higher, and that the package HYPRE.jl is installed.</p></div></div><p><a href="https://github.com/fredrikekre/HYPRE.jl">HYPRE.jl</a> is an interface to <a href="https://computing.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods"><code>hypre</code></a> and provide iterative solvers and preconditioners for sparse linear systems. It is mainly developed for large multi-process distributed problems (using MPI), but can also be used for single-process problems with Julias standard sparse matrices.</p><p>The algorithm is defined as:</p><pre><code class="language-julia hljs">alg = HYPREAlgorithm(X)</code></pre><p>where <code>X</code> is one of the following supported solvers:</p><ul><li><code>HYPRE.BiCGSTAB</code></li><li><code>HYPRE.BoomerAMG</code></li><li><code>HYPRE.FlexGMRES</code></li><li><code>HYPRE.GMRES</code></li><li><code>HYPRE.Hybrid</code></li><li><code>HYPRE.ILU</code></li><li><code>HYPRE.ParaSails</code> (as preconditioner only)</li><li><code>HYPRE.PCG</code></li></ul><p>Some of the solvers above can also be used as preconditioners by passing via the <code>Pl</code> keyword argument.</p><p>For example, to use <code>HYPRE.PCG</code> as the solver, with <code>HYPRE.BoomerAMG</code> as the preconditioner, the algorithm should be defined as follows:</p><pre><code class="language-julia hljs">A, b = setup_system(...)
prob = LinearProblem(A, b)
alg = HYPREAlgorithm(HYPRE.PCG)
prec = HYPRE.BoomerAMG
sol = solve(prob, alg; Pl = prec)</code></pre><p>If you need more fine-grained control over the solver/preconditioner options you can alternatively pass an already created solver to <code>HYPREAlgorithm</code> (and to the <code>Pl</code> keyword argument). See HYPRE.jl docs for how to set up solvers with specific options.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../basics/FAQ/">« Frequently Asked Questions</a><a class="docs-footer-nextpage" href="../../advanced/developing/">Developing New Linear Solvers »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Wednesday 25 January 2023 11:30">Wednesday 25 January 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
