var documenterSearchIndex = {"docs":
[{"location":"basics/LinearProblem/#Linear-Problems","page":"Linear Problems","title":"Linear Problems","text":"","category":"section"},{"location":"basics/LinearProblem/","page":"Linear Problems","title":"Linear Problems","text":"LinearProblem","category":"page"},{"location":"basics/LinearProblem/#SciMLBase.LinearProblem","page":"Linear Problems","title":"SciMLBase.LinearProblem","text":"Defines a linear system problem. Documentation Page: https://docs.sciml.ai/LinearSolve/stable/basics/LinearProblem/\n\nMathematical Specification of a Linear Problem\n\nConcrete LinearProblem\n\nTo define a LinearProblem, you simply need to give the AbstractMatrix A and an AbstractVector b which defines the linear system:\n\nAu = b\n\nMatrix-Free LinearProblem\n\nFor matrix-free versions, the specification of the problem is given by an operator A(u,p,t) which computes A*u, or in-place as A(du,u,p,t). These are specified via the AbstractSciMLOperator interface. For more details, see the SciMLBase Documentation.\n\nNote that matrix-free versions of LinearProblem definitions are not compatible with all solvers. To check a solver for compatibility, use the function xxxxx.\n\nProblem Type\n\nConstructors\n\nOptionally, an initial guess u₀ can be supplied which is used for iterative methods.\n\nLinearProblem{isinplace}(A,x,p=NullParameters();u0=nothing,kwargs...)\nLinearProblem(f::AbstractSciMLOperator,u0,p=NullParameters();u0=nothing,kwargs...)\n\nisinplace optionally sets whether the function is in-place or not, i.e. whether the solvers are allowed to mutate. By default this is true for AbstractMatrix, and for AbstractSciMLOperators it matches the choice of the operator definition.\n\nParameters are optional, and if not given, then a NullParameters() singleton will be used, which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers.\n\nFields\n\nA: The representation of the linear operator.\nb: The right-hand side of the linear system.\np: The parameters for the problem. Defaults to NullParameters. Currently unused.\nu0: The initial condition used by iterative solvers.\nkwargs: The keyword arguments passed on to the solvers.\n\n\n\n","category":"type"},{"location":"advanced/custom/#custom","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"","category":"section"},{"location":"advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"Julia users are building a wide variety of applications in the SciML ecosystem, often requiring problem-specific handling of their linear solves. As existing solvers in LinearSolve.jl may not be optimally suited for novel applications, it is essential for the linear solve interface to be easily extendable by users. To that end, the linear solve algorithm LinearSolveFunction() accepts a user-defined function for handling the solve. A user can pass in their custom linear solve function, say my_linsolve, to LinearSolveFunction(). A contrived example of solving a linear system with a custom solver is below.","category":"page"},{"location":"advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"using LinearSolve, LinearAlgebra\n\nfunction my_linsolve(A, b, u, p, newA, Pl, Pr, solverdata; verbose = true, kwargs...)\n    if verbose == true\n        println(\"solving Ax=b\")\n    end\n    u = A \\ b\n    return u\nend\n\nprob = LinearProblem(Diagonal(rand(4)), rand(4))\nalg = LinearSolveFunction(my_linsolve)\nsol = solve(prob, alg)\nsol.u","category":"page"},{"location":"advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"The inputs to the function are as follows:","category":"page"},{"location":"advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"u, the solution initialized as zero(b),\nA, the linear operator\nb, the right-hand-side\np, a set of parameters\nnewA, a Bool which is true if A has been modified since last solve\nPl, left-preconditioner\nPr, right-preconditioner\nsolverdata, solver cache set to nothing if solver hasn't been initialized\nkwargs, standard SciML keyword arguments such as verbose, maxiters, abstol, reltol","category":"page"},{"location":"advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"The function my_linsolve must accept the above specified arguments and modify the, and return the solution, u. As memory for u is already allocated, the user may choose to modify u in place as follows:","category":"page"},{"location":"advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"function my_linsolve!(A, b, u, p, newA, Pl, Pr, solverdata; verbose = true, kwargs...)\n    if verbose == true\n        println(\"solving Ax=b\")\n    end\n    u .= A \\ b # in place\n    return u\nend\n\nalg = LinearSolveFunction(my_linsolve!)\nsol = solve(prob, alg)\nsol.u","category":"page"},{"location":"basics/OperatorAssumptions/#assumptions","page":"Linear Solve Operator Assumptions","title":"Linear Solve Operator Assumptions","text":"","category":"section"},{"location":"basics/OperatorAssumptions/","page":"Linear Solve Operator Assumptions","title":"Linear Solve Operator Assumptions","text":"OperatorAssumptions\nOperatorCondition","category":"page"},{"location":"basics/OperatorAssumptions/#LinearSolve.OperatorAssumptions","page":"Linear Solve Operator Assumptions","title":"LinearSolve.OperatorAssumptions","text":"OperatorAssumptions(issquare = nothing; condition::OperatorCondition.T = IllConditioned)\n\nSets the operator A assumptions used as part of the default algorithm\n\n\n\n\n\n","category":"type"},{"location":"basics/OperatorAssumptions/#LinearSolve.OperatorCondition","page":"Linear Solve Operator Assumptions","title":"LinearSolve.OperatorCondition","text":"OperatorCondition\n\nSpecifies the assumption of matrix conditioning for the default linear solver choices. Condition number is defined as the ratio of eigenvalues. The numerical stability of many linear solver algorithms can be dependent on the condition number of the matrix. The condition number can be computed as:\n\nusing LinearAlgebra\ncond(rand(100,100))\n\nHowever, in practice this computation is very expensive and thus not possible for most practical cases. Therefore, OperatorCondition lets one share to LinearSolve the expected conditioning. The higher the expected condition number, the safer the algorithm needs to be and thus there is a trade-off between numerical performance and stability. By default the method assumes the operator may be ill-conditioned for the standard linear solvers to converge (such as LU-factorization), though more extreme  ill-conditioning or well-conditioning could be the case and specified through this assumption.\n\n\n\n\n\n","category":"module"},{"location":"basics/OperatorAssumptions/#Condition-Number-Specifications","page":"Linear Solve Operator Assumptions","title":"Condition Number Specifications","text":"","category":"section"},{"location":"basics/OperatorAssumptions/","page":"Linear Solve Operator Assumptions","title":"Linear Solve Operator Assumptions","text":"OperatorCondition.IllConditioned\nOperatorCondition.VeryIllConditioned\nOperatorCondition.SuperIllConditioned\nOperatorCondition.WellConditioned","category":"page"},{"location":"basics/OperatorAssumptions/#LinearSolve.OperatorCondition.IllConditioned","page":"Linear Solve Operator Assumptions","title":"LinearSolve.OperatorCondition.IllConditioned","text":"OperatorCondition.IllConditioned\n\nThe default assumption of LinearSolve. Assumes that the operator can have minor ill-conditioning and thus needs to use safe algorithms.\n\n\n\n\n\n","category":"constant"},{"location":"basics/OperatorAssumptions/#LinearSolve.OperatorCondition.VeryIllConditioned","page":"Linear Solve Operator Assumptions","title":"LinearSolve.OperatorCondition.VeryIllConditioned","text":"OperatorCondition.VeryIllConditioned\n\nAssumes that the operator can have fairly major ill-conditioning and thus the standard linear algebra algorithms cannot be used.\n\n\n\n\n\n","category":"constant"},{"location":"basics/OperatorAssumptions/#LinearSolve.OperatorCondition.SuperIllConditioned","page":"Linear Solve Operator Assumptions","title":"LinearSolve.OperatorCondition.SuperIllConditioned","text":"OperatorCondition.SuperIllConditioned\n\nAssumes that the operator can have fairly extreme ill-conditioning and thus the most stable algorithm is used.\n\n\n\n\n\n","category":"constant"},{"location":"basics/OperatorAssumptions/#LinearSolve.OperatorCondition.WellConditioned","page":"Linear Solve Operator Assumptions","title":"LinearSolve.OperatorCondition.WellConditioned","text":"OperatorCondition.WellConditioned\n\nAssumes that the operator can have fairly contained conditioning and thus the fastest algorithm is used.\n\n\n\n\n\n","category":"constant"},{"location":"basics/FAQ/#Frequently-Asked-Questions","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"","category":"section"},{"location":"basics/FAQ/#How-is-LinearSolve.jl-compared-to-just-using-normal-\\,-i.e.-A\\b?","page":"Frequently Asked Questions","title":"How is LinearSolve.jl compared to just using normal \\, i.e. A\\b?","text":"","category":"section"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Check out this video from JuliaCon 2022 which goes into detail on how and why LinearSolve.jl can be a more general and efficient interface.","category":"page"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Note that if \\ is good enough for you, great! We still tend to use \\ in the REPL all the time! However, if you're building a package, you may want to consider using LinearSolve.jl for the improved efficiency and ability to choose solvers.","category":"page"},{"location":"basics/FAQ/#I'm-seeing-some-dynamic-dispatches-in-the-default-algorithm-choice,-how-do-I-reduce-that?","page":"Frequently Asked Questions","title":"I'm seeing some dynamic dispatches in the default algorithm choice, how do I reduce that?","text":"","category":"section"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Make sure you set the OperatorAssumptions to get the full performance, especially the issquare choice as otherwise that will need to be determined at runtime.","category":"page"},{"location":"basics/FAQ/#I-found-a-faster-algorithm-that-can-be-used-than-what-LinearSolve.jl-chose?","page":"Frequently Asked Questions","title":"I found a faster algorithm that can be used than what LinearSolve.jl chose?","text":"","category":"section"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"What assumptions are made as part of your method? If your method only works on well-conditioned operators, then make sure you set the WellConditioned assumption in the assumptions. See the OperatorAssumptions page for more details. If using the right assumptions does not improve the performance to the expected state, please open an issue and we will improve the default algorithm.","category":"page"},{"location":"basics/FAQ/#Python's-NumPy/SciPy-just-calls-fast-Fortran/C-code,-why-would-LinearSolve.jl-be-any-better?","page":"Frequently Asked Questions","title":"Python's NumPy/SciPy just calls fast Fortran/C code, why would LinearSolve.jl be any better?","text":"","category":"section"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"This is addressed in the JuliaCon 2022 video. This happens in a few ways:","category":"page"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"The Fortran/C code that NumPy/SciPy uses is actually slow. It's OpenBLAS, a library developed in part by the Julia Lab back in 2012 as a fast open source BLAS implementation. Many open source environments now use this build, including many R distributions. However, the Julia Lab has greatly improved its ability to generate optimized SIMD in platform-specific ways. This, and improved multithreading support (OpenBLAS's multithreading is rather slow), has led to pure Julia-based BLAS implementations which the lab now works on. This includes RecursiveFactorization.jl which generally outperforms OpenBLAS by 2x-10x depending on the platform. It even outperforms MKL for small matrices (<100). LinearSolve.jl uses RecursiveFactorization.jl by default sometimes, but switches to BLAS when it would be faster (in a platform and matrix-specific way).\nStandard approaches to handling linear solves re-allocate the pivoting vector each time. This leads to GC pauses that can slow down calculations. LinearSolve.jl has proper caches for fully preallocated no-GC workflows.\nLinearSolve.jl makes many other optimizations, like factorization reuse and symbolic factorization reuse, automatic. Many of these optimizations are not even possible from the high-level APIs of things like Python's major libraries and MATLAB.\nLinearSolve.jl has a much more extensive set of sparse matrix solvers, which is why you see a major difference (2x-10x) for sparse matrices. Which sparse matrix solver between KLU, UMFPACK, Pardiso, etc. is optimal depends a lot on matrix sizes, sparsity patterns, and threading overheads. LinearSolve.jl's heuristics handle these kinds of issues.","category":"page"},{"location":"basics/FAQ/#How-do-I-use-IterativeSolvers-solvers-with-a-weighted-tolerance-vector?","page":"Frequently Asked Questions","title":"How do I use IterativeSolvers solvers with a weighted tolerance vector?","text":"","category":"section"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"IterativeSolvers.jl computes the norm after the application of the left preconditioner Pl. Thus, in order to use a vector tolerance weights, one can mathematically hack the system via the following formulation:","category":"page"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"using LinearSolve, LinearAlgebra\n\nn = 2\nA = rand(n, n)\nb = rand(n)\n\nweights = [1e-1, 1]\nPl = LinearSolve.InvPreconditioner(Diagonal(weights))\nPr = Diagonal(weights)\n\nprob = LinearProblem(A, b)\nsol = solve(prob, KrylovJL_GMRES(), Pl = Pl, Pr = Pr)\n\nsol.u","category":"page"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"If you want to use a “real” preconditioner under the norm weights, then one can use ComposePreconditioner to apply the preconditioner after the application of the weights like as follows:","category":"page"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"using LinearSolve, LinearAlgebra\n\nn = 4\nA = rand(n, n)\nb = rand(n)\n\nweights = rand(n)\nrealprec = lu(rand(n, n)) # some random preconditioner\nPl = LinearSolve.ComposePreconditioner(LinearSolve.InvPreconditioner(Diagonal(weights)),\n    realprec)\nPr = Diagonal(weights)\n\nprob = LinearProblem(A, b)\nsol = solve(prob, KrylovJL_GMRES(), Pl = Pl, Pr = Pr)","category":"page"},{"location":"advanced/developing/#Developing-New-Linear-Solvers","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"","category":"section"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"Developing new or custom linear solvers for the SciML interface can be done in one of two ways:","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"You can either create a completely new set of dispatches for init and solve.\nYou can extend LinearSolve.jl's internal mechanisms.","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"For developer ease, we highly recommend (2) as that will automatically make the caching API work. Thus, this is the documentation for how to do that.","category":"page"},{"location":"advanced/developing/#Developing-New-Linear-Solvers-with-LinearSolve.jl-Primitives","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers with LinearSolve.jl Primitives","text":"","category":"section"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"Let's create a new wrapper for a simple LU-factorization which uses only the basic machinery. A simplified version is:","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"struct MyLUFactorization{P} <: SciMLBase.AbstractLinearAlgorithm end\n\nfunction init_cacheval(alg::MyLUFactorization, A, b, u, Pl, Pr, maxiters, abstol, reltol,\n    verbose)\n    lu!(convert(AbstractMatrix, A))\nend\n\nfunction SciMLBase.solve!(cache::LinearCache, alg::MyLUFactorization; kwargs...)\n    if cache.isfresh\n        A = convert(AbstractMatrix, A)\n        fact = lu!(A)\n        cache = set_cacheval(cache, fact)\n    end\n    y = ldiv!(cache.u, cache.cacheval, cache.b)\n    SciMLBase.build_linear_solution(alg, y, nothing, cache)\nend","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"The way this works is as follows. LinearSolve.jl has a LinearCache that everything shares (this is what gives most of the ease of use). However, many algorithms need to cache their own things, and so there's one value cacheval that is for the algorithms to modify. The function:","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"init_cacheval(alg::MyLUFactorization, A, b, u, Pl, Pr, maxiters, abstol, reltol, verbose)","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"is what is called at init time to create the first cacheval. Note that this should match the type of the cache later used in solve as many algorithms, like those in OrdinaryDiffEq.jl, expect type-groundedness in the linear solver definitions. While there are cheaper ways to obtain this type for LU factorizations (specifically, ArrayInterface.lu_instance(A)), for a demonstration, this just performs an LU-factorization to get an LU{T, Matrix{T}} which it puts into the cacheval so it is typed for future use.","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"After the init_cacheval, the only thing left to do is to define SciMLBase.solve(cache::LinearCache, alg::MyLUFactorization). Many algorithms may use a lazy matrix-free representation of the operator A. Thus, if the algorithm requires a concrete matrix, like LU-factorization does, the algorithm should convert(AbstractMatrix,cache.A). The flag cache.isfresh states whether A has changed since the last solve. Since we only need to factorize when A is new, the factorization part of the algorithm is done in a if cache.isfresh. cache = set_cacheval(cache, fact) puts the new factorization into the cache, so it's updated for future solves. Then y = ldiv!(cache.u, cache.cacheval, cache.b) performs the solve and a linear solution is returned via SciMLBase.build_linear_solution(alg,y,nothing,cache).","category":"page"},{"location":"basics/Preconditioners/#prec","page":"Preconditioners","title":"Preconditioners","text":"","category":"section"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"Many linear solvers can be accelerated by using what is known as a preconditioner, an approximation to the matrix inverse action which is cheap to evaluate. These can improve the numerical conditioning of the solver process and in turn improve the performance. LinearSolve.jl provides an interface for the definition of preconditioners which works with the wrapped packages.","category":"page"},{"location":"basics/Preconditioners/#Using-Preconditioners","page":"Preconditioners","title":"Using Preconditioners","text":"","category":"section"},{"location":"basics/Preconditioners/#Mathematical-Definition","page":"Preconditioners","title":"Mathematical Definition","text":"","category":"section"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"Preconditioners are specified in the keyword arguments to init or solve: Pl for left and Pr for right preconditioner, respectively. The right preconditioner, P_r transforms the linear system Au = b into the form:","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"AP_r^-1(P_r u) = AP_r^-1y = b","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"which is solved for y, and then P_r u = y is solved for u. The left preconditioner, P_l, transforms the linear system into the form:","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"P_l^-1Au = P_l^-1b","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"A two-sided preconditioned system is of the form:","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"P_l^-1A P_r^-1 (P_r u) = P_l^-1b","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"By default, if no preconditioner is given, the preconditioner is assumed to be the identity I.","category":"page"},{"location":"basics/Preconditioners/#Using-Preconditioners-2","page":"Preconditioners","title":"Using Preconditioners","text":"","category":"section"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"In the following, we will use the DiagonalPreconditioner to define a two-sided preconditioned system which first divides by some random numbers and then multiplies by the same values. This is commonly used in the case where if, instead of random, s is an approximation to the eigenvalues of a system.","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"using LinearSolve, LinearAlgebra\nn = 4\ns = rand(n)\nPl = Diagonal(s)\n\nA = rand(n, n)\nb = rand(n)\n\nprob = LinearProblem(A, b)\nsol = solve(prob, KrylovJL_GMRES(), Pl = Pl)\nsol.u","category":"page"},{"location":"basics/Preconditioners/#Preconditioner-Interface","page":"Preconditioners","title":"Preconditioner Interface","text":"","category":"section"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"To define a new preconditioner you define a Julia type which satisfies the following interface:","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"Base.eltype(::Preconditioner) (Required only for Krylov.jl)\nLinearAlgebra.ldiv!(::AbstractVector,::Preconditioner,::AbstractVector) and LinearAlgebra.ldiv!(::Preconditioner,::AbstractVector)","category":"page"},{"location":"basics/Preconditioners/#Curated-List-of-Pre-Defined-Preconditioners","page":"Preconditioners","title":"Curated List of Pre-Defined Preconditioners","text":"","category":"section"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"The following preconditioners match the interface of LinearSolve.jl.","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"LinearSolve.ComposePreconditioner(prec1,prec2): composes the preconditioners to apply prec1 before prec2.\nLinearSolve.InvPreconditioner(prec): inverts mul! and ldiv! in a preconditioner definition as a lazy inverse.\nLinearAlgera.Diagonal(s::Union{Number,AbstractVector}): the lazy Diagonal matrix type of Base.LinearAlgebra. Used for efficient construction of a diagonal preconditioner.\nOther Base.LinearAlgera types: all define the full Preconditioner interface.\nIncompleteLU.ilu: an implementation of the incomplete LU-factorization preconditioner. This requires A as a SparseMatrixCSC.\nPreconditioners.CholeskyPreconditioner(A, i): An incomplete Cholesky preconditioner with cut-off level i. Requires A as a AbstractMatrix and positive semi-definite.\nAlgebraicMultiGrid: Implementations of the algebraic multigrid method. Must be converted to a preconditioner via AlgebraicMultiGrid.aspreconditioner(AlgebraicMultiGrid.precmethod(A)). Requires A as a AbstractMatrix. Provides the following methods:\nAlgebraicMultiGrid.ruge_stuben(A)\nAlgebraicMultiGrid.smoothed_aggregation(A)\nPyAMG: Implementations of the algebraic multigrid method. Must be converted to a preconditioner via PyAMG.aspreconditioner(PyAMG.precmethod(A)). Requires A as a AbstractMatrix. Provides the following methods:\nPyAMG.RugeStubenSolver(A)\nPyAMG.SmoothedAggregationSolver(A)\nILUZero.ILU0Precon(A::SparseMatrixCSC{T,N}, b_type = T): An incomplete LU implementation. Requires A as a SparseMatrixCSC.\nLimitedLDLFactorizations.lldl: A limited-memory LDLᵀ factorization for symmetric matrices. Requires A as a SparseMatrixCSC. Applying F = lldl(A); F.D .= abs.(F.D) before usage as a preconditioner makes the preconditioner symmetric positive definite and thus is required for Krylov methods which are specialized for symmetric linear systems.\nRandomizedPreconditioners.NystromPreconditioner A randomized sketching method for positive semidefinite matrices A. Builds a preconditioner P  A + μ*I for the system (A + μ*I)x = b.\nHYPRE.jl A set of solvers with preconditioners which supports distributed computing via MPI. These can be written using the LinearSolve.jl interface choosing algorithms like HYPRE.ILU and HYPRE.BoomerAMG.","category":"page"},{"location":"tutorials/linear/#Solving-Linear-Systems-in-Julia","page":"Solving Linear Systems in Julia","title":"Solving Linear Systems in Julia","text":"","category":"section"},{"location":"tutorials/linear/","page":"Solving Linear Systems in Julia","title":"Solving Linear Systems in Julia","text":"A linear system Au=b is specified by defining an AbstractMatrix A, or by providing a matrix-free operator for performing A*x operations via the function A(u,p,t) out-of-place and A(du,u,p,t) for in-place. For the sake of simplicity, this tutorial will only showcase concrete matrices.","category":"page"},{"location":"tutorials/linear/","page":"Solving Linear Systems in Julia","title":"Solving Linear Systems in Julia","text":"The following defines a matrix and a LinearProblem which is subsequently solved by the default linear solver.","category":"page"},{"location":"tutorials/linear/","page":"Solving Linear Systems in Julia","title":"Solving Linear Systems in Julia","text":"using LinearSolve\n\nA = rand(4, 4)\nb = rand(4)\nprob = LinearProblem(A, b)\nsol = solve(prob)\nsol.u","category":"page"},{"location":"tutorials/linear/","page":"Solving Linear Systems in Julia","title":"Solving Linear Systems in Julia","text":"Note that solve(prob) is equivalent to solve(prob,nothing) where nothing denotes the choice of the default linear solver. This is equivalent to the Julia built-in A\\b, where the solution is recovered via sol.u. The power of this package comes into play when changing the algorithms. For example, Krylov.jl has some nice methods like GMRES which can be faster in some cases. With LinearSolve.jl, there is one interface and changing linear solvers is simply the switch of the algorithm choice:","category":"page"},{"location":"tutorials/linear/","page":"Solving Linear Systems in Julia","title":"Solving Linear Systems in Julia","text":"sol = solve(prob, KrylovJL_GMRES())\nsol.u","category":"page"},{"location":"tutorials/linear/","page":"Solving Linear Systems in Julia","title":"Solving Linear Systems in Julia","text":"Thus, a package which uses LinearSolve.jl simply needs to allow the user to pass in an algorithm struct and all wrapped linear solvers are immediately available as tweaks to the general algorithm.","category":"page"},{"location":"release_notes/#Release-Notes","page":"Release Notes","title":"Release Notes","text":"","category":"section"},{"location":"release_notes/#v2.0","page":"Release Notes","title":"v2.0","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"LinearCache changed from immutable to mutable. With this, the out of place interfaces like set_A were deprecated for simply mutating the cache, cache.A = .... This fixes some correctness checks and makes the package more robust while improving performance.\nThe default algorithm is now type-stable and does not rely on a dynamic dispatch for the choice.\nIterativeSolvers.jl and KrylovKit.jl were made into extension packages.\nDocumentation of the solvers has changed to docstrings","category":"page"},{"location":"basics/common_solver_opts/#Common-Solver-Options-(Keyword-Arguments-for-Solve)","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"While many algorithms have specific arguments within their constructor, the keyword arguments for solve are common across all the algorithms in order to give composability. These are also the options taken at init time. The following are the options these algorithms take, along with their defaults.","category":"page"},{"location":"basics/common_solver_opts/#General-Controls","page":"Common Solver Options (Keyword Arguments for Solve)","title":"General Controls","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"alias_A::Bool: Whether to alias the matrix A or use a copy by default. When true, algorithms like LU-factorization can be faster by reusing the memory via lu!, but care must be taken as the original input will be modified. Default is true if the algorithm is known not to modify A, otherwise is false.\nalias_b::Bool: Whether to alias the matrix b or use a copy by default. When true, algorithms can write and change b upon usage. Care must be taken as the original input will be modified. Default is true if the algorithm is known not to modify b, otherwise false.\nverbose: Whether to print extra information. Defaults to false.\nassumptions: Sets the assumptions of the operator in order to effect the default choice algorithm. See the Operator Assumptions page for more details.","category":"page"},{"location":"basics/common_solver_opts/#Iterative-Solver-Controls","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Iterative Solver Controls","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"Error controls are not used by all algorithms. Specifically, direct solves always solve completely. Error controls only apply to iterative solvers.","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"abstol: The absolute tolerance. Defaults to √(eps(eltype(A)))\nreltol: The relative tolerance. Defaults to √(eps(eltype(A)))\nmaxiters: The number of iterations allowed. Defaults to length(prob.b)\nPl,Pr: The left and right preconditioners, respectively. For more information, see the Preconditioners page.","category":"page"},{"location":"solvers/solvers/#linearsystemsolvers","page":"Linear System Solvers","title":"Linear System Solvers","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"solve(prob::LinearProblem,alg;kwargs)","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Solves for Au=b in the problem defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.","category":"page"},{"location":"solvers/solvers/#Recommended-Methods","page":"Linear System Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"The default algorithm nothing is good for picking an algorithm that will work, but one may need to change this to receive more performance or precision. If more precision is necessary, QRFactorization() and SVDFactorization() are the best choices, with SVD being the slowest but most precise.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"For efficiency, RFLUFactorization is the fastest for dense LU-factorizations. FastLUFactorization will be faster than LUFactorization which is the Base.LinearAlgebra (\\ default) implementation of LU factorization. SimpleLUFactorization will be fast on very small matrices.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"For sparse LU-factorizations, KLUFactorization if there is less structure to the sparsity pattern and UMFPACKFactorization if there is more structure. Pardiso.jl's methods are also known to be very efficient sparse linear solvers.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"While these sparse factorizations are based on implementations in other languages, and therefore constrained to standard number types (Float64,  Float32 and their complex counterparts),  SparspakFactorization is able to handle general number types, e.g. defined by ForwardDiff.jl, MultiFloats.jl, or IntervalArithmetics.jl.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"As sparse matrices get larger, iterative solvers tend to get more efficient than factorization methods if a lower tolerance of the solution is required.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Krylov.jl generally outperforms IterativeSolvers.jl and KrylovKit.jl, and is compatible with CPUs and GPUs, and thus is the generally preferred form for Krylov methods.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Finally, a user can pass a custom function for handling the linear solve using LinearSolveFunction() if existing solvers are not optimally suited for their application. The interface is detailed here.","category":"page"},{"location":"solvers/solvers/#Full-List-of-Methods","page":"Linear System Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"solvers/solvers/#RecursiveFactorization.jl","page":"Linear System Solvers","title":"RecursiveFactorization.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"RFLUFactorization","category":"page"},{"location":"solvers/solvers/#LinearSolve.RFLUFactorization","page":"Linear System Solvers","title":"LinearSolve.RFLUFactorization","text":"RFLUFactorization() \n\nA fast pure Julia LU-factorization implementation using RecursiveFactorization.jl. This is by far the fastest LU-factorization implementation, usually outperforming OpenBLAS and MKL for smaller matrices (<500x500), but currently optimized only for Base Array with Float32 or Float64.   Additional optimization for complex matrices is in the works.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#Base.LinearAlgebra","page":"Linear System Solvers","title":"Base.LinearAlgebra","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"These overloads tend to work for many array types, such as CuArrays for GPU-accelerated solving, using the overloads provided by the respective packages. Given that this can be customized per-package, details given below describe a subset of important arrays (Matrix, SparseMatrixCSC, CuMatrix, etc.)","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"LUFactorization\nGenericLUFactorization\nQRFactorization\nSVDFactorization\nCholeskyFactorization\nBunchKaufmanFactorization\nCHOLMODFactorization\nNormalCholeskyFactorization\nNormalBunchKaufmanFactorization","category":"page"},{"location":"solvers/solvers/#LinearSolve.LUFactorization","page":"Linear System Solvers","title":"LinearSolve.LUFactorization","text":"LUFactorization(pivot=LinearAlgebra.RowMaximum())\n\nJulia's built in lu. Equivalent to calling lu!(A)\n\nOn dense matrices, this uses the current BLAS implementation of the user's computer,\n\nwhich by default is OpenBLAS but will use MKL if the user does using MKL in their system.\n\nOn sparse matrices, this will use UMFPACK from SuiteSparse. Note that this will not\n\ncache the symbolic factorization.\n\nOn CuMatrix, it will use a CUDA-accelerated LU from CuSolver.\nOn BandedMatrix and BlockBandedMatrix, it will use a banded LU.\n\nPositional Arguments\n\npivot: The choice of pivoting. Defaults to LinearAlgebra.RowMaximum(). The other choice is LinearAlgebra.NoPivot().\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.GenericLUFactorization","page":"Linear System Solvers","title":"LinearSolve.GenericLUFactorization","text":"GenericLUFactorization(pivot=LinearAlgebra.RowMaximum())\n\nJulia's built in generic LU factorization. Equivalent to calling LinearAlgebra.generic_lufact!. Supports arbitrary number types but does not achieve as good scaling as BLAS-based LU implementations. Has low overhead and is good for small matrices.\n\nPositional Arguments\n\npivot: The choice of pivoting. Defaults to LinearAlgebra.RowMaximum(). The other choice is LinearAlgebra.NoPivot().\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.QRFactorization","page":"Linear System Solvers","title":"LinearSolve.QRFactorization","text":"QRFactorization(pivot=LinearAlgebra.NoPivot(),blocksize=16)\n\nJulia's built in qr. Equivalent to calling qr!(A).\n\nOn dense matrices, this uses the current BLAS implementation of the user's computer\n\nwhich by default is OpenBLAS but will use MKL if the user does using MKL in their system.\n\nOn sparse matrices, this will use SPQR from SuiteSparse\nOn CuMatrix, it will use a CUDA-accelerated QR from CuSolver.\nOn BandedMatrix and BlockBandedMatrix, it will use a banded QR.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.SVDFactorization","page":"Linear System Solvers","title":"LinearSolve.SVDFactorization","text":"SVDFactorization(full=false,alg=LinearAlgebra.DivideAndConquer())\n\nJulia's built in svd. Equivalent to svd!(A).\n\nOn dense matrices, this uses the current BLAS implementation of the user's computer\n\nwhich by default is OpenBLAS but will use MKL if the user does using MKL in their system.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.CholeskyFactorization","page":"Linear System Solvers","title":"LinearSolve.CholeskyFactorization","text":"CholeskyFactorization(; pivot = nothing, tol = 0.0, shift = 0.0, perm = nothing)\n\nJulia's built in cholesky. Equivalent to calling cholesky!(A).\n\nKeyword Arguments\n\npivot: defaluts to NoPivot, can also be RowMaximum.\ntol: the tol argument in CHOLMOD. Only used for sparse matrices.\nshift: the shift argument in CHOLMOD. Only used for sparse matrices.\nperm: the perm argument in CHOLMOD. Only used for sparse matrices.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.BunchKaufmanFactorization","page":"Linear System Solvers","title":"LinearSolve.BunchKaufmanFactorization","text":"BunchKaufmanFactorization(; rook = false)\n\nJulia's built in bunchkaufman. Equivalent to calling bunchkaufman(A). Only for Symmetric matrices.\n\nKeyword Arguments\n\nrook: whether to perform rook pivoting. Defaults to false.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.CHOLMODFactorization","page":"Linear System Solvers","title":"LinearSolve.CHOLMODFactorization","text":"CHOLMODFactorization(; shift = 0.0, perm = nothing)\n\nA wrapper of CHOLMOD's polyalgorithm, mixing Cholesky factorization and ldlt. Tries cholesky for performance and retries ldlt if conditioning causes Cholesky to fail.\n\nOnly supports sparse matrices.\n\nKeyword Arguments\n\nshift: the shift argument in CHOLMOD. \nperm: the perm argument in CHOLMOD\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.NormalCholeskyFactorization","page":"Linear System Solvers","title":"LinearSolve.NormalCholeskyFactorization","text":"NormalCholeskyFactorization(pivot = RowMaximum())\n\nA fast factorization which uses a Cholesky factorization on A * A'. Can be much faster than LU factorization, but is not as numerically stable and thus should only be applied to well-conditioned matrices.\n\nPositional Arguments\n\npivot: Defaults to RowMaximum(), but can be NoPivot()\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.NormalBunchKaufmanFactorization","page":"Linear System Solvers","title":"LinearSolve.NormalBunchKaufmanFactorization","text":"NormalBunchKaufmanFactorization(rook = false)\n\nA fast factorization which uses a BunchKaufman factorization on A * A'. Can be much faster than LU factorization, but is not as numerically stable and thus should only be applied to well-conditioned matrices.\n\nPositional Arguments\n\nrook: whether to perform rook pivoting. Defaults to false.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.jl","page":"Linear System Solvers","title":"LinearSolve.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"LinearSolve.jl contains some linear solvers built in for specailized cases.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"SimpleLUFactorization\nDiagonalFactorization","category":"page"},{"location":"solvers/solvers/#LinearSolve.SimpleLUFactorization","page":"Linear System Solvers","title":"LinearSolve.SimpleLUFactorization","text":"SimpleLUFactorization(pivot::Bool = true)\n\nA simple LU-factorization implementation without BLAS. Fast for small matrices.\n\nPositional Arguments\n\npivot::Bool: whether to perform pivoting. Defaults to true\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.DiagonalFactorization","page":"Linear System Solvers","title":"LinearSolve.DiagonalFactorization","text":"DiagonalFactorization()\n\nA special implementation only for solving Diagonal matrices fast.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#FastLapackInterface.jl","page":"Linear System Solvers","title":"FastLapackInterface.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"FastLapackInterface.jl is a package that allows for a lower-level interface to the LAPACK calls to allow for preallocating workspaces to decrease the overhead of the wrappers. LinearSolve.jl provides a wrapper to these routines in a way where an initialized solver has a non-allocating LU factorization. In theory, this post-initialized solve should always be faster than the Base.LinearAlgebra version.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"FastLUFactorization\nFastQRFactorization","category":"page"},{"location":"solvers/solvers/#LinearSolve.FastLUFactorization","page":"Linear System Solvers","title":"LinearSolve.FastLUFactorization","text":"FastLUFactorization() \n\nThe FastLapackInterface.jl version of the LU factorization. Notably, this version does not allow for choice of pivoting method.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.FastQRFactorization","page":"Linear System Solvers","title":"LinearSolve.FastQRFactorization","text":"FastQRFactorization() \n\nThe FastLapackInterface.jl version of the QR factorization.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#SuiteSparse.jl","page":"Linear System Solvers","title":"SuiteSparse.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"KLUFactorization\nUMFPACKFactorization","category":"page"},{"location":"solvers/solvers/#LinearSolve.KLUFactorization","page":"Linear System Solvers","title":"LinearSolve.KLUFactorization","text":"KLUFactorization(;reuse_symbolic=true, check_pattern=true)\n\nA fast sparse LU-factorization which specializes on sparsity patterns with “less structure”.\n\nnote: Note\nBy default, the SuiteSparse.jl are implemented for efficiency by caching the symbolic factorization. I.e., if set_A is used, it is expected that the new A has the same sparsity pattern as the previous A. If this algorithm is to be used in a context where that assumption does not hold, set reuse_symbolic=false.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.UMFPACKFactorization","page":"Linear System Solvers","title":"LinearSolve.UMFPACKFactorization","text":"UMFPACKFactorization(;reuse_symbolic=true, check_pattern=true)\n\nA fast sparse multithreaded LU-factorization which specializes on sparsity  patterns with “more structure”.\n\nnote: Note\nBy default, the SuiteSparse.jl are implemented for efficiency by caching the symbolic factorization. I.e., if set_A is used, it is expected that the new A has the same sparsity pattern as the previous A. If this algorithm is to be used in a context where that assumption does not hold, set reuse_symbolic=false.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#Sparspak.jl","page":"Linear System Solvers","title":"Sparspak.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"SparspakFactorization","category":"page"},{"location":"solvers/solvers/#LinearSolve.SparspakFactorization","page":"Linear System Solvers","title":"LinearSolve.SparspakFactorization","text":"SparspakFactorization(reuse_symbolic = true)\n\nThis is the translation of the well-known sparse matrix software Sparspak (Waterloo Sparse Matrix Package), solving large sparse systems of linear algebraic equations. Sparspak is composed of the subroutines from the book \"Computer Solution of Large Sparse Positive Definite Systems\" by Alan George and Joseph Liu. Originally written in Fortran 77, later rewritten in Fortran 90. Here is the software translated into Julia.\n\nThe Julia rewrite is released  under the MIT license with an express permission from the authors of the Fortran package. The package uses multiple dispatch to route around standard BLAS routines in the case e.g. of arbitrary-precision floating point numbers or ForwardDiff.Dual. This e.g. allows for Automatic Differentiation (AD) of a sparse-matrix solve.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#Krylov.jl","page":"Linear System Solvers","title":"Krylov.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"KrylovJL_CG\nKrylovJL_MINRES\nKrylovJL_GMRES\nKrylovJL_BICGSTAB\nKrylovJL_LSMR\nKrylovJL_CRAIGMR\nKrylovJL","category":"page"},{"location":"solvers/solvers/#LinearSolve.KrylovJL_CG","page":"Linear System Solvers","title":"LinearSolve.KrylovJL_CG","text":"KrylovJL_CG(args...;  kwargs...)\n\nA generic CG implementation for Hermitian and positive definite linear systems\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.KrylovJL_MINRES","page":"Linear System Solvers","title":"LinearSolve.KrylovJL_MINRES","text":"KrylovJL_MINRES(args...;  kwargs...)\n\nA generic MINRES implementation for Hermitian linear systems\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.KrylovJL_GMRES","page":"Linear System Solvers","title":"LinearSolve.KrylovJL_GMRES","text":"KrylovJL_GMRES(args...;  gmres_restart = 0, window = 0, kwargs...)\n\nA generic GMRES implementation for square non-Hermitian linear systems\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.KrylovJL_BICGSTAB","page":"Linear System Solvers","title":"LinearSolve.KrylovJL_BICGSTAB","text":"KrylovJL_BICGSTAB(args...;  kwargs...)\n\nA generic BICGSTAB implementation for square non-Hermitian linear systems\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.KrylovJL_LSMR","page":"Linear System Solvers","title":"LinearSolve.KrylovJL_LSMR","text":"KrylovJL_LSMR(args...;  kwargs...)\n\nA generic LSMR implementation for least-squares problems\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.KrylovJL_CRAIGMR","page":"Linear System Solvers","title":"LinearSolve.KrylovJL_CRAIGMR","text":"KrylovJL_CRAIGMR(args...;  kwargs...)\n\nA generic CRAIGMR implementation for least-norm problems\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.KrylovJL","page":"Linear System Solvers","title":"LinearSolve.KrylovJL","text":"KrylovJL(args...; KrylovAlg = Krylov.gmres!,\n         Pl = nothing, Pr = nothing,\n         gmres_restart = 0, window = 0,\n         kwargs...)\n\nA generic wrapper over the Krylov.jl krylov-subspace iterative solvers.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#Pardiso.jl","page":"Linear System Solvers","title":"Pardiso.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing this solver requires adding the package Pardiso.jl, i.e. using Pardiso","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"MKLPardisoFactorize\nMKLPardisoIterate\nLinearSolve.PardisoJL","category":"page"},{"location":"solvers/solvers/#LinearSolve.MKLPardisoFactorize","page":"Linear System Solvers","title":"LinearSolve.MKLPardisoFactorize","text":"```julia MKLPardisoFactorize(; nprocs::Union{Int, Nothing} = nothing,                     matrix_type = nothing,                     iparm::Union{Vector{Tuple{Int, Int}}, Nothing} = nothing,                     dparm::Union{Vector{Tuple{Int, Int}}, Nothing} = nothing)\n\nA sparse factorization method using MKL Pardiso.\n\nnote: Note\nUsing this solver requires adding the package Pardiso.jl, i.e. using Pardiso\n\nKeyword Arguments\n\nFor the definition of the keyword arguments, see the Pardiso.jl documentation. All values default to nothing and the solver internally determines the values given the input types, and these keyword arguments are only for overriding the default handling process. This should not be required by most users.\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.MKLPardisoIterate","page":"Linear System Solvers","title":"LinearSolve.MKLPardisoIterate","text":"MKLPardisoIterate(; nprocs::Union{Int, Nothing} = nothing,\n                    matrix_type = nothing,\n                    iparm::Union{Vector{Tuple{Int, Int}}, Nothing} = nothing,\n                    dparm::Union{Vector{Tuple{Int, Int}}, Nothing} = nothing)\n\nA mixed factorization+iterative method using MKL Pardiso.\n\nnote: Note\nUsing this solver requires adding the package Pardiso.jl, i.e. using Pardiso\n\nKeyword Arguments\n\nFor the definition of the keyword arguments, see the Pardiso.jl documentation. All values default to nothing and the solver internally determines the values given the input types, and these keyword arguments are only for overriding the default handling process. This should not be required by most users.\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#CUDA.jl","page":"Linear System Solvers","title":"CUDA.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Note that CuArrays are supported by GenericFactorization in the “normal” way. The following are non-standard GPU factorization routines.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing this solver requires adding the package CUDA.jl, i.e. using CUDA","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"CudaOffloadFactorization","category":"page"},{"location":"solvers/solvers/#LinearSolve.CudaOffloadFactorization","page":"Linear System Solvers","title":"LinearSolve.CudaOffloadFactorization","text":"CudaOffloadFactorization()\n\nAn offloading technique used to GPU-accelerate CPU-based computations.  Requires a sufficiently large A to overcome the data transfer costs.\n\nnote: Note\nUsing this solver requires adding the package CUDA.jl, i.e. using CUDA\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#IterativeSolvers.jl","page":"Linear System Solvers","title":"IterativeSolvers.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing these solvers requires adding the package IterativeSolvers.jl, i.e. using IterativeSolvers","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"IterativeSolversJL_CG\nIterativeSolversJL_GMRES\nIterativeSolversJL_BICGSTAB\nIterativeSolversJL_MINRES\nIterativeSolversJL","category":"page"},{"location":"solvers/solvers/#LinearSolve.IterativeSolversJL_CG","page":"Linear System Solvers","title":"LinearSolve.IterativeSolversJL_CG","text":"IterativeSolversJL_CG(args...; Pl = nothing, Pr = nothing, kwargs...)\n\nA wrapper over the IterativeSolvers.jl CG.\n\nnote: Note\nUsing this solver requires adding the package IterativeSolvers.jl, i.e. using IterativeSolvers\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.IterativeSolversJL_GMRES","page":"Linear System Solvers","title":"LinearSolve.IterativeSolversJL_GMRES","text":"IterativeSolversJL_GMRES(args...; Pl = nothing, Pr = nothing, gmres_restart=0, kwargs...)\n\nA wrapper over the IterativeSolvers.jl GMRES.\n\nnote: Note\nUsing this solver requires adding the package IterativeSolvers.jl, i.e. using IterativeSolvers\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.IterativeSolversJL_BICGSTAB","page":"Linear System Solvers","title":"LinearSolve.IterativeSolversJL_BICGSTAB","text":"IterativeSolversJL_BICGSTAB(args...; Pl = nothing, Pr = nothing, kwargs...)\n\nA wrapper over the IterativeSolvers.jl BICGSTAB.\n\nnote: Note\nUsing this solver requires adding the package IterativeSolvers.jl, i.e. using IterativeSolvers\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.IterativeSolversJL_MINRES","page":"Linear System Solvers","title":"LinearSolve.IterativeSolversJL_MINRES","text":"IterativeSolversJL_MINRES(args...; Pl = nothing, Pr = nothing, kwargs...)\n\nA wrapper over the IterativeSolvers.jl MINRES.\n\nnote: Note\nUsing this solver requires adding the package IterativeSolvers.jl, i.e. using IterativeSolvers\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.IterativeSolversJL","page":"Linear System Solvers","title":"LinearSolve.IterativeSolversJL","text":"IterativeSolversJL(args...;\n                   generate_iterator = IterativeSolvers.gmres_iterable!,\n                   Pl = nothing, Pr = nothing,\n                   gmres_restart = 0, kwargs...)\n\nA generic wrapper over the IterativeSolvers.jl solvers.\n\nnote: Note\nUsing this solver requires adding the package IterativeSolvers.jl, i.e. using IterativeSolvers\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#KrylovKit.jl","page":"Linear System Solvers","title":"KrylovKit.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing these solvers requires adding the package KrylovKit.jl, i.e. using KrylovKit","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"KrylovKitJL_CG\nKrylovKitJL_GMRES\nKrylovKitJL","category":"page"},{"location":"solvers/solvers/#LinearSolve.KrylovKitJL_CG","page":"Linear System Solvers","title":"LinearSolve.KrylovKitJL_CG","text":"KrylovKitJL_CG(args...; Pl = nothing, Pr = nothing, kwargs...)\n\nA generic CG implementation for Hermitian and positive definite linear systems\n\nnote: Note\nUsing this solver requires adding the package KrylovKit.jl, i.e. using KrylovKit    \n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.KrylovKitJL_GMRES","page":"Linear System Solvers","title":"LinearSolve.KrylovKitJL_GMRES","text":"KrylovKitJL_GMRES(args...; Pl = nothing, Pr = nothing, gmres_restart = 0, kwargs...)\n\nA generic GMRES implementation.\n\nnote: Note\nUsing this solver requires adding the package KrylovKit.jl, i.e. using KrylovKit\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.KrylovKitJL","page":"Linear System Solvers","title":"LinearSolve.KrylovKitJL","text":"KrylovKitJL(args...; KrylovAlg = Krylov.gmres!, kwargs...)\n\nA generic iterative solver implementation allowing the choice of KrylovKit.jl solvers.\n\nnote: Note\nUsing this solver requires adding the package KrylovKit.jl, i.e. using KrylovKit\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#HYPRE.jl","page":"Linear System Solvers","title":"HYPRE.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing HYPRE solvers requires Julia version 1.9 or higher, and that the package HYPRE.jl is installed.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"HYPREAlgorithm","category":"page"},{"location":"solvers/solvers/#LinearSolve.HYPREAlgorithm","page":"Linear System Solvers","title":"LinearSolve.HYPREAlgorithm","text":"HYPREAlgorithm(solver; Pl = nothing)\n\nHYPRE.jl is an interface to hypre and provide iterative solvers and preconditioners for sparse linear systems. It is mainly developed for large multi-process distributed problems (using MPI), but can also be used for single-process problems with Julias standard sparse matrices.\n\nIf you need more fine-grained control over the solver/preconditioner options you can alternatively pass an already created solver to HYPREAlgorithm (and to the Pl keyword argument). See HYPRE.jl docs for how to set up solvers with specific options.\n\nnote: Note\nUsing HYPRE solvers requires Julia version 1.9 or higher, and that the package HYPRE.jl is installed.\n\nPositional Arguments\n\nThe single positional argument solver has the following choices:\n\nHYPRE.BiCGSTAB\nHYPRE.BoomerAMG\nHYPRE.FlexGMRES\nHYPRE.GMRES\nHYPRE.Hybrid\nHYPRE.ILU\nHYPRE.ParaSails (as preconditioner only)\nHYPRE.PCG\n\nKeyword Arguments\n\nPl: A choice of left preconditioner.\n\nExample\n\nFor example, to use HYPRE.PCG as the solver, with HYPRE.BoomerAMG as the preconditioner, the algorithm should be defined as follows:\n\nA, b = setup_system(...)\nprob = LinearProblem(A, b)\nalg = HYPREAlgorithm(HYPRE.PCG)\nprec = HYPRE.BoomerAMG\nsol = solve(prob, alg; Pl = prec)\n\n\n\n\n\n","category":"type"},{"location":"#LinearSolve.jl:-High-Performance-Unified-Linear-Solvers","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"","category":"section"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"LinearSolve.jl is a unified interface for the linear solving packages of Julia. It interfaces with other packages of the Julia ecosystem to make it easy to test alternative solver packages and pass small types to control algorithm swapping. It also interfaces with the ModelingToolkit.jl world of symbolic modeling to allow for automatically generating high-performance code.","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"Performance is key: the current methods are made to be highly performant on scalar and statically sized small problems, with options for large-scale systems. If you run into any performance issues, please file an issue.","category":"page"},{"location":"#Installation","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"Installation","text":"","category":"section"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"To install LinearSolve.jl, use the Julia package manager:","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"using Pkg\nPkg.add(\"LinearSolve\")","category":"page"},{"location":"#Contributing","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"Contributing","text":"","category":"section"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nSee the SciML Style Guide for common coding practices and other style decisions.\nThere are a few community forums:\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Slack\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Zulip\nOn the Julia Discourse forums\nSee also SciML Community page","category":"page"},{"location":"#Roadmap","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"Roadmap","text":"","category":"section"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"Wrappers for every linear solver in the Julia language is on the roadmap. If there are any important ones that are missing that you would like to see added, please open an issue. The current algorithms should support automatic differentiation. Pre-defined preconditioners would be a welcome addition.","category":"page"},{"location":"#Reproducibility","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"</details>","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"</details>","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"using Pkg # hide\nPkg.status(; mode = PKGMODE_MANIFEST) # hide","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"</details>","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"You can also download the \n<a href=\"","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"using TOML\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n       \"/assets/Manifest.toml\"","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"\">manifest</a> file and the\n<a href=\"","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"using TOML\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n       \"/assets/Project.toml\"","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"\">project</a> file.","category":"page"},{"location":"tutorials/caching_interface/#Linear-Solve-with-Caching-Interface","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"","category":"section"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"Often, one may want to cache information that is reused between different linear solves. For example, if one is going to perform:","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"A \\ b1\nA \\ b2","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"then it would be more efficient to LU-factorize one time and reuse the factorization:","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"lu!(A)\nA \\ b1\nA \\ b2","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"LinearSolve.jl's caching interface automates this process to use the most efficient means of solving and resolving linear systems. To do this with LinearSolve.jl, you simply init a cache, solve, replace b, and solve again. This looks like:","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"using LinearSolve\n\nn = 4\nA = rand(n, n)\nb1 = rand(n);\nb2 = rand(n);\nprob = LinearProblem(A, b1)\n\nlinsolve = init(prob)\nsol1 = solve!(linsolve)","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"linsolve.b = b2\nsol2 = solve!(linsolve)\n\nsol2.u","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"Then refactorization will occur when a new A is given:","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"A2 = rand(n, n)\nlinsolve.A = A2\nsol3 = solve!(linsolve)\n\nsol3.u","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"The factorization occurs on the first solve, and it stores the factorization in the cache. You can retrieve this cache via sol.cache, which is the same object as the init, but updated to know not to re-solve the factorization.","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"The advantage of course with using LinearSolve.jl in this form is that it is efficient while being agnostic to the linear solver. One can easily swap in iterative solvers, sparse solvers, etc. and it will do all the tricks like caching the symbolic factorization if the sparsity pattern is unchanged.","category":"page"}]
}
