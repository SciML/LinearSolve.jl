var documenterSearchIndex = {"docs":
[{"location":"tutorials/accelerating_choices/#Accelerating-your-Linear-Solves","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"","category":"section"},{"location":"tutorials/accelerating_choices/","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"note: Note\nThis section is essential if you wish to achieve maximum performance with LinearSolve.jl, especially on v7 and above. Please ensure the tips of this section are adhered to when optimizing code and benchmarking.","category":"page"},{"location":"tutorials/accelerating_choices/","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"Great, you've learned how to use LinearSolve.jl and you're using it daily, either directly or through other SciML libraries, and you want to improve your performance. How can this be done? While it might seem at first like a hopeless endeavour, \"A\\b uses a BLAS library and so it's already highly optimized C code\", it turns out there are many factors you need to consider to squeeze out the last 10x of performance. And yes, it can be about a factor of 10 in some scenarios, so let's dive in.","category":"page"},{"location":"tutorials/accelerating_choices/#Understanding-Performance-of-Dense-Linear-Solves","page":"Accelerating your Linear Solves","title":"Understanding Performance of Dense Linear Solves","text":"","category":"section"},{"location":"tutorials/accelerating_choices/","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"The performance of dense linear solvers is highly dependent on the size of the matrix and the chosen architecture to run on, i.e. the CPU. This issue gathered benchmark data from many different users and is summarized in the following graphs:","category":"page"},{"location":"tutorials/accelerating_choices/","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"(Image: Dense Linear Solve Benchmarks)","category":"page"},{"location":"tutorials/accelerating_choices/","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"Now one thing that is immediate is for example that AppleAccelerate generally does well on Apple M-series chips, MKL generally does well on Intel, etc. And we know this in LinearSolve.jl, in fact we automatically default to different BLASes based on the CPU architecture already as part of the design! So that covers most of the variation, but there are a few major tips to note when fine tuning the results to your system:","category":"page"},{"location":"tutorials/accelerating_choices/","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"One of the best methods for size 150x150 matrices and below is RecursiveFactorization.jl. This is a pure Julia BLAS system, but it has a high load time overhead, and thus as of v7 it's no longer loaded by default! Thus if your matrices are in this range and you would value better run times at the cost of compile and load times, it is recommended you add using RecursiveFactorization. The defaulting algorithm will then consider it in its list and will automatically (in an architecture-specific way) insert it as it feels necessary.\nOne of the major factors that can inhibit BLAS performance on LU factorization is multithreading. In many of these plots you can see a giant dip in GFLOPs (higher is better) when a certain size threshold is hit. This is because, for the number of chosen threads, there was not enough work and thus when the threading threshold is hit you get a hit to the performance due to the added overhead. The threading performance can be a per-system thing, and it can be greatly influenced by the number of cores on your system and the number of threads you allow. Thus for example, OpenBLAS' LU factorization seems to generally be really bad at guessing the thread switch point for CPUs with really high core/thread counts. If this is the case, you may want to investigate decreasing your number of BLAS threads, i.e. via BLAS.set_num_threads(i). Note that RecursiveFactorization.jl uses your Julia thread pool instead of the BLAS threads.\nThe switch points between algorithms can be fairly inexact. LinearSolve.jl tried to keep a tab on where they are per platform and keep updated, but it can be a moving battle. You may be able to eek out some performance by testing between the various options on your platform, i.e. RFLUFactorization vs LUFactorization vs AppleAccelerateLUFactorization (M-series) vs MKLFactorization (X86) and hardcoding the choice for your problem if the default did not make the right guess.","category":"page"},{"location":"tutorials/accelerating_choices/","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"warn: Warn\nAs noted, RecursiveFactorization.jl is one of the fastest linear solvers for smaller dense matrices but requires using RecursiveFactorization in order to be used in the default solver setups! Thus it's recommended that any optimized code or benchmarks sets this up.","category":"page"},{"location":"tutorials/accelerating_choices/#Understanding-Performance-of-Sparse-Linear-Solves","page":"Accelerating your Linear Solves","title":"Understanding Performance of Sparse Linear Solves","text":"","category":"section"},{"location":"tutorials/accelerating_choices/","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"Sparse linear solvers are not as dependent on the CPU but highly dependent on the problem that is being solved. For example, this is for a 1D laplacian vs a 3D laplacian, changing N to make smaller and bigger versions:","category":"page"},{"location":"tutorials/accelerating_choices/","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"(Image: Sparse Linear Solve Benchmarks)","category":"page"},{"location":"tutorials/accelerating_choices/","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"Notice that the optimal linear solver changes based on problem (i.e. sparsity pattern) and size. LinearSolve.jl just uses a very simple \"if small then use KLU and if large use UMFPACK\", which is validated by this plot, but leaves a lot to be desired. In particular, the following rules should be thought about:","category":"page"},{"location":"tutorials/accelerating_choices/","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"Pardiso is a great solver, you should try using Pardiso and using MKLPardiso() in many scenarios.\nThe more structured a sparsity pattern is, the worse KLU is in comparison to the other algorithms.\nA Krylov subspace method with proper preconditioning will be better than direct solvers when the matrices get large enough. You could always precondition a sparse matrix with iLU as an easy choice, though the tolerance would need to be tuned in a problem-specific way. Please see the preconditioenrs page for more information on defining and using preconditioners.","category":"page"},{"location":"tutorials/accelerating_choices/","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"note: Note\nUMFPACK does better when the BLAS is not OpenBLAS. Try using MKL on Intel and AMD Ryzen platforms and UMPACK will be faster! LinearSolve.jl cannot default to this as this changes global settings and thus only defaults to MKL locally, and thus cannot change the setting within UMFPACK.","category":"page"},{"location":"tutorials/gpu/#GPU-Accelerated-Linear-Solving-in-Julia","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"","category":"section"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"LinearSolve.jl provides two ways to GPU accelerate linear solves:","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"Offloading: offloading takes a CPU-based problem and automatically transforms it into a GPU-based problem in the background, and returns the solution on CPU. Thus using offloading requires no change on the part of the user other than to choose an offloading solver.\nArray type interface: the array type interface requires that the user defines the LinearProblem using an AbstractGPUArray type and chooses an appropriate solver (or uses the default solver). The solution will then be returned as a GPU array type.","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"The offloading approach has the advantage of being simpler and requiring no change to existing CPU code, while having the disadvantage of having more overhead. In the following sections we will demonstrate how to use each of the approaches.","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"warn: Warn\nGPUs are not always faster! Your matrices need to be sufficiently large in order for GPU accelerations to actually be faster. For offloading it's around 1,000 x 1,000 matrices and for Array type interface it's around 100 x 100. For sparse matrices, it is highly dependent on the sparsity pattern and the amount of fill-in.","category":"page"},{"location":"tutorials/gpu/#GPU-Offloading","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Offloading","text":"","category":"section"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"GPU offloading is simple as it's done simply by changing the solver algorithm. Take the example from the start of the documentation:","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"import LinearSolve as LS\n\nA = rand(4, 4)\nb = rand(4)\nprob = LS.LinearProblem(A, b)\nsol = LS.solve(prob)\nsol.u","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"This computation can be moved to the GPU by the following:","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"using CUDA # Add the GPU library\nsol = LS.solve(prob, LS.CudaOffloadFactorization())\nsol.u","category":"page"},{"location":"tutorials/gpu/#GPUArray-Interface","page":"GPU-Accelerated Linear Solving in Julia","title":"GPUArray Interface","text":"","category":"section"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"For more manual control over the factorization setup, you can use the GPUArray interface, the most common instantiation being CuArray for CUDA-based arrays on NVIDIA GPUs. To use this, we simply send the matrix A and the value b over to the GPU and solve:","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"using CUDA\n\nA = rand(4, 4) |> cu\nb = rand(4) |> cu\nprob = LS.LinearProblem(A, b)\nsol = LS.solve(prob)\nsol.u","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"4-element CuArray{Float32, 1, CUDA.DeviceMemory}:\n -27.02665\n  16.338171\n -77.650116\n 106.335686","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"Notice that the solution is a CuArray, and thus one must use Array(sol.u) if you with to return it to the CPU. This setup does no automated memory transfers and will thus only move things to CPU on command.","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"warn: Warn\nMany GPU functionalities, such as CUDA.cu, have a built-in preference for Float32. Generally it is much faster to use 32-bit floating point operations on GPU than 64-bit operations, and thus this is generally the right choice if going to such platforms. However, this change in numerical precision needs to be accounted for in your mathematics as it could lead to instabilities. To disable this, use a constructor that is more specific about the bitsize, such as CuArray{Float64}(A). Additionally, preferring more stable factorization methods, such as LS.QRFactorization(), can improve the numerics in such cases.","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"Similarly to other use cases, you can choose the solver, for example:","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"sol = LS.solve(prob, LS.QRFactorization())","category":"page"},{"location":"tutorials/gpu/#Sparse-Matrices-on-GPUs","page":"GPU-Accelerated Linear Solving in Julia","title":"Sparse Matrices on GPUs","text":"","category":"section"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"Currently, sparse matrix computations on GPUs are only supported for CUDA. This is done using the CUDA.CUSPARSE sublibrary.","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"import LinearAlgebra as LA\nimport SparseArrays as SA\nimport CUDA\nT = Float32\nn = 100\nA_cpu = SA.sprand(T, n, n, 0.05) + LA.I\nx_cpu = zeros(T, n)\nb_cpu = rand(T, n)\n\nA_gpu_csr = CuSparseMatrixCSR(A_cpu)\nb_gpu = CuVector(b_cpu)","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"In order to solve such problems using a direct method, you must add CUDSS.jl. This looks like:","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"using CUDSS\nsol = LS.solve(prob, LS.LUFactorization())","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"note: Note\nFor now, CUDSS only supports CuSparseMatrixCSR type matrices.","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"For high-performance sparse LU factorization on GPUs, you can also use CUSOLVERRF.jl:","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"using CUSOLVERRF\nsol = LS.solve(prob, LS.CUSOLVERRFFactorization())","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"CUSOLVERRF provides access to NVIDIA's cusolverRF library, which offers significant  performance improvements for sparse LU factorization on GPUs. It supports both  :RF (default) and :KLU symbolic factorization methods, and can reuse symbolic  factorization for matrices with the same sparsity pattern:","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"# Use KLU for symbolic factorization\nsol = LS.solve(prob, LS.CUSOLVERRFFactorization(symbolic = :KLU))\n\n# Reuse symbolic factorization for better performance\nsol = LS.solve(prob, LS.CUSOLVERRFFactorization(reuse_symbolic = true))","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"note: Note\nCUSOLVERRF only supports Float64 element types with Int32 indices.","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"Note that KrylovJL methods also work with sparse GPU arrays:","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"sol = LS.solve(prob, LS.KrylovJL_GMRES())","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"Note that CUSPARSE also has some GPU-based preconditioners, such as a built-in ilu. However:","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"sol = LS.solve(\n    prob, LS.KrylovJL_GMRES(precs = (A, p) -> (CUDA.CUSPARSE.ilu02!(A, 'O'), LA.I)))","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"However, right now CUSPARSE is missing the right ldiv! implementation for this to work in general. See https://github.com/SciML/LinearSolve.jl/issues/341 for details.","category":"page"},{"location":"basics/LinearProblem/#Linear-Problems","page":"Linear Problems","title":"Linear Problems","text":"","category":"section"},{"location":"basics/LinearProblem/#SciMLBase.LinearProblem","page":"Linear Problems","title":"SciMLBase.LinearProblem","text":"Defines a linear system problem. Documentation Page: https://docs.sciml.ai/LinearSolve/stable/basics/LinearProblem/\n\nMathematical Specification of a Linear Problem\n\nConcrete LinearProblem\n\nTo define a LinearProblem, you simply need to give the AbstractMatrix A and an AbstractVector b which defines the linear system:\n\nAu = b\n\nMatrix-Free LinearProblem\n\nFor matrix-free versions, the specification of the problem is given by an operator A(u,p,t) which computes A*u, or in-place as A(du,u,p,t). These are specified via the AbstractSciMLOperator interface. For more details, see the SciMLBase Documentation.\n\nNote that matrix-free versions of LinearProblem definitions are not compatible with all solvers. To check a solver for compatibility, use the function needs_concrete_A(alg::AbstractLinearAlgorithm).\n\nProblem Type\n\nConstructors\n\nOptionally, an initial guess u₀ can be supplied which is used for iterative methods.\n\nLinearProblem{isinplace}(A,b,p=NullParameters();u0=nothing,kwargs...)\nLinearProblem(f::AbstractSciMLOperator,b,p=NullParameters();u0=nothing,kwargs...)\n\nisinplace optionally sets whether the function is in-place or not, i.e. whether the solvers are allowed to mutate. By default this is true for AbstractMatrix, and for AbstractSciMLOperators it matches the choice of the operator definition.\n\nParameters are optional, and if not given, then a NullParameters() singleton will be used, which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers.\n\nFields\n\nA: The representation of the linear operator.\nb: The right-hand side of the linear system.\np: The parameters for the problem. Defaults to NullParameters. Currently unused.\nu0: The initial condition used by iterative solvers.\nsymbolic_interface: An instance of SymbolicLinearInterface if the problem was generated by a symbolic backend.\nkwargs: The keyword arguments passed on to the solvers.\n\n\n\n\n\n","category":"type"},{"location":"advanced/custom/#custom","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"","category":"section"},{"location":"advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"Julia users are building a wide variety of applications in the SciML ecosystem, often requiring problem-specific handling of their linear solves. As existing solvers in LinearSolve.jl may not be optimally suited for novel applications, it is essential for the linear solve interface to be easily extendable by users. To that end, the linear solve algorithm LS.LinearSolveFunction() accepts a user-defined function for handling the solve. A user can pass in their custom linear solve function, say my_linsolve, to LS.LinearSolveFunction(). A contrived example of solving a linear system with a custom solver is below.","category":"page"},{"location":"advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"import LinearSolve as LS\nimport LinearAlgebra as LA\n\nfunction my_linsolve(A, b, u, p, newA, Pl, Pr, solverdata; verbose = true, kwargs...)\n    if verbose == true\n        println(\"solving Ax=b\")\n    end\n    u = A \\ b\n    return u\nend\n\nprob = LS.LinearProblem(LA.Diagonal(rand(4)), rand(4))\nalg = LS.LinearSolveFunction(my_linsolve)\nsol = LS.solve(prob, alg)\nsol.u","category":"page"},{"location":"advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"The inputs to the function are as follows:","category":"page"},{"location":"advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"u, the solution initialized as zero(b),\nA, the linear operator\nb, the right-hand-side\np, a set of parameters\nnewA, a Bool which is true if A has been modified since last solve\nPl, left-preconditioner\nPr, right-preconditioner\nsolverdata, solver cache set to nothing if solver hasn't been initialized\nkwargs, standard SciML keyword arguments such as verbose, maxiters, abstol, reltol","category":"page"},{"location":"advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"The function my_linsolve must accept the above specified arguments and modify them, and return the solution, u. As memory for u is already allocated, the user may choose to modify u in place as follows:","category":"page"},{"location":"advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"function my_linsolve!(A, b, u, p, newA, Pl, Pr, solverdata; verbose = true, kwargs...)\n    if verbose == true\n        println(\"solving Ax=b\")\n    end\n    u .= A \\ b # in place\n    return u\nend\n\nalg = LS.LinearSolveFunction(my_linsolve!)\nsol = LS.solve(prob, alg)\nsol.u","category":"page"},{"location":"basics/OperatorAssumptions/#assumptions","page":"Linear Solve Operator Assumptions","title":"Linear Solve Operator Assumptions","text":"","category":"section"},{"location":"basics/OperatorAssumptions/#LinearSolve.OperatorAssumptions","page":"Linear Solve Operator Assumptions","title":"LinearSolve.OperatorAssumptions","text":"OperatorAssumptions(issquare = nothing; condition::OperatorCondition.T = IllConditioned)\n\nSets the operator A assumptions used as part of the default algorithm\n\n\n\n\n\n","category":"type"},{"location":"basics/OperatorAssumptions/#LinearSolve.OperatorCondition","page":"Linear Solve Operator Assumptions","title":"LinearSolve.OperatorCondition","text":"OperatorCondition\n\nSpecifies the assumption of matrix conditioning for the default linear solver choices. Condition number is defined as the ratio of eigenvalues. The numerical stability of many linear solver algorithms can be dependent on the condition number of the matrix. The condition number can be computed as:\n\nusing LinearAlgebra\ncond(rand(100, 100))\n\nHowever, in practice this computation is very expensive and thus not possible for most practical cases. Therefore, OperatorCondition lets one share to LinearSolve the expected conditioning. The higher the expected condition number, the safer the algorithm needs to be and thus there is a trade-off between numerical performance and stability. By default the method assumes the operator may be ill-conditioned for the standard linear solvers to converge (such as LU-factorization), though more extreme ill-conditioning or well-conditioning could be the case and specified through this assumption.\n\n\n\n\n\n","category":"module"},{"location":"basics/OperatorAssumptions/#Condition-Number-Specifications","page":"Linear Solve Operator Assumptions","title":"Condition Number Specifications","text":"","category":"section"},{"location":"basics/OperatorAssumptions/#LinearSolve.OperatorCondition.IllConditioned","page":"Linear Solve Operator Assumptions","title":"LinearSolve.OperatorCondition.IllConditioned","text":"OperatorCondition.IllConditioned\n\nThe default assumption of LinearSolve. Assumes that the operator can have minor ill-conditioning and thus needs to use safe algorithms.\n\n\n\n\n\n","category":"constant"},{"location":"basics/OperatorAssumptions/#LinearSolve.OperatorCondition.VeryIllConditioned","page":"Linear Solve Operator Assumptions","title":"LinearSolve.OperatorCondition.VeryIllConditioned","text":"OperatorCondition.VeryIllConditioned\n\nAssumes that the operator can have fairly major ill-conditioning and thus the standard linear algebra algorithms cannot be used.\n\n\n\n\n\n","category":"constant"},{"location":"basics/OperatorAssumptions/#LinearSolve.OperatorCondition.SuperIllConditioned","page":"Linear Solve Operator Assumptions","title":"LinearSolve.OperatorCondition.SuperIllConditioned","text":"OperatorCondition.SuperIllConditioned\n\nAssumes that the operator can have fairly extreme ill-conditioning and thus the most stable algorithm is used.\n\n\n\n\n\n","category":"constant"},{"location":"basics/OperatorAssumptions/#LinearSolve.OperatorCondition.WellConditioned","page":"Linear Solve Operator Assumptions","title":"LinearSolve.OperatorCondition.WellConditioned","text":"OperatorCondition.WellConditioned\n\nAssumes that the operator can have fairly contained conditioning and thus the fastest algorithm is used.\n\n\n\n\n\n","category":"constant"},{"location":"basics/FAQ/#Frequently-Asked-Questions","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"","category":"section"},{"location":"basics/FAQ/#How-is-LinearSolve.jl-compared-to-just-using-normal-\\,-i.e.-A\\b?","page":"Frequently Asked Questions","title":"How is LinearSolve.jl compared to just using normal \\, i.e. A\\b?","text":"","category":"section"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Check out this video from JuliaCon 2022 which goes into detail on how and why LinearSolve.jl can be a more general and efficient interface.","category":"page"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Note that if \\ is good enough for you, great! We still tend to use \\ in the REPL all the time! However, if you're building a package, you may want to consider using LinearSolve.jl for the improved efficiency and ability to choose solvers.","category":"page"},{"location":"basics/FAQ/#I'm-seeing-some-dynamic-dispatches-in-the-default-algorithm-choice,-how-do-I-reduce-that?","page":"Frequently Asked Questions","title":"I'm seeing some dynamic dispatches in the default algorithm choice, how do I reduce that?","text":"","category":"section"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Make sure you set the OperatorAssumptions to get the full performance, especially the issquare choice as otherwise that will need to be determined at runtime.","category":"page"},{"location":"basics/FAQ/#I-found-a-faster-algorithm-that-can-be-used-than-what-LinearSolve.jl-chose?","page":"Frequently Asked Questions","title":"I found a faster algorithm that can be used than what LinearSolve.jl chose?","text":"","category":"section"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"What assumptions are made as part of your method? If your method only works on well-conditioned operators, then make sure you set the WellConditioned assumption in the assumptions. See the OperatorAssumptions page for more details. If using the right assumptions does not improve the performance to the expected state, please open an issue and we will improve the default algorithm.","category":"page"},{"location":"basics/FAQ/#Python's-NumPy/SciPy-just-calls-fast-Fortran/C-code,-why-would-LinearSolve.jl-be-any-better?","page":"Frequently Asked Questions","title":"Python's NumPy/SciPy just calls fast Fortran/C code, why would LinearSolve.jl be any better?","text":"","category":"section"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"This is addressed in the JuliaCon 2022 video. This happens in a few ways:","category":"page"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"The Fortran/C code that NumPy/SciPy uses is actually slow. It's OpenBLAS, a library developed in part by the Julia Lab back in 2012 as a fast open source BLAS implementation. Many open source environments now use this build, including many R distributions. However, the Julia Lab has greatly improved its ability to generate optimized SIMD in platform-specific ways. This, and improved multithreading support (OpenBLAS's multithreading is rather slow), has led to pure Julia-based BLAS implementations which the lab now works on. This includes RecursiveFactorization.jl which generally outperforms OpenBLAS by 2x-10x depending on the platform. It even outperforms MKL for small matrices (<100). LinearSolve.jl uses RecursiveFactorization.jl by default sometimes, but switches to BLAS when it would be faster (in a platform and matrix-specific way).\nStandard approaches to handling linear solves re-allocate the pivoting vector each time. This leads to GC pauses that can slow down calculations. LinearSolve.jl has proper caches for fully preallocated no-GC workflows.\nLinearSolve.jl makes many other optimizations, like factorization reuse and symbolic factorization reuse, automatic. Many of these optimizations are not even possible from the high-level APIs of things like Python's major libraries and MATLAB.\nLinearSolve.jl has a much more extensive set of sparse matrix solvers, which is why you see a major difference (2x-10x) for sparse matrices. Which sparse matrix solver between KLU, UMFPACK, Pardiso, etc. is optimal depends a lot on matrix sizes, sparsity patterns, and threading overheads. LinearSolve.jl's heuristics handle these kinds of issues.","category":"page"},{"location":"basics/FAQ/#How-do-I-use-IterativeSolvers-solvers-with-a-weighted-tolerance-vector?","page":"Frequently Asked Questions","title":"How do I use IterativeSolvers solvers with a weighted tolerance vector?","text":"","category":"section"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"IterativeSolvers.jl computes the norm after the application of the left preconditioner. Thus, in order to use a vector tolerance weights, one can mathematically hack the system via the following formulation:","category":"page"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"import LinearSolve as LS\nimport LinearAlgebra as LA\n\nn = 2\nA = rand(n, n)\nb = rand(n)\n\nweights = [1e-1, 1]\nprecs = Returns((LS.InvPreconditioner(LA.Diagonal(weights)), LA.Diagonal(weights)))\n\nprob = LS.LinearProblem(A, b)\nsol = LS.solve(prob, LS.KrylovJL_GMRES(precs))\n\nsol.u","category":"page"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"If you want to use a “real” preconditioner under the norm weights, then one can use ComposePreconditioner to apply the preconditioner after the application of the weights like as follows:","category":"page"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"import LinearSolve as LS\nimport LinearAlgebra as LA\n\nn = 4\nA = rand(n, n)\nb = rand(n)\n\nweights = rand(n)\nrealprec = LA.lu(rand(n, n)) # some random preconditioner\nPl = LS.ComposePreconditioner(LS.InvPreconditioner(LA.Diagonal(weights)),\n    realprec)\nPr = LA.Diagonal(weights)\n\nprob = LS.LinearProblem(A, b)\nsol = LS.solve(prob, LS.KrylovJL_GMRES(precs = Returns((Pl, Pr))))","category":"page"},{"location":"advanced/developing/#Developing-New-Linear-Solvers","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"","category":"section"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"Developing new or custom linear solvers for the SciML interface can be done in one of two ways:","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"You can either create a completely new set of dispatches for init and solve.\nYou can extend LinearSolve.jl's internal mechanisms.","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"For developer ease, we highly recommend (2) as that will automatically make the caching API work. Thus, this is the documentation for how to do that.","category":"page"},{"location":"advanced/developing/#Developing-New-Linear-Solvers-with-LinearSolve.jl-Primitives","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers with LinearSolve.jl Primitives","text":"","category":"section"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"Let's create a new wrapper for a simple LU-factorization which uses only the basic machinery. A simplified version is:","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"struct MyLUFactorization{P} <: LinearSolve.SciMLLinearSolveAlgorithm end\n\nfunction LinearSolve.init_cacheval(\n        alg::MyLUFactorization, A, b, u, Pl, Pr, maxiters::Int, abstol, reltol,\n        verbose::Bool, assump::LinearSolve.OperatorAssumptions)\n    lu!(convert(AbstractMatrix, A))\nend\n\nfunction SciMLBase.solve!(cache::LinearSolve.LinearCache, alg::MyLUFactorization; kwargs...)\n    if cache.isfresh\n        A = cache.A\n        A = convert(AbstractMatrix, A)\n        fact = lu!(A)\n        cache.cacheval = fact\n        cache.isfresh = false\n    end\n    y = ldiv!(cache.u, cache.cacheval, cache.b)\n    SciMLBase.build_linear_solution(alg, y, nothing, cache)\nend","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"The way this works is as follows. LinearSolve.jl has a LinearCache that everything shares (this is what gives most of the ease of use). However, many algorithms need to cache their own things, and so there's one value cacheval that is for the algorithms to modify. The function:","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"init_cacheval(\n    alg::MyLUFactorization, A, b, u, Pl, Pr, maxiters, abstol, reltol, verbose, assump)","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"is what is called at init time to create the first cacheval. Note that this should match the type of the cache later used in solve as many algorithms, like those in OrdinaryDiffEq.jl, expect type-groundedness in the linear solver definitions. While there are cheaper ways to obtain this type for LU factorizations (specifically, ArrayInterface.lu_instance(A)), for a demonstration, this just performs an LU-factorization to get an LU{T, Matrix{T}} which it puts into the cacheval so it is typed for future use.","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"After the init_cacheval, the only thing left to do is to define SciMLBase.solve!(cache::LinearCache, alg::MyLUFactorization). Many algorithms may use a lazy matrix-free representation of the operator A. Thus, if the algorithm requires a concrete matrix, like LU-factorization does, the algorithm should convert(AbstractMatrix,cache.A). The flag cache.isfresh states whether A has changed since the last solve. Since we only need to factorize when A is new, the factorization part of the algorithm is done in a if cache.isfresh. cache.cacheval = fact; cache.isfresh = false puts the new factorization into the cache, so it's updated for future solves. Then y = ldiv!(cache.u, cache.cacheval, cache.b) performs the solve and a linear solution is returned via SciMLBase.build_linear_solution(alg,y,nothing,cache).","category":"page"},{"location":"basics/Preconditioners/#prec","page":"Preconditioners","title":"Preconditioners","text":"","category":"section"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"Many linear solvers can be accelerated by using what is known as a preconditioner, an approximation to the matrix inverse action which is cheap to evaluate. These can improve the numerical conditioning of the solver process and in turn improve the performance. LinearSolve.jl provides an interface for the definition of preconditioners which works with the wrapped iterative solver packages.","category":"page"},{"location":"basics/Preconditioners/#Using-Preconditioners","page":"Preconditioners","title":"Using Preconditioners","text":"","category":"section"},{"location":"basics/Preconditioners/#Mathematical-Definition","page":"Preconditioners","title":"Mathematical Definition","text":"","category":"section"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"A right preconditioner, P_r transforms the linear system Au = b into the form:","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"AP_r^-1(P_r u) = AP_r^-1y = b","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"which is solved for y, and then P_r u = y is solved for u. The left preconditioner, P_l, transforms the linear system into the form:","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"P_l^-1Au = P_l^-1b","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"A two-sided preconditioned system is of the form:","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"P_l^-1A P_r^-1 (P_r u) = P_l^-1b","category":"page"},{"location":"basics/Preconditioners/#Specifying-Preconditioners","page":"Preconditioners","title":"Specifying  Preconditioners","text":"","category":"section"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"One way to specify preconditioners uses the Pl and Pr  keyword arguments to init or solve: Pl for left and Pr for right preconditioner, respectively. By default, if no preconditioner is given, the preconditioner is assumed to be the identity I.","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"In the following, we will use a left sided diagonal (Jacobi) preconditioner.","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"import LinearSolve as LS\nimport LinearAlgebra as LA\nn = 4\n\nA = rand(n, n)\nb = rand(n)\n\nPl = LA.Diagonal(A)\n\nprob = LS.LinearProblem(A, b)\nsol = LS.solve(prob, LS.KrylovJL_GMRES(), Pl = Pl)\nsol.u","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"Alternatively, preconditioners can be specified via the  precs  argument to the constructor of an iterative solver specification. This argument shall deliver a factory method mapping A and a parameter p to a tuple (Pl,Pr) consisting a left and a right preconditioner.","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"import LinearSolve as LS\nimport LinearAlgebra as LA\nn = 4\n\nA = rand(n, n)\nb = rand(n)\n\nprob = LS.LinearProblem(A, b)\nsol = LS.solve(prob, LS.KrylovJL_GMRES(precs = (A, p) -> (LA.Diagonal(A), LA.I)))\nsol.u","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"This approach has the advantage that the specification of the preconditioner is possible without the knowledge of a concrete matrix A. It also allows to specify the preconditioner via a callable object and to  pass parameters to the constructor of the preconditioner instances. The example below also shows how to reuse the preconditioner once constructed for the subsequent solution of a modified problem.","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"import LinearSolve as LS\nimport LinearAlgebra as LA\n\nBase.@kwdef struct WeightedDiagonalPreconBuilder\n    w::Float64\nend\n\n(builder::WeightedDiagonalPreconBuilder)(A, p) = (builder.w * LA.Diagonal(A), LA.I)\n\nn = 4\nA = n * LA.I - rand(n, n)\nb = rand(n)\n\nprob = LS.LinearProblem(A, b)\nsol = LS.solve(prob, LS.KrylovJL_GMRES(precs = WeightedDiagonalPreconBuilder(w = 0.9)))\nsol.u\n\nB = A .+ 0.1\ncache = sol.cache\nLS.reinit!(cache, A = B, reuse_precs = true)\nsol = LS.solve!(cache, LS.KrylovJL_GMRES(precs = WeightedDiagonalPreconBuilder(w = 0.9)))\nsol.u","category":"page"},{"location":"basics/Preconditioners/#Preconditioner-Interface","page":"Preconditioners","title":"Preconditioner Interface","text":"","category":"section"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"To define a new preconditioner you define a Julia type which satisfies the following interface:","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"Base.eltype(::Preconditioner) (Required only for Krylov.jl)\nLinearAlgebra.ldiv!(::AbstractVector,::Preconditioner,::AbstractVector) and LinearAlgebra.ldiv!(::Preconditioner,::AbstractVector)","category":"page"},{"location":"basics/Preconditioners/#Curated-List-of-Pre-Defined-Preconditioners","page":"Preconditioners","title":"Curated List of Pre-Defined Preconditioners","text":"","category":"section"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"The following preconditioners match the interface of LinearSolve.jl.","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"LinearSolve.ComposePreconditioner(prec1,prec2): composes the preconditioners to apply prec1 before prec2.\nLinearSolve.InvPreconditioner(prec): inverts mul! and ldiv! in a preconditioner definition as a lazy inverse.\nLinearAlgera.Diagonal(s::Union{Number,AbstractVector}): the lazy Diagonal matrix type of Base.LinearAlgebra. Used for efficient construction of a diagonal preconditioner.\nOther Base.LinearAlgera types: all define the full Preconditioner interface.\nIncompleteLU.ilu: an implementation of the incomplete LU-factorization preconditioner. This requires A as a SparseMatrixCSC.\nPreconditioners.CholeskyPreconditioner(A, i): An incomplete Cholesky preconditioner with cut-off level i. Requires A as a AbstractMatrix and positive semi-definite.\nAlgebraicMultigrid: Implementations of the algebraic multigrid method. Must be converted to a preconditioner via AlgebraicMultigrid.aspreconditioner(AlgebraicMultigrid.precmethod(A)). Requires A as a AbstractMatrix. Provides the following methods:\nAlgebraicMultigrid.ruge_stuben(A)\nAlgebraicMultigrid.smoothed_aggregation(A)\nPyAMG: Implementations of the algebraic multigrid method. Must be converted to a preconditioner via PyAMG.aspreconditioner(PyAMG.precmethod(A)). Requires A as a AbstractMatrix. Provides the following methods:\nPyAMG.RugeStubenSolver(A)\nPyAMG.SmoothedAggregationSolver(A)\nILUZero.ILU0Precon(A::SparseMatrixCSC{T,N}, b_type = T): An incomplete LU implementation. Requires A as a SparseMatrixCSC.\nLimitedLDLFactorizations.lldl: A limited-memory LDLᵀ factorization for symmetric matrices. Requires A as a SparseMatrixCSC. Applying F = lldl(A); F.D .= abs.(F.D) before usage as a preconditioner makes the preconditioner symmetric positive definite and thus is required for Krylov methods which are specialized for symmetric linear systems.\nRandomizedPreconditioners.NystromPreconditioner A randomized sketching method for positive semidefinite matrices A. Builds a preconditioner P  A + μ*I for the system (A + μ*I)x = b.\nHYPRE.jl A set of solvers with preconditioners which supports distributed computing via MPI. These can be written using the LinearSolve.jl interface choosing algorithms like HYPRE.ILU and HYPRE.BoomerAMG.\nKrylovPreconditioners.jl: Provides GPU-ready preconditioners via KernelAbstractions.jl. At the time of writing the package provides the following methods:\nIncomplete Cholesky decomposition KrylovPreconditioners.kp_ic0(A)\nIncomplete LU decomposition KrylovPreconditioners.kp_ilu0(A)\nBlock Jacobi KrylovPreconditioners.kp_block_jacobi(A)","category":"page"},{"location":"tutorials/linear/#Getting-Started-with-Solving-Linear-Systems-in-Julia","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"","category":"section"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"A linear system Au=b is specified by defining an AbstractMatrix or AbstractSciMLOperator. For the sake of simplicity, this tutorial will start by only showcasing concrete matrices. And specifically, we will start by using the basic Julia Matrix type.","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"The following defines a Matrix and a LinearProblem which is subsequently solved by the default linear solver.","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"import LinearSolve as LS\n\nA = rand(4, 4)\nb = rand(4)\nprob = LS.LinearProblem(A, b)\nsol = LS.solve(prob)\nsol.u","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"Note that LS.solve(prob) is equivalent to LS.solve(prob,nothing) where nothing denotes the choice of the default linear solver. This is equivalent to the Julia built-in A\\b, where the solution is recovered via sol.u. The power of this package comes into play when changing the algorithms. For example, Krylov.jl has some nice methods like GMRES which can be faster in some cases. With LinearSolve.jl, there is one interface and changing linear solvers is simply the switch of the algorithm choice:","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"sol = LS.solve(prob, LS.KrylovJL_GMRES())\nsol.u","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"Thus, a package which uses LinearSolve.jl simply needs to allow the user to pass in an algorithm struct and all wrapped linear solvers are immediately available as tweaks to the general algorithm. For more information on the available solvers, see the solvers page","category":"page"},{"location":"tutorials/linear/#Sparse-and-Structured-Matrices","page":"Getting Started with Solving Linear Systems in Julia","title":"Sparse and Structured Matrices","text":"","category":"section"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"There is no difference in the interface for LinearSolve.jl on sparse and structured matrices. For example, the following now uses Julia's built-in SparseArrays.jl to define a sparse matrix (SparseMatrixCSC) and solve the system with LinearSolve.jl. Note that sprand is a shorthand for quickly creating a sparse random matrix (see SparseArrays.jl for more details on defining sparse matrices).","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"import LinearSolve as LS\nimport SparseArrays as SA\n\nA = SA.sprand(4, 4, 0.75)\nb = rand(4)\nprob = LS.LinearProblem(A, b)\nsol = LS.solve(prob)\nsol.u\n\nsol = LS.solve(prob, LS.KrylovJL_GMRES()) # Choosing algorithms is done the same way\nsol.u","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"Similarly structure matrix types, like banded matrices, can be provided using special matrix types. While any AbstractMatrix type should be compatible via the general Julia interfaces, LinearSolve.jl specifically tests with the following cases:","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"LinearAlgebra.jl\nSymmetric\nHermitian\nUpperTriangular\nUnitUpperTriangular\nLowerTriangular\nUnitLowerTriangular\nSymTridiagonal\nTridiagonal\nBidiagonal\nDiagonal\nBandedMatrices.jl BandedMatrix\nBlockDiagonals.jl BlockDiagonal\nCUDA.jl (CUDA GPU-based dense and sparse matrices) CuArray (GPUArray)\nFastAlmostBandedMatrices.jl FastAlmostBandedMatrix\nMetal.jl (Apple M-series GPU-based dense matrices) MetalArray","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"note: Note\n","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"Choosing the most specific matrix structure that matches your specific system will give you the most performance. Thus if your matrix is symmetric, specifically building with Symmetric(A) will be faster than simply using A, and will generally lead to better automatic linear solver choices. Note that you can also choose the type for b, but generally a dense vector will be the fastest here and many solvers will not support a sparse b.","category":"page"},{"location":"tutorials/linear/#Using-Matrix-Free-Operators-via-SciMLOperators.jl","page":"Getting Started with Solving Linear Systems in Julia","title":"Using Matrix-Free Operators via SciMLOperators.jl","text":"","category":"section"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"In many cases where a sparse matrix gets really large, even the sparse representation cannot be stored in memory. However, in many such cases, such as with PDE discretizations, you may be able to write down a function that directly computes A*x. These \"matrix-free\" operators allow the user to define the Ax=b problem to be solved giving only the definition of A*x and allowing specific solvers (Krylov methods) to act without ever constructing the full matrix.","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"The Matrix-Free operators are provided by the SciMLOperators.jl interface. For example, for the matrix A defined via:","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"A = [-2.0 1 0 0 0\n     1 -2 1 0 0\n     0 1 -2 1 0\n     0 0 1 -2 1\n     0 0 0 1 -2]","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"We can define the FunctionOperator that does the A*v operations, without using the matrix A. This is done by defining a function func(w,v,u,p,t) which calculates w = A(u,p,t)*v (for the purposes of this tutorial, A is just a constant operator. See the SciMLOperators.jl documentation for more details on defining non-constant operators, operator algebras, and many more features). This is done by:","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"function Afunc!(w, v, u, p, t)\n    w[1] = -2v[1] + v[2]\n    for i in 2:4\n        w[i] = v[i - 1] - 2v[i] + v[i + 1]\n    end\n    w[5] = v[4] - 2v[5]\n    nothing\nend\n\nfunction Afunc!(v, u, p, t)\n    w = zeros(5)\n    Afunc!(w, v, u, p, t)\n    w\nend\n\nimport SciMLOperators as SMO\nmfopA = SMO.FunctionOperator(Afunc!, zeros(5), zeros(5))","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"Let's check these are the same:","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"v = rand(5)\nmfopA*v - A*v","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"Notice mfopA does this without having to have A because it just uses the equivalent Afunc! instead. Now, even though we don't have a matrix, we can still solve linear systems defined by this operator. For example:","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"b = rand(5)\nprob = LS.LinearProblem(mfopA, b)\nsol = LS.solve(prob)\nsol.u","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"And we can check this is successful:","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"mfopA * sol.u - b","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"note: Note\n","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"Note that not all methods can use a matrix-free operator. For example, LS.LUFactorization() requires a matrix. If you use an invalid method, you will get an error. The methods particularly from KrylovJL are the ones preferred for these cases (and are defaulted to).","category":"page"},{"location":"release_notes/#Release-Notes","page":"Release Notes","title":"Release Notes","text":"","category":"section"},{"location":"release_notes/#v2.0","page":"Release Notes","title":"v2.0","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"LinearCache changed from immutable to mutable. With this, the out of place interfaces like set_A were deprecated for simply mutating the cache, cache.A = .... This fixes some correctness checks and makes the package more robust while improving performance.\nThe default algorithm is now type-stable and does not rely on a dynamic dispatch for the choice.\nIterativeSolvers.jl and KrylovKit.jl were made into extension packages.\nDocumentation of the solvers has changed to docstrings","category":"page"},{"location":"basics/common_solver_opts/#Common-Solver-Options-(Keyword-Arguments-for-Solve)","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"While many algorithms have specific arguments within their constructor, the keyword arguments for solve are common across all the algorithms in order to give composability. These are also the options taken at init time. The following are the options these algorithms take, along with their defaults.","category":"page"},{"location":"basics/common_solver_opts/#General-Controls","page":"Common Solver Options (Keyword Arguments for Solve)","title":"General Controls","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"alias::LinearAliasSpecifier: Holds the fields alias_A and alias_b which specify whether to alias the matrices A and b respectively. When these fields are true, A and b can be written to and changed by the solver algorithm. When fields are nothing the default behavior is used, which is to default to true when the algorithm is known not to modify the matrices, and false otherwise.\nverbose: Whether to print extra information. Defaults to false.\nassumptions: Sets the assumptions of the operator in order to effect the default choice algorithm. See the Operator Assumptions page for more details.","category":"page"},{"location":"basics/common_solver_opts/#Iterative-Solver-Controls","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Iterative Solver Controls","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"Error controls are not used by all algorithms. Specifically, direct solves always solve completely. Error controls only apply to iterative solvers.","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"abstol: The absolute tolerance. Defaults to √(eps(eltype(A)))\nreltol: The relative tolerance. Defaults to √(eps(eltype(A)))\nmaxiters: The number of iterations allowed. Defaults to length(prob.b)\nPl,Pr: The left and right preconditioners, respectively. For more information, see the Preconditioners page.","category":"page"},{"location":"solvers/solvers/#linearsystemsolvers","page":"Linear System Solvers","title":"Linear System Solvers","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"LS.solve(prob::LS.LinearProblem,alg;kwargs)","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Solves for Au=b in the problem defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.","category":"page"},{"location":"solvers/solvers/#Recommended-Methods","page":"Linear System Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"solvers/solvers/#Dense-Matrices","page":"Linear System Solvers","title":"Dense Matrices","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"The default algorithm nothing is good for picking an algorithm that will work, but one may need to change this to receive more performance or precision. If more precision is necessary, LS.QRFactorization() and LS.SVDFactorization() are the best choices, with SVD being the slowest but most precise.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"For efficiency, RFLUFactorization is the fastest for dense LU-factorizations until around 150x150 matrices, though this can be dependent on the exact details of the hardware. After this point, MKLLUFactorization is usually faster on most hardware. Note that on Mac computers that AppleAccelerateLUFactorization is generally always the fastest. LUFactorization will use your base system BLAS which can be fast or slow depending on the hardware configuration. SimpleLUFactorization will be fast only on very small matrices but can cut down on compile times.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"For very large dense factorizations, offloading to the GPU can be preferred. Metal.jl can be used on Mac hardware to offload, and has a cutoff point of being faster at around size 20,000 x 20,000 matrices (and only supports Float32). CudaOffloadFactorization can be more efficient at a much smaller cutoff, possibly around size 1,000 x 1,000 matrices, though this is highly dependent on the chosen GPU hardware. CudaOffloadFactorization requires a CUDA-compatible NVIDIA GPU. CUDA offload supports Float64 but most consumer GPU hardware will be much faster on Float32 (many are >32x faster for Float32 operations than Float64 operations) and thus for most hardware this is only recommended for Float32 matrices.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nPerformance details for dense LU-factorizations can be highly dependent on the hardware configuration. For details see this issue. If one is looking to best optimize their system, we suggest running the performance tuning benchmark.","category":"page"},{"location":"solvers/solvers/#Sparse-Matrices","page":"Linear System Solvers","title":"Sparse Matrices","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"For sparse LU-factorizations, KLUFactorization if there is less structure to the sparsity pattern and UMFPACKFactorization if there is more structure. Pardiso.jl's methods are also known to be very efficient sparse linear solvers.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"For GPU-accelerated sparse LU-factorizations, there are two high-performance options. When using CuSparseMatrixCSR arrays with CUDSS.jl loaded, LUFactorization() will automatically use NVIDIA's cuDSS library. Alternatively, CUSOLVERRFFactorization provides access to NVIDIA's cusolverRF library. Both offer significant performance improvements for sparse systems on CUDA-capable GPUs and are particularly effective for large sparse matrices that can benefit from GPU parallelization. CUDSS is more for Float32 while CUSOLVERRFFactorization is for Float64.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"While these sparse factorizations are based on implementations in other languages, and therefore constrained to standard number types (Float64,  Float32 and their complex counterparts),  SparspakFactorization is able to handle general number types, e.g. defined by ForwardDiff.jl, MultiFloats.jl, or IntervalArithmetics.jl.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"As sparse matrices get larger, iterative solvers tend to get more efficient than factorization methods if a lower tolerance of the solution is required.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Krylov.jl generally outperforms IterativeSolvers.jl and KrylovKit.jl, and is compatible with CPUs and GPUs, and thus is the generally preferred form for Krylov methods. The choice of Krylov method should be the one most constrained to the type of operator one has, for example if positive definite then Krylov_CG(), but if no good properties then use Krylov_GMRES().","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Finally, a user can pass a custom function for handling the linear solve using LS.LinearSolveFunction() if existing solvers are not optimally suited for their application. The interface is detailed here.","category":"page"},{"location":"solvers/solvers/#Lazy-SciMLOperators","page":"Linear System Solvers","title":"Lazy SciMLOperators","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"If the linear operator is given as a lazy non-concrete operator, such as a FunctionOperator, then using a Krylov method is preferred in order to not concretize the matrix. Krylov.jl generally outperforms IterativeSolvers.jl and KrylovKit.jl, and is compatible with CPUs and GPUs, and thus is the generally preferred form for Krylov methods. The choice of Krylov method should be the one most constrained to the type of operator one has, for example if positive definite then Krylov_CG(), but if no good properties then use Krylov_GMRES().","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"tip: Tip\nIf your materialized operator is a uniform block diagonal matrix, then you can use SimpleGMRES(; blocksize = <known block size>) to further improve performance. This often shows up in Neural Networks where the Jacobian wrt the Inputs (almost always) is a Uniform Block Diagonal matrix of Block Size = size of the input divided by the batch size.","category":"page"},{"location":"solvers/solvers/#Full-List-of-Methods","page":"Linear System Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"solvers/solvers/#Polyalgorithms","page":"Linear System Solvers","title":"Polyalgorithms","text":"","category":"section"},{"location":"solvers/solvers/#LinearSolve.DefaultLinearSolver","page":"Linear System Solvers","title":"LinearSolve.DefaultLinearSolver","text":"DefaultLinearSolver(;safetyfallback=true)\n\nThe default linear solver. This is the algorithm chosen when solve(prob) is called. It's a polyalgorithm that detects the optimal method for a given A, b and hardware (Intel, AMD, GPU, etc.).\n\nKeyword Arguments\n\nsafetyfallback: determines whether to fallback to a column-pivoted QR factorization when an LU factorization fails. This can be required if A is rank-deficient. Defaults to true.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#RecursiveFactorization.jl","page":"Linear System Solvers","title":"RecursiveFactorization.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing this solver requires adding the package RecursiveFactorization.jl, i.e. using RecursiveFactorization","category":"page"},{"location":"solvers/solvers/#LinearSolve.RFLUFactorization","page":"Linear System Solvers","title":"LinearSolve.RFLUFactorization","text":"RFLUFactorization()\n\nA fast pure Julia LU-factorization implementation using RecursiveFactorization.jl. This is by far the fastest LU-factorization implementation, usually outperforming OpenBLAS and MKL for smaller matrices (<500x500), but currently optimized only for Base Array with Float32 or Float64. Additional optimization for complex matrices is in the works.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#Base.LinearAlgebra","page":"Linear System Solvers","title":"Base.LinearAlgebra","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"These overloads tend to work for many array types, such as CuArrays for GPU-accelerated solving, using the overloads provided by the respective packages. Given that this can be customized per-package, details given below describe a subset of important arrays (Matrix, SparseMatrixCSC, CuMatrix, etc.)","category":"page"},{"location":"solvers/solvers/#LinearSolve.LUFactorization","page":"Linear System Solvers","title":"LinearSolve.LUFactorization","text":"LUFactorization(pivot=LinearAlgebra.RowMaximum())\n\nJulia's built in lu. Equivalent to calling lu!(A)\n\nOn dense matrices, this uses the current BLAS implementation of the user's computer, which by default is OpenBLAS but will use MKL if the user does using MKL in their system.\nOn sparse matrices, this will use UMFPACK from SparseArrays. Note that this will not cache the symbolic factorization.\nOn CuMatrix, it will use a CUDA-accelerated LU from CuSolver.\nOn BandedMatrix and BlockBandedMatrix, it will use a banded LU.\n\nPositional Arguments\n\npivot: The choice of pivoting. Defaults to LinearAlgebra.RowMaximum(). The other choice is LinearAlgebra.NoPivot().\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.GenericLUFactorization","page":"Linear System Solvers","title":"LinearSolve.GenericLUFactorization","text":"GenericLUFactorization(pivot=LinearAlgebra.RowMaximum())\n\nJulia's built in generic LU factorization. Equivalent to calling LinearAlgebra.generic_lufact!. Supports arbitrary number types but does not achieve as good scaling as BLAS-based LU implementations. Has low overhead and is good for small matrices.\n\nPositional Arguments\n\npivot: The choice of pivoting. Defaults to LinearAlgebra.RowMaximum(). The other choice is LinearAlgebra.NoPivot().\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.QRFactorization","page":"Linear System Solvers","title":"LinearSolve.QRFactorization","text":"QRFactorization(pivot=LinearAlgebra.NoPivot(),blocksize=16)\n\nJulia's built in qr. Equivalent to calling qr!(A).\n\nOn dense matrices, this uses the current BLAS implementation of the user's computer which by default is OpenBLAS but will use MKL if the user does using MKL in their system.\nOn sparse matrices, this will use SPQR from SparseArrays\nOn CuMatrix, it will use a CUDA-accelerated QR from CuSolver.\nOn BandedMatrix and BlockBandedMatrix, it will use a banded QR.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.SVDFactorization","page":"Linear System Solvers","title":"LinearSolve.SVDFactorization","text":"SVDFactorization(full=false,alg=LinearAlgebra.DivideAndConquer())\n\nJulia's built in svd. Equivalent to svd!(A).\n\nOn dense matrices, this uses the current BLAS implementation of the user's computer which by default is OpenBLAS but will use MKL if the user does using MKL in their system.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.CholeskyFactorization","page":"Linear System Solvers","title":"LinearSolve.CholeskyFactorization","text":"CholeskyFactorization(; pivot = nothing, tol = 0.0, shift = 0.0, perm = nothing)\n\nJulia's built in cholesky. Equivalent to calling cholesky!(A).\n\nKeyword Arguments\n\npivot: defaluts to NoPivot, can also be RowMaximum.\ntol: the tol argument in CHOLMOD. Only used for sparse matrices.\nshift: the shift argument in CHOLMOD. Only used for sparse matrices.\nperm: the perm argument in CHOLMOD. Only used for sparse matrices.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.BunchKaufmanFactorization","page":"Linear System Solvers","title":"LinearSolve.BunchKaufmanFactorization","text":"BunchKaufmanFactorization(; rook = false)\n\nJulia's built in bunchkaufman. Equivalent to calling bunchkaufman(A). Only for Symmetric matrices.\n\nKeyword Arguments\n\nrook: whether to perform rook pivoting. Defaults to false.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.CHOLMODFactorization","page":"Linear System Solvers","title":"LinearSolve.CHOLMODFactorization","text":"CHOLMODFactorization(; shift = 0.0, perm = nothing)\n\nA wrapper of CHOLMOD's polyalgorithm, mixing Cholesky factorization and ldlt. Tries cholesky for performance and retries ldlt if conditioning causes Cholesky to fail.\n\nOnly supports sparse matrices.\n\nKeyword Arguments\n\nshift: the shift argument in CHOLMOD.\nperm: the perm argument in CHOLMOD\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.NormalCholeskyFactorization","page":"Linear System Solvers","title":"LinearSolve.NormalCholeskyFactorization","text":"NormalCholeskyFactorization(pivot = RowMaximum())\n\nA fast factorization which uses a Cholesky factorization on A * A'. Can be much faster than LU factorization, but is not as numerically stable and thus should only be applied to well-conditioned matrices.\n\nwarn: Warn\nNormalCholeskyFactorization should only be applied to well-conditioned matrices. As a method it is not able to easily identify possible numerical issues. As a check it is recommended that the user checks A*u-b is approximately zero, as this may be untrue even if sol.retcode === ReturnCode.Success due to numerical stability issues.\n\nPositional Arguments\n\npivot: Defaults to RowMaximum(), but can be NoPivot()\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.NormalBunchKaufmanFactorization","page":"Linear System Solvers","title":"LinearSolve.NormalBunchKaufmanFactorization","text":"NormalBunchKaufmanFactorization(rook = false)\n\nA fast factorization which uses a BunchKaufman factorization on A * A'. Can be much faster than LU factorization, but is not as numerically stable and thus should only be applied to well-conditioned matrices.\n\nPositional Arguments\n\nrook: whether to perform rook pivoting. Defaults to false.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.jl","page":"Linear System Solvers","title":"LinearSolve.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"LinearSolve.jl contains some linear solvers built in for specialized cases.","category":"page"},{"location":"solvers/solvers/#LinearSolve.SimpleLUFactorization","page":"Linear System Solvers","title":"LinearSolve.SimpleLUFactorization","text":"SimpleLUFactorization(pivot::Bool = true)\n\nA simple LU-factorization implementation without BLAS. Fast for small matrices.\n\nPositional Arguments\n\npivot::Bool: whether to perform pivoting. Defaults to true\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.DiagonalFactorization","page":"Linear System Solvers","title":"LinearSolve.DiagonalFactorization","text":"DiagonalFactorization()\n\nA special implementation only for solving Diagonal matrices fast.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.SimpleGMRES","page":"Linear System Solvers","title":"LinearSolve.SimpleGMRES","text":"SimpleGMRES(; restart::Bool = true, blocksize::Int = 0, warm_start::Bool = false,\n    memory::Int = 20)\n\nA simple GMRES implementation for square non-Hermitian linear systems.\n\nThis implementation handles Block Diagonal Matrices with Uniformly Sized Square Blocks with specialized dispatches.\n\nArguments\n\nrestart::Bool: If true, then the solver will restart after memory iterations.\nmemory::Int = 20: The number of iterations before restarting. If restart is false, this value is used to allocate memory and later expanded if more memory is required.\nblocksize::Int = 0: If blocksize is > 0, the solver assumes that the matrix has a uniformly sized block diagonal structure with square blocks of size blocksize. Misusing this option will lead to incorrect results.\nIf this is set ≤ 0 and during runtime we get a Block Diagonal Matrix, then we will check if the specialized dispatch can be used.\n\nwarning: Warning\nMost users should be using the KrylovJL_GMRES solver instead of this implementation.\n\ntip: Tip\nWe can automatically detect if the matrix is a Block Diagonal Matrix with Uniformly Sized Square Blocks. If this is the case, then we can use a specialized dispatch. However, on most modern systems performing a single matrix-vector multiplication is faster than performing multiple smaller matrix-vector multiplications (as in the case of Block Diagonal Matrix). We recommend making the matrix dense (if size permits) and specifying the blocksize argument.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#FastLapackInterface.jl","page":"Linear System Solvers","title":"FastLapackInterface.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"FastLapackInterface.jl is a package that allows for a lower-level interface to the LAPACK calls to allow for preallocating workspaces to decrease the overhead of the wrappers. LinearSolve.jl provides a wrapper to these routines in a way where an initialized solver has a non-allocating LU factorization. In theory, this post-initialized solve should always be faster than the Base.LinearAlgebra version. In practice, with the way we wrap the solvers, we do not see a performance benefit and in fact benchmarks tend to show this inhibits performance.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing this solver requires adding the package FastLapackInterface.jl, i.e. using FastLapackInterface","category":"page"},{"location":"solvers/solvers/#LinearSolve.FastLUFactorization","page":"Linear System Solvers","title":"LinearSolve.FastLUFactorization","text":"FastLUFactorization()\n\nThe FastLapackInterface.jl version of the LU factorization. Notably, this version does not allow for choice of pivoting method.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.FastQRFactorization","page":"Linear System Solvers","title":"LinearSolve.FastQRFactorization","text":"FastQRFactorization()\n\nThe FastLapackInterface.jl version of the QR factorization.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#SuiteSparse.jl","page":"Linear System Solvers","title":"SuiteSparse.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing this solver requires adding the package SparseArrays.jl, i.e. using SparseArrays","category":"page"},{"location":"solvers/solvers/#LinearSolve.KLUFactorization","page":"Linear System Solvers","title":"LinearSolve.KLUFactorization","text":"KLUFactorization(;reuse_symbolic=true, check_pattern=true)\n\nA fast sparse LU-factorization which specializes on sparsity patterns with “less structure”.\n\nnote: Note\nBy default, the SparseArrays.jl are implemented for efficiency by caching the symbolic factorization. If the sparsity pattern of A may change between solves, set reuse_symbolic=false. If the pattern is assumed or known to be constant, set reuse_symbolic=true to avoid unnecessary recomputation. To further reduce computational overhead, you can disable pattern checks entirely by setting check_pattern = false. Note that this may error if the sparsity pattern does change unexpectedly.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.UMFPACKFactorization","page":"Linear System Solvers","title":"LinearSolve.UMFPACKFactorization","text":"UMFPACKFactorization(;reuse_symbolic=true, check_pattern=true)\n\nA fast sparse multithreaded LU-factorization which specializes on sparsity patterns with “more structure”.\n\nnote: Note\nBy default, the SparseArrays.jl are implemented for efficiency by caching the symbolic factorization. If the sparsity pattern of A may change between solves, set reuse_symbolic=false. If the pattern is assumed or known to be constant, set reuse_symbolic=true to avoid unnecessary recomputation. To further reduce computational overhead, you can disable pattern checks entirely by setting check_pattern = false. Note that this may error if the sparsity pattern does change unexpectedly.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#Sparspak.jl","page":"Linear System Solvers","title":"Sparspak.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing this solver requires adding the package Sparspak.jl, i.e. using Sparspak","category":"page"},{"location":"solvers/solvers/#LinearSolve.SparspakFactorization","page":"Linear System Solvers","title":"LinearSolve.SparspakFactorization","text":"SparspakFactorization(reuse_symbolic = true)\n\nThis is the translation of the well-known sparse matrix software Sparspak (Waterloo Sparse Matrix Package), solving large sparse systems of linear algebraic equations. Sparspak is composed of the subroutines from the book \"Computer Solution of Large Sparse Positive Definite Systems\" by Alan George and Joseph Liu. Originally written in Fortran 77, later rewritten in Fortran 90. Here is the software translated into Julia.\n\nThe Julia rewrite is released  under the MIT license with an express permission from the authors of the Fortran package. The package uses multiple dispatch to route around standard BLAS routines in the case e.g. of arbitrary-precision floating point numbers or ForwardDiff.Dual. This e.g. allows for Automatic Differentiation (AD) of a sparse-matrix solve.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#Krylov.jl","page":"Linear System Solvers","title":"Krylov.jl","text":"","category":"section"},{"location":"solvers/solvers/#LinearSolve.KrylovJL_CG","page":"Linear System Solvers","title":"LinearSolve.KrylovJL_CG","text":"KrylovJL_CG(args...; kwargs...)\n\nA generic CG implementation for Hermitian and positive definite linear systems\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.KrylovJL_MINRES","page":"Linear System Solvers","title":"LinearSolve.KrylovJL_MINRES","text":"KrylovJL_MINRES(args...; kwargs...)\n\nA generic MINRES implementation for Hermitian linear systems\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.KrylovJL_GMRES","page":"Linear System Solvers","title":"LinearSolve.KrylovJL_GMRES","text":"KrylovJL_GMRES(args...; gmres_restart = 0, window = 0, kwargs...)\n\nA generic GMRES implementation for square non-Hermitian linear systems\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.KrylovJL_BICGSTAB","page":"Linear System Solvers","title":"LinearSolve.KrylovJL_BICGSTAB","text":"KrylovJL_BICGSTAB(args...; kwargs...)\n\nA generic BICGSTAB implementation for square non-Hermitian linear systems\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.KrylovJL_LSMR","page":"Linear System Solvers","title":"LinearSolve.KrylovJL_LSMR","text":"KrylovJL_LSMR(args...; kwargs...)\n\nA generic LSMR implementation for least-squares problems\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.KrylovJL_CRAIGMR","page":"Linear System Solvers","title":"LinearSolve.KrylovJL_CRAIGMR","text":"KrylovJL_CRAIGMR(args...; kwargs...)\n\nA generic CRAIGMR implementation for least-norm problems\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.KrylovJL","page":"Linear System Solvers","title":"LinearSolve.KrylovJL","text":"KrylovJL(args...; KrylovAlg = Krylov.gmres!,\n    Pl = nothing, Pr = nothing,\n    gmres_restart = 0, window = 0,\n    kwargs...)\n\nA generic wrapper over the Krylov.jl krylov-subspace iterative solvers.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#MKL.jl","page":"Linear System Solvers","title":"MKL.jl","text":"","category":"section"},{"location":"solvers/solvers/#LinearSolve.MKLLUFactorization","page":"Linear System Solvers","title":"LinearSolve.MKLLUFactorization","text":"MKLLUFactorization()\n\nA wrapper over Intel's Math Kernel Library (MKL). Direct calls to MKL in a way that pre-allocates workspace to avoid allocations and does not require libblastrampoline.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#AppleAccelerate.jl","page":"Linear System Solvers","title":"AppleAccelerate.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing this solver requires a Mac with Apple Accelerate. This should come standard in most \"modern\" Mac computers.","category":"page"},{"location":"solvers/solvers/#LinearSolve.AppleAccelerateLUFactorization","page":"Linear System Solvers","title":"LinearSolve.AppleAccelerateLUFactorization","text":"AppleAccelerateLUFactorization()\n\nA wrapper over Apple's Accelerate Library. Direct calls to Acceelrate in a way that pre-allocates workspace to avoid allocations and does not require libblastrampoline.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#Metal.jl","page":"Linear System Solvers","title":"Metal.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing this solver requires adding the package Metal.jl, i.e. using Metal. This package is only compatible with Mac M-Series computers with a Metal-compatible GPU.","category":"page"},{"location":"solvers/solvers/#LinearSolve.MetalLUFactorization","page":"Linear System Solvers","title":"LinearSolve.MetalLUFactorization","text":"MetalLUFactorization()\n\nA wrapper over Apple's Metal GPU library. Direct calls to Metal in a way that pre-allocates workspace to avoid allocations and automatically offloads to the GPU.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#Pardiso.jl","page":"Linear System Solvers","title":"Pardiso.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing this solver requires adding the package Pardiso.jl, i.e. using Pardiso","category":"page"},{"location":"solvers/solvers/#LinearSolve.MKLPardisoFactorize","page":"Linear System Solvers","title":"LinearSolve.MKLPardisoFactorize","text":"MKLPardisoFactorize(; nprocs::Union{Int, Nothing} = nothing,\n    matrix_type = nothing,\n    cache_analysis = false,\n    iparm::Union{Vector{Tuple{Int, Int}}, Nothing} = nothing,\n    dparm::Union{Vector{Tuple{Int, Int}}, Nothing} = nothing)\n\nA sparse factorization method using MKL Pardiso.\n\nnote: Note\nUsing this solver requires adding the package Pardiso.jl, i.e. using Pardiso\n\nKeyword Arguments\n\nSetting cache_analysis = true disables Pardiso's scaling and matching defaults and caches the result of the initial analysis phase for all further computations with this solver.\n\nFor the definition of the other keyword arguments, see the Pardiso.jl documentation. All values default to nothing and the solver internally determines the values given the input types, and these keyword arguments are only for overriding the default handling process. This should not be required by most users.\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.MKLPardisoIterate","page":"Linear System Solvers","title":"LinearSolve.MKLPardisoIterate","text":"MKLPardisoIterate(; nprocs::Union{Int, Nothing} = nothing,\n    matrix_type = nothing,\n    cache_analysis = false,\n    iparm::Union{Vector{Tuple{Int, Int}}, Nothing} = nothing,\n    dparm::Union{Vector{Tuple{Int, Int}}, Nothing} = nothing)\n\nA mixed factorization+iterative method using MKL Pardiso.\n\nnote: Note\nUsing this solver requires adding the package Pardiso.jl, i.e. using Pardiso\n\nKeyword Arguments\n\nSetting cache_analysis = true disables Pardiso's scaling and matching defaults and caches the result of the initial analysis phase for all further computations with this solver.\n\nFor the definition of the other keyword arguments, see the Pardiso.jl documentation. All values default to nothing and the solver internally determines the values given the input types, and these keyword arguments are only for overriding the default handling process. This should not be required by most users.\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.PardisoJL","page":"Linear System Solvers","title":"LinearSolve.PardisoJL","text":"PardisoJL(; nprocs::Union{Int, Nothing} = nothing,\n    solver_type = nothing,\n    matrix_type = nothing,\n    iparm::Union{Vector{Tuple{Int, Int}}, Nothing} = nothing,\n    dparm::Union{Vector{Tuple{Int, Int}}, Nothing} = nothing,\n    vendor::Union{Symbol, Nothing} = nothing\n)\n\nA generic method using  Pardiso. Specifying solver_type is required.\n\nnote: Note\nUsing this solver requires adding the package Pardiso.jl, i.e. using Pardiso\n\nKeyword Arguments\n\nThe vendor keyword allows to choose between Panua pardiso  (former pardiso-project.org; vendor=:Panua) and  MKL Pardiso (vendor=:MKL). If vendor==nothing, Panua pardiso is preferred over MKL Pardiso.\n\nFor the definition of the other keyword arguments, see the Pardiso.jl documentation. All values default to nothing and the solver internally determines the values given the input types, and these keyword arguments are only for overriding the default handling process. This should not be required by most users.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#CUDA.jl","page":"Linear System Solvers","title":"CUDA.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Note that CuArrays are supported by GenericFactorization in the \"normal\" way. The following are non-standard GPU factorization routines.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing this solver requires adding the package CUDA.jl, i.e. using CUDA","category":"page"},{"location":"solvers/solvers/#LinearSolve.CudaOffloadFactorization","page":"Linear System Solvers","title":"LinearSolve.CudaOffloadFactorization","text":"CudaOffloadFactorization()\n\nAn offloading technique used to GPU-accelerate CPU-based computations. Requires a sufficiently large A to overcome the data transfer costs.\n\nnote: Note\nUsing this solver requires adding the package CUDA.jl, i.e. using CUDA\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#CUSOLVERRF.jl","page":"Linear System Solvers","title":"CUSOLVERRF.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing this solver requires adding the package CUSOLVERRF.jl, i.e. using CUSOLVERRF","category":"page"},{"location":"solvers/solvers/#LinearSolve.CUSOLVERRFFactorization","page":"Linear System Solvers","title":"LinearSolve.CUSOLVERRFFactorization","text":"CUSOLVERRFFactorization(; symbolic = :RF, reuse_symbolic = true)\n\nA GPU-accelerated sparse LU factorization using NVIDIA's cusolverRF library. This solver is specifically designed for sparse matrices on CUDA GPUs and  provides high-performance factorization and solve capabilities.\n\nKeyword Arguments\n\nsymbolic: The symbolic factorization method to use. Options are:\n:RF (default): Use cusolverRF's built-in symbolic analysis\n:KLU: Use KLU for symbolic analysis\nreuse_symbolic: Whether to reuse the symbolic factorization when the  sparsity pattern doesn't change (default: true)\n\nnote: Note\nThis solver requires CUSOLVERRF.jl to be loaded and only supports  Float64 element types with Int32 indices.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#IterativeSolvers.jl","page":"Linear System Solvers","title":"IterativeSolvers.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing these solvers requires adding the package IterativeSolvers.jl, i.e. using IterativeSolvers","category":"page"},{"location":"solvers/solvers/#LinearSolve.IterativeSolversJL_CG","page":"Linear System Solvers","title":"LinearSolve.IterativeSolversJL_CG","text":"IterativeSolversJL_CG(args...; Pl = nothing, Pr = nothing, kwargs...)\n\nA wrapper over the IterativeSolvers.jl CG.\n\nnote: Note\nUsing this solver requires adding the package IterativeSolvers.jl, i.e. using IterativeSolvers\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.IterativeSolversJL_GMRES","page":"Linear System Solvers","title":"LinearSolve.IterativeSolversJL_GMRES","text":"IterativeSolversJL_GMRES(args...; Pl = nothing, Pr = nothing, gmres_restart = 0, kwargs...)\n\nA wrapper over the IterativeSolvers.jl GMRES.\n\nnote: Note\nUsing this solver requires adding the package IterativeSolvers.jl, i.e. using IterativeSolvers\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.IterativeSolversJL_BICGSTAB","page":"Linear System Solvers","title":"LinearSolve.IterativeSolversJL_BICGSTAB","text":"IterativeSolversJL_BICGSTAB(args...; Pl = nothing, Pr = nothing, kwargs...)\n\nA wrapper over the IterativeSolvers.jl BICGSTAB.\n\nnote: Note\nUsing this solver requires adding the package IterativeSolvers.jl, i.e. using IterativeSolvers\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.IterativeSolversJL_MINRES","page":"Linear System Solvers","title":"LinearSolve.IterativeSolversJL_MINRES","text":"IterativeSolversJL_MINRES(args...; Pl = nothing, Pr = nothing, kwargs...)\n\nA wrapper over the IterativeSolvers.jl MINRES.\n\nnote: Note\nUsing this solver requires adding the package IterativeSolvers.jl, i.e. using IterativeSolvers\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.IterativeSolversJL_IDRS","page":"Linear System Solvers","title":"LinearSolve.IterativeSolversJL_IDRS","text":"IterativeSolversJL_IDRS(args...; Pl = nothing, kwargs...)\n\nA wrapper over the IterativeSolvers.jl IDR(S).\n\nnote: Note\nUsing this solver requires adding the package IterativeSolvers.jl, i.e. using IterativeSolvers\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.IterativeSolversJL","page":"Linear System Solvers","title":"LinearSolve.IterativeSolversJL","text":"IterativeSolversJL(args...;\n    generate_iterator = IterativeSolvers.gmres_iterable!,\n    Pl = nothing, Pr = nothing,\n    gmres_restart = 0, kwargs...)\n\nA generic wrapper over the IterativeSolvers.jl solvers.\n\nnote: Note\nUsing this solver requires adding the package IterativeSolvers.jl, i.e. using IterativeSolvers\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#KrylovKit.jl","page":"Linear System Solvers","title":"KrylovKit.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing these solvers requires adding the package KrylovKit.jl, i.e. using KrylovKit","category":"page"},{"location":"solvers/solvers/#LinearSolve.KrylovKitJL_CG","page":"Linear System Solvers","title":"LinearSolve.KrylovKitJL_CG","text":"KrylovKitJL_CG(args...; Pl = nothing, Pr = nothing, kwargs...)\n\nA generic CG implementation for Hermitian and positive definite linear systems\n\nnote: Note\nUsing this solver requires adding the package KrylovKit.jl, i.e. using KrylovKit\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.KrylovKitJL_GMRES","page":"Linear System Solvers","title":"LinearSolve.KrylovKitJL_GMRES","text":"KrylovKitJL_GMRES(args...; Pl = nothing, Pr = nothing, gmres_restart = 0, kwargs...)\n\nA generic GMRES implementation.\n\nnote: Note\nUsing this solver requires adding the package KrylovKit.jl, i.e. using KrylovKit\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.KrylovKitJL","page":"Linear System Solvers","title":"LinearSolve.KrylovKitJL","text":"KrylovKitJL(args...; KrylovAlg = Krylov.gmres!, kwargs...)\n\nA generic iterative solver implementation allowing the choice of KrylovKit.jl solvers.\n\nnote: Note\nUsing this solver requires adding the package KrylovKit.jl, i.e. using KrylovKit\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#HYPRE.jl","page":"Linear System Solvers","title":"HYPRE.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing HYPRE solvers requires Julia version 1.9 or higher, and that the package HYPRE.jl is installed.","category":"page"},{"location":"solvers/solvers/#LinearSolve.HYPREAlgorithm","page":"Linear System Solvers","title":"LinearSolve.HYPREAlgorithm","text":"HYPREAlgorithm(solver; Pl = nothing)\n\nHYPRE.jl is an interface to hypre and provide iterative solvers and preconditioners for sparse linear systems. It is mainly developed for large multi-process distributed problems (using MPI), but can also be used for single-process problems with Julias standard sparse matrices.\n\nIf you need more fine-grained control over the solver/preconditioner options you can alternatively pass an already created solver to HYPREAlgorithm (and to the Pl keyword argument). See HYPRE.jl docs for how to set up solvers with specific options.\n\nnote: Note\nUsing HYPRE solvers requires Julia version 1.9 or higher, and that the package HYPRE.jl is installed.\n\nPositional Arguments\n\nThe single positional argument solver has the following choices:\n\nHYPRE.BiCGSTAB\nHYPRE.BoomerAMG\nHYPRE.FlexGMRES\nHYPRE.GMRES\nHYPRE.Hybrid\nHYPRE.ILU\nHYPRE.ParaSails (as preconditioner only)\nHYPRE.PCG\n\nKeyword Arguments\n\nPl: A choice of left preconditioner.\n\nExample\n\nFor example, to use HYPRE.PCG as the solver, with HYPRE.BoomerAMG as the preconditioner, the algorithm should be defined as follows:\n\nA, b = setup_system(...)\nprob = LinearProblem(A, b)\nalg = HYPREAlgorithm(HYPRE.PCG)\nprec = HYPRE.BoomerAMG\nsol = solve(prob, alg; Pl = prec)\n\n\n\n\n\n","category":"type"},{"location":"#LinearSolve.jl:-High-Performance-Unified-Linear-Solvers","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"","category":"section"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"LinearSolve.jl is a unified interface for the linear solving packages of Julia. It interfaces with other packages of the Julia ecosystem to make it easy to test alternative solver packages and pass small types to control algorithm swapping. It also interfaces with the ModelingToolkit.jl world of symbolic modeling to allow for automatically generating high-performance code.","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"Performance is key: the current methods are made to be highly performant on scalar and statically sized small problems, with options for large-scale systems. If you run into any performance issues, please file an issue.","category":"page"},{"location":"#Installation","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"Installation","text":"","category":"section"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"To install LinearSolve.jl, use the Julia package manager:","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"using Pkg\nPkg.add(\"LinearSolve\")","category":"page"},{"location":"#Contributing","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"Contributing","text":"","category":"section"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nSee the SciML Style Guide for common coding practices and other style decisions.\nThere are a few community forums:\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Slack\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Zulip\nOn the Julia Discourse forums\nSee also SciML Community page","category":"page"},{"location":"#Roadmap","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"Roadmap","text":"","category":"section"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"Wrappers for every linear solver in the Julia language is on the roadmap. If there are any important ones that are missing that you would like to see added, please open an issue. The current algorithms should support automatic differentiation. Pre-defined preconditioners would be a welcome addition.","category":"page"},{"location":"#Reproducibility","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"</details>","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"</details>","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"using Pkg # hide\nPkg.status(; mode = PKGMODE_MANIFEST) # hide","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"</details>","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"using TOML\nusing Markdown\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink_manifest = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n                \"/assets/Manifest.toml\"\nlink_project = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n               \"/assets/Project.toml\"\nMarkdown.parse(\"\"\"You can also download the\n[manifest]($link_manifest)\nfile and the\n[project]($link_project)\nfile.\n\"\"\")","category":"page"},{"location":"tutorials/caching_interface/#Linear-Solve-with-Caching-Interface","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"","category":"section"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"Often, one may want to cache information that is reused between different linear solves. For example, if one is going to perform:","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"A \\ b1\nA \\ b2","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"then it would be more efficient to LU-factorize one time and reuse the factorization:","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"LA.lu!(A)\nA \\ b1\nA \\ b2","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"LinearSolve.jl's caching interface automates this process to use the most efficient means of solving and resolving linear systems. To do this with LinearSolve.jl, you simply init a cache, solve, replace b, and solve again. This looks like:","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"import LinearSolve as LS\nimport LinearAlgebra as LA\n\nn = 4\nA = rand(n, n)\nb1 = rand(n);\nb2 = rand(n);\nprob = LS.LinearProblem(A, b1)\n\nlinsolve = LS.init(prob)\nsol1 = LS.solve!(linsolve)","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"linsolve.b = b2\nsol2 = LS.solve!(linsolve)\n\nsol2.u","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"Then refactorization will occur when a new A is given:","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"A2 = rand(n, n)\nlinsolve.A = A2\nsol3 = LS.solve!(linsolve)\n\nsol3.u","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"The factorization occurs on the first solve, and it stores the factorization in the cache. You can retrieve this cache via sol.cache, which is the same object as the init, but updated to know not to re-solve the factorization.","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"The advantage of course with import LinearSolve.jl in this form is that it is efficient while being agnostic to the linear solver. One can easily swap in iterative solvers, sparse solvers, etc. and it will do all the tricks like caching the symbolic factorization if the sparsity pattern is unchanged.","category":"page"}]
}
