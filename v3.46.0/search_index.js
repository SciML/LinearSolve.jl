var documenterSearchIndex = {"docs":
[{"location":"tutorials/accelerating_choices/#Accelerating-your-Linear-Solves","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"","category":"section"},{"location":"tutorials/accelerating_choices/","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"note: Note\nThis section is essential if you wish to achieve maximum performance with LinearSolve.jl, especially on v7 and above. Please ensure the tips of this section are adhered to when optimizing code and benchmarking.","category":"page"},{"location":"tutorials/accelerating_choices/","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"Great, you've learned how to use LinearSolve.jl and you're using it daily, either directly or through other SciML libraries, and you want to improve your performance. How can this be done? While it might seem at first like a hopeless endeavour, \"A\\b uses a BLAS library and so it's already highly optimized C code\", it turns out there are many factors you need to consider to squeeze out the last 10x of performance. And yes, it can be about a factor of 10 in some scenarios, so let's dive in.","category":"page"},{"location":"tutorials/accelerating_choices/#Understanding-Performance-of-Dense-Linear-Solves","page":"Accelerating your Linear Solves","title":"Understanding Performance of Dense Linear Solves","text":"","category":"section"},{"location":"tutorials/accelerating_choices/","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"The performance of dense linear solvers is highly dependent on the size of the matrix and the chosen architecture to run on, i.e. the CPU. This issue gathered benchmark data from many different users and is summarized in the following graphs:","category":"page"},{"location":"tutorials/accelerating_choices/","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"(Image: Dense Linear Solve Benchmarks)","category":"page"},{"location":"tutorials/accelerating_choices/","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"Now one thing that is immediate is for example that AppleAccelerate generally does well on Apple M-series chips, MKL generally does well on Intel, etc. And we know this in LinearSolve.jl, in fact we automatically default to different BLASes based on the CPU architecture already as part of the design! So that covers most of the variation, but there are a few major tips to note when fine tuning the results to your system:","category":"page"},{"location":"tutorials/accelerating_choices/","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"One of the best methods for size 150x150 matrices and below is RecursiveFactorization.jl. This is a pure Julia BLAS system, but it has a high load time overhead, and thus as of v7 it's no longer loaded by default! Thus if your matrices are in this range and you would value better run times at the cost of compile and load times, it is recommended you add using RecursiveFactorization. The defaulting algorithm will then consider it in its list and will automatically (in an architecture-specific way) insert it as it feels necessary.\nOne of the major factors that can inhibit BLAS performance on LU factorization is multithreading. In many of these plots you can see a giant dip in GFLOPs (higher is better) when a certain size threshold is hit. This is because, for the number of chosen threads, there was not enough work and thus when the threading threshold is hit you get a hit to the performance due to the added overhead. The threading performance can be a per-system thing, and it can be greatly influenced by the number of cores on your system and the number of threads you allow. Thus for example, OpenBLAS' LU factorization seems to generally be really bad at guessing the thread switch point for CPUs with really high core/thread counts. If this is the case, you may want to investigate decreasing your number of BLAS threads, i.e. via BLAS.set_num_threads(i). Note that RecursiveFactorization.jl uses your Julia thread pool instead of the BLAS threads.\nThe switch points between algorithms can be fairly inexact. LinearSolve.jl tried to keep a tab on where they are per platform and keep updated, but it can be a moving battle. You may be able to eek out some performance by testing between the various options on your platform, i.e. RFLUFactorization vs LUFactorization vs AppleAccelerateLUFactorization (M-series) vs MKLFactorization (X86) and hardcoding the choice for your problem if the default did not make the right guess.","category":"page"},{"location":"tutorials/accelerating_choices/","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"warn: Warn\nAs noted, RecursiveFactorization.jl is one of the fastest linear solvers for smaller dense matrices but requires using RecursiveFactorization in order to be used in the default solver setups! Thus it's recommended that any optimized code or benchmarks sets this up.","category":"page"},{"location":"tutorials/accelerating_choices/#Understanding-Performance-of-Sparse-Linear-Solves","page":"Accelerating your Linear Solves","title":"Understanding Performance of Sparse Linear Solves","text":"","category":"section"},{"location":"tutorials/accelerating_choices/","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"Sparse linear solvers are not as dependent on the CPU but highly dependent on the problem that is being solved. For example, this is for a 1D laplacian vs a 3D laplacian, changing N to make smaller and bigger versions:","category":"page"},{"location":"tutorials/accelerating_choices/","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"(Image: Sparse Linear Solve Benchmarks)","category":"page"},{"location":"tutorials/accelerating_choices/","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"Notice that the optimal linear solver changes based on problem (i.e. sparsity pattern) and size. LinearSolve.jl just uses a very simple \"if small then use KLU and if large use UMFPACK\", which is validated by this plot, but leaves a lot to be desired. In particular, the following rules should be thought about:","category":"page"},{"location":"tutorials/accelerating_choices/","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"Pardiso is a great solver, you should try using Pardiso and using MKLPardiso() in many scenarios.\nThe more structured a sparsity pattern is, the worse KLU is in comparison to the other algorithms.\nA Krylov subspace method with proper preconditioning will be better than direct solvers when the matrices get large enough. You could always precondition a sparse matrix with iLU as an easy choice, though the tolerance would need to be tuned in a problem-specific way. Please see the preconditioenrs page for more information on defining and using preconditioners.","category":"page"},{"location":"tutorials/accelerating_choices/","page":"Accelerating your Linear Solves","title":"Accelerating your Linear Solves","text":"note: Note\nUMFPACK does better when the BLAS is not OpenBLAS. Try using MKL on Intel and AMD Ryzen platforms and UMPACK will be faster! LinearSolve.jl cannot default to this as this changes global settings and thus only defaults to MKL locally, and thus cannot change the setting within UMFPACK.","category":"page"},{"location":"tutorials/gpu/#GPU-Accelerated-Linear-Solving-in-Julia","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"","category":"section"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"LinearSolve.jl provides two ways to GPU accelerate linear solves:","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"Offloading: offloading takes a CPU-based problem and automatically transforms it into a GPU-based problem in the background, and returns the solution on CPU. Thus using offloading requires no change on the part of the user other than to choose an offloading solver.\nArray type interface: the array type interface requires that the user defines the LinearProblem using an AbstractGPUArray type and chooses an appropriate solver (or uses the default solver). The solution will then be returned as a GPU array type.","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"The offloading approach has the advantage of being simpler and requiring no change to existing CPU code, while having the disadvantage of having more overhead. In the following sections we will demonstrate how to use each of the approaches.","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"warn: Warn\nGPUs are not always faster! Your matrices need to be sufficiently large in order for GPU accelerations to actually be faster. For offloading it's around 1,000 x 1,000 matrices and for Array type interface it's around 100 x 100. For sparse matrices, it is highly dependent on the sparsity pattern and the amount of fill-in.","category":"page"},{"location":"tutorials/gpu/#GPU-Offloading","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Offloading","text":"","category":"section"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"GPU offloading is simple as it's done simply by changing the solver algorithm. Take the example from the start of the documentation:","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"import LinearSolve as LS\n\nA = rand(4, 4)\nb = rand(4)\nprob = LS.LinearProblem(A, b)\nsol = LS.solve(prob)\nsol.u","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"This computation can be moved to the GPU by the following:","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"using CUDA # Add the GPU library for NVIDIA GPUs\nsol = LS.solve(prob, LS.CudaOffloadLUFactorization())\n# or\nsol = LS.solve(prob, LS.CudaOffloadQRFactorization())\nsol.u","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"For AMD GPUs, you can use the AMDGPU.jl package:","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"using AMDGPU # Add the GPU library for AMD GPUs\nsol = LS.solve(prob, LS.AMDGPUOffloadLUFactorization())  # LU factorization\n# or\nsol = LS.solve(prob, LS.AMDGPUOffloadQRFactorization())  # QR factorization\nsol.u","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"LinearSolve.jl provides multiple  GPU offloading algorithms:","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"CudaOffloadLUFactorization() - Uses LU factorization on NVIDIA GPUs (generally faster for well-conditioned problems)\nCudaOffloadQRFactorization() - Uses QR factorization on NVIDIA GPUs (more stable for ill-conditioned problems)\nAMDGPUOffloadLUFactorization() - Uses LU factorization on AMD GPUs (generally faster for well-conditioned problems)\nAMDGPUOffloadQRFactorization() - Uses QR factorization on AMD GPUs (more stable for ill-conditioned problems)\n","category":"page"},{"location":"tutorials/gpu/#GPUArray-Interface","page":"GPU-Accelerated Linear Solving in Julia","title":"GPUArray Interface","text":"","category":"section"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"For more manual control over the factorization setup, you can use the GPUArray interface, the most common instantiation being CuArray for CUDA-based arrays on NVIDIA GPUs. To use this, we simply send the matrix A and the value b over to the GPU and solve:","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"using CUDA\n\nA = rand(4, 4) |> cu\nb = rand(4) |> cu\nprob = LS.LinearProblem(A, b)\nsol = LS.solve(prob)\nsol.u","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"4-element CuArray{Float32, 1, CUDA.DeviceMemory}:\n -27.02665\n  16.338171\n -77.650116\n 106.335686","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"Notice that the solution is a CuArray, and thus one must use Array(sol.u) if you with to return it to the CPU. This setup does no automated memory transfers and will thus only move things to CPU on command.","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"warn: Warn\nMany GPU functionalities, such as CUDA.cu, have a built-in preference for Float32. Generally it is much faster to use 32-bit floating point operations on GPU than 64-bit operations, and thus this is generally the right choice if going to such platforms. However, this change in numerical precision needs to be accounted for in your mathematics as it could lead to instabilities. To disable this, use a constructor that is more specific about the bitsize, such as CuArray{Float64}(A). Additionally, preferring more stable factorization methods, such as LS.QRFactorization(), can improve the numerics in such cases.","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"Similarly to other use cases, you can choose the solver, for example:","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"sol = LS.solve(prob, LS.QRFactorization())","category":"page"},{"location":"tutorials/gpu/#Sparse-Matrices-on-GPUs","page":"GPU-Accelerated Linear Solving in Julia","title":"Sparse Matrices on GPUs","text":"","category":"section"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"Currently, sparse matrix computations on GPUs are only supported for CUDA. This is done using the CUDA.CUSPARSE sublibrary.","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"import LinearAlgebra as LA\nimport SparseArrays as SA\nimport CUDA\nT = Float32\nn = 100\nA_cpu = SA.sprand(T, n, n, 0.05) + LA.I\nx_cpu = zeros(T, n)\nb_cpu = rand(T, n)\n\nA_gpu_csr = CuSparseMatrixCSR(A_cpu)\nb_gpu = CuVector(b_cpu)","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"In order to solve such problems using a direct method, you must add CUDSS.jl. This looks like:","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"using CUDSS\nsol = LS.solve(prob, LS.LUFactorization())","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"note: Note\nFor now, CUDSS only supports CuSparseMatrixCSR type matrices.","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"For high-performance sparse LU factorization on GPUs, you can also use CUSOLVERRF.jl:","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"using CUSOLVERRF\nsol = LS.solve(prob, LS.CUSOLVERRFFactorization())","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"CUSOLVERRF provides access to NVIDIA's cusolverRF library, which offers significant  performance improvements for sparse LU factorization on GPUs. It supports both  :RF (default) and :KLU symbolic factorization methods, and can reuse symbolic  factorization for matrices with the same sparsity pattern:","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"# Use KLU for symbolic factorization\nsol = LS.solve(prob, LS.CUSOLVERRFFactorization(symbolic = :KLU))\n\n# Reuse symbolic factorization for better performance\nsol = LS.solve(prob, LS.CUSOLVERRFFactorization(reuse_symbolic = true))","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"note: Note\nCUSOLVERRF only supports Float64 element types with Int32 indices.","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"Note that KrylovJL methods also work with sparse GPU arrays:","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"sol = LS.solve(prob, LS.KrylovJL_GMRES())","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"Note that CUSPARSE also has some GPU-based preconditioners, such as a built-in ilu. However:","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"sol = LS.solve(\n    prob, LS.KrylovJL_GMRES(precs = (A, p) -> (CUDA.CUSPARSE.ilu02!(A, 'O'), LA.I)))","category":"page"},{"location":"tutorials/gpu/","page":"GPU-Accelerated Linear Solving in Julia","title":"GPU-Accelerated Linear Solving in Julia","text":"However, right now CUSPARSE is missing the right ldiv! implementation for this to work in general. See https://github.com/SciML/LinearSolve.jl/issues/341 for details.","category":"page"},{"location":"basics/LinearProblem/#Linear-Problems","page":"Linear Problems","title":"Linear Problems","text":"","category":"section"},{"location":"basics/LinearProblem/#SciMLBase.LinearProblem","page":"Linear Problems","title":"SciMLBase.LinearProblem","text":"Defines a linear system problem. Documentation Page: https://docs.sciml.ai/LinearSolve/stable/basics/LinearProblem/\n\nMathematical Specification of a Linear Problem\n\nConcrete LinearProblem\n\nTo define a LinearProblem, you simply need to give the AbstractMatrix A and an AbstractVector b which defines the linear system:\n\nAu = b\n\nMatrix-Free LinearProblem\n\nFor matrix-free versions, the specification of the problem is given by an operator A(u,p,t) which computes A*u, or in-place as A(du,u,p,t). These are specified via the AbstractSciMLOperator interface. For more details, see the SciMLBase Documentation.\n\nNote that matrix-free versions of LinearProblem definitions are not compatible with all solvers. To check a solver for compatibility, use the function needs_concrete_A(alg::AbstractLinearAlgorithm).\n\nProblem Type\n\nConstructors\n\nOptionally, an initial guess u₀ can be supplied which is used for iterative methods.\n\nLinearProblem{isinplace}(A,b,p=NullParameters();u0=nothing,kwargs...)\nLinearProblem(f::AbstractSciMLOperator,b,p=NullParameters();u0=nothing,kwargs...)\n\nisinplace optionally sets whether the function is in-place or not, i.e. whether the solvers are allowed to mutate. By default this is true for AbstractMatrix, and for AbstractSciMLOperators it matches the choice of the operator definition.\n\nParameters are optional, and if not given, then a NullParameters() singleton will be used, which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers.\n\nFields\n\nA: The representation of the linear operator.\nb: The right-hand side of the linear system.\np: The parameters for the problem. Defaults to NullParameters. Currently unused.\nu0: The initial condition used by iterative solvers.\nsymbolic_interface: An instance of SymbolicLinearInterface if the problem was generated by a symbolic backend.\nkwargs: The keyword arguments passed on to the solvers.\n\n\n\n\n\n","category":"type"},{"location":"advanced/custom/#custom","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"","category":"section"},{"location":"advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"Julia users are building a wide variety of applications in the SciML ecosystem, often requiring problem-specific handling of their linear solves. As existing solvers in LinearSolve.jl may not be optimally suited for novel applications, it is essential for the linear solve interface to be easily extendable by users. To that end, the linear solve algorithm LS.LinearSolveFunction() accepts a user-defined function for handling the solve. A user can pass in their custom linear solve function, say my_linsolve, to LS.LinearSolveFunction(). A contrived example of solving a linear system with a custom solver is below.","category":"page"},{"location":"advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"import LinearSolve as LS\nimport LinearAlgebra as LA\n\nfunction my_linsolve(A, b, u, p, newA, Pl, Pr, solverdata; verbose = true, kwargs...)\n    if verbose == true\n        println(\"solving Ax=b\")\n    end\n    u = A \\ b\n    return u\nend\n\nprob = LS.LinearProblem(LA.Diagonal(rand(4)), rand(4))\nalg = LS.LinearSolveFunction(my_linsolve)\nsol = LS.solve(prob, alg)\nsol.u","category":"page"},{"location":"advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"The inputs to the function are as follows:","category":"page"},{"location":"advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"u, the solution initialized as zero(b),\nA, the linear operator\nb, the right-hand-side\np, a set of parameters\nnewA, a Bool which is true if A has been modified since last solve\nPl, left-preconditioner\nPr, right-preconditioner\nsolverdata, solver cache set to nothing if solver hasn't been initialized\nkwargs, standard SciML keyword arguments such as verbose, maxiters, abstol, reltol","category":"page"},{"location":"advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"The function my_linsolve must accept the above specified arguments and modify them, and return the solution, u. As memory for u is already allocated, the user may choose to modify u in place as follows:","category":"page"},{"location":"advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"function my_linsolve!(A, b, u, p, newA, Pl, Pr, solverdata; verbose = true, kwargs...)\n    if verbose == true\n        println(\"solving Ax=b\")\n    end\n    u .= A \\ b # in place\n    return u\nend\n\nalg = LS.LinearSolveFunction(my_linsolve!)\nsol = LS.solve(prob, alg)\nsol.u","category":"page"},{"location":"basics/OperatorAssumptions/#assumptions","page":"Linear Solve Operator Assumptions","title":"Linear Solve Operator Assumptions","text":"","category":"section"},{"location":"basics/OperatorAssumptions/#LinearSolve.OperatorAssumptions","page":"Linear Solve Operator Assumptions","title":"LinearSolve.OperatorAssumptions","text":"OperatorAssumptions(issquare = nothing; condition::OperatorCondition.T = IllConditioned)\n\nSets the operator A assumptions used as part of the default algorithm\n\n\n\n\n\n","category":"type"},{"location":"basics/OperatorAssumptions/#LinearSolve.OperatorCondition","page":"Linear Solve Operator Assumptions","title":"LinearSolve.OperatorCondition","text":"OperatorCondition\n\nSpecifies the assumption of matrix conditioning for the default linear solver choices. Condition number is defined as the ratio of eigenvalues. The numerical stability of many linear solver algorithms can be dependent on the condition number of the matrix. The condition number can be computed as:\n\nusing LinearAlgebra\ncond(rand(100, 100))\n\nHowever, in practice this computation is very expensive and thus not possible for most practical cases. Therefore, OperatorCondition lets one share to LinearSolve the expected conditioning. The higher the expected condition number, the safer the algorithm needs to be and thus there is a trade-off between numerical performance and stability. By default the method assumes the operator may be ill-conditioned for the standard linear solvers to converge (such as LU-factorization), though more extreme ill-conditioning or well-conditioning could be the case and specified through this assumption.\n\n\n\n\n\n","category":"module"},{"location":"basics/OperatorAssumptions/#Condition-Number-Specifications","page":"Linear Solve Operator Assumptions","title":"Condition Number Specifications","text":"","category":"section"},{"location":"basics/OperatorAssumptions/#LinearSolve.OperatorCondition.IllConditioned","page":"Linear Solve Operator Assumptions","title":"LinearSolve.OperatorCondition.IllConditioned","text":"OperatorCondition.IllConditioned\n\nThe default assumption of LinearSolve. Assumes that the operator can have minor ill-conditioning and thus needs to use safe algorithms.\n\n\n\n\n\n","category":"constant"},{"location":"basics/OperatorAssumptions/#LinearSolve.OperatorCondition.VeryIllConditioned","page":"Linear Solve Operator Assumptions","title":"LinearSolve.OperatorCondition.VeryIllConditioned","text":"OperatorCondition.VeryIllConditioned\n\nAssumes that the operator can have fairly major ill-conditioning and thus the standard linear algebra algorithms cannot be used.\n\n\n\n\n\n","category":"constant"},{"location":"basics/OperatorAssumptions/#LinearSolve.OperatorCondition.SuperIllConditioned","page":"Linear Solve Operator Assumptions","title":"LinearSolve.OperatorCondition.SuperIllConditioned","text":"OperatorCondition.SuperIllConditioned\n\nAssumes that the operator can have fairly extreme ill-conditioning and thus the most stable algorithm is used.\n\n\n\n\n\n","category":"constant"},{"location":"basics/OperatorAssumptions/#LinearSolve.OperatorCondition.WellConditioned","page":"Linear Solve Operator Assumptions","title":"LinearSolve.OperatorCondition.WellConditioned","text":"OperatorCondition.WellConditioned\n\nAssumes that the operator can have fairly contained conditioning and thus the fastest algorithm is used.\n\n\n\n\n\n","category":"constant"},{"location":"basics/FAQ/#Frequently-Asked-Questions","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"","category":"section"},{"location":"basics/FAQ/#How-is-LinearSolve.jl-compared-to-just-using-normal-\\,-i.e.-A\\b?","page":"Frequently Asked Questions","title":"How is LinearSolve.jl compared to just using normal \\, i.e. A\\b?","text":"","category":"section"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Check out this video from JuliaCon 2022 which goes into detail on how and why LinearSolve.jl can be a more general and efficient interface.","category":"page"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Note that if \\ is good enough for you, great! We still tend to use \\ in the REPL all the time! However, if you're building a package, you may want to consider using LinearSolve.jl for the improved efficiency and ability to choose solvers.","category":"page"},{"location":"basics/FAQ/#I'm-seeing-some-dynamic-dispatches-in-the-default-algorithm-choice,-how-do-I-reduce-that?","page":"Frequently Asked Questions","title":"I'm seeing some dynamic dispatches in the default algorithm choice, how do I reduce that?","text":"","category":"section"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Make sure you set the OperatorAssumptions to get the full performance, especially the issquare choice as otherwise that will need to be determined at runtime.","category":"page"},{"location":"basics/FAQ/#I-found-a-faster-algorithm-that-can-be-used-than-what-LinearSolve.jl-chose?","page":"Frequently Asked Questions","title":"I found a faster algorithm that can be used than what LinearSolve.jl chose?","text":"","category":"section"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"What assumptions are made as part of your method? If your method only works on well-conditioned operators, then make sure you set the WellConditioned assumption in the assumptions. See the OperatorAssumptions page for more details. If using the right assumptions does not improve the performance to the expected state, please open an issue and we will improve the default algorithm.","category":"page"},{"location":"basics/FAQ/#Python's-NumPy/SciPy-just-calls-fast-Fortran/C-code,-why-would-LinearSolve.jl-be-any-better?","page":"Frequently Asked Questions","title":"Python's NumPy/SciPy just calls fast Fortran/C code, why would LinearSolve.jl be any better?","text":"","category":"section"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"This is addressed in the JuliaCon 2022 video. This happens in a few ways:","category":"page"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"The Fortran/C code that NumPy/SciPy uses is actually slow. It's OpenBLAS, a library developed in part by the Julia Lab back in 2012 as a fast open source BLAS implementation. Many open source environments now use this build, including many R distributions. However, the Julia Lab has greatly improved its ability to generate optimized SIMD in platform-specific ways. This, and improved multithreading support (OpenBLAS's multithreading is rather slow), has led to pure Julia-based BLAS implementations which the lab now works on. This includes RecursiveFactorization.jl which generally outperforms OpenBLAS by 2x-10x depending on the platform. It even outperforms MKL for small matrices (<100). LinearSolve.jl uses RecursiveFactorization.jl by default sometimes, but switches to BLAS when it would be faster (in a platform and matrix-specific way).\nStandard approaches to handling linear solves re-allocate the pivoting vector each time. This leads to GC pauses that can slow down calculations. LinearSolve.jl has proper caches for fully preallocated no-GC workflows.\nLinearSolve.jl makes many other optimizations, like factorization reuse and symbolic factorization reuse, automatic. Many of these optimizations are not even possible from the high-level APIs of things like Python's major libraries and MATLAB.\nLinearSolve.jl has a much more extensive set of sparse matrix solvers, which is why you see a major difference (2x-10x) for sparse matrices. Which sparse matrix solver between KLU, UMFPACK, Pardiso, etc. is optimal depends a lot on matrix sizes, sparsity patterns, and threading overheads. LinearSolve.jl's heuristics handle these kinds of issues.","category":"page"},{"location":"basics/FAQ/#How-do-I-use-IterativeSolvers-solvers-with-a-weighted-tolerance-vector?","page":"Frequently Asked Questions","title":"How do I use IterativeSolvers solvers with a weighted tolerance vector?","text":"","category":"section"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"IterativeSolvers.jl computes the norm after the application of the left preconditioner. Thus, in order to use a vector tolerance weights, one can mathematically hack the system via the following formulation:","category":"page"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"import LinearSolve as LS\nimport LinearAlgebra as LA\n\nn = 2\nA = rand(n, n)\nb = rand(n)\n\nweights = [1e-1, 1]\nprecs = Returns((LS.InvPreconditioner(LA.Diagonal(weights)), LA.Diagonal(weights)))\n\nprob = LS.LinearProblem(A, b)\nsol = LS.solve(prob, LS.KrylovJL_GMRES(precs))\n\nsol.u","category":"page"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"If you want to use a “real” preconditioner under the norm weights, then one can use ComposePreconditioner to apply the preconditioner after the application of the weights like as follows:","category":"page"},{"location":"basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"import LinearSolve as LS\nimport LinearAlgebra as LA\n\nn = 4\nA = rand(n, n)\nb = rand(n)\n\nweights = rand(n)\nrealprec = LA.lu(rand(n, n)) # some random preconditioner\nPl = LS.ComposePreconditioner(LS.InvPreconditioner(LA.Diagonal(weights)),\n    realprec)\nPr = LA.Diagonal(weights)\n\nprob = LS.LinearProblem(A, b)\nsol = LS.solve(prob, LS.KrylovJL_GMRES(precs = Returns((Pl, Pr))))","category":"page"},{"location":"advanced/developing/#Developing-New-Linear-Solvers","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"","category":"section"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"Developing new or custom linear solvers for the SciML interface can be done in one of two ways:","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"You can either create a completely new set of dispatches for init and solve.\nYou can extend LinearSolve.jl's internal mechanisms.","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"For developer ease, we highly recommend (2) as that will automatically make the caching API work. Thus, this is the documentation for how to do that.","category":"page"},{"location":"advanced/developing/#Developing-New-Linear-Solvers-with-LinearSolve.jl-Primitives","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers with LinearSolve.jl Primitives","text":"","category":"section"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"Let's create a new wrapper for a simple LU-factorization which uses only the basic machinery. A simplified version is:","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"struct MyLUFactorization{P} <: LinearSolve.SciMLLinearSolveAlgorithm end\n\nfunction LinearSolve.init_cacheval(\n        alg::MyLUFactorization, A, b, u, Pl, Pr, maxiters::Int, abstol, reltol,\n        verbose::Bool, assump::LinearSolve.OperatorAssumptions)\n    lu!(convert(AbstractMatrix, A))\nend\n\nfunction SciMLBase.solve!(cache::LinearSolve.LinearCache, alg::MyLUFactorization; kwargs...)\n    if cache.isfresh\n        A = cache.A\n        A = convert(AbstractMatrix, A)\n        fact = lu!(A)\n        cache.cacheval = fact\n        cache.isfresh = false\n    end\n    y = ldiv!(cache.u, cache.cacheval, cache.b)\n    SciMLBase.build_linear_solution(alg, y, nothing, cache)\nend","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"The way this works is as follows. LinearSolve.jl has a LinearCache that everything shares (this is what gives most of the ease of use). However, many algorithms need to cache their own things, and so there's one value cacheval that is for the algorithms to modify. The function:","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"init_cacheval(\n    alg::MyLUFactorization, A, b, u, Pl, Pr, maxiters, abstol, reltol, verbose, assump)","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"is what is called at init time to create the first cacheval. Note that this should match the type of the cache later used in solve as many algorithms, like those in OrdinaryDiffEq.jl, expect type-groundedness in the linear solver definitions. While there are cheaper ways to obtain this type for LU factorizations (specifically, ArrayInterface.lu_instance(A)), for a demonstration, this just performs an LU-factorization to get an LU{T, Matrix{T}} which it puts into the cacheval so it is typed for future use.","category":"page"},{"location":"advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"After the init_cacheval, the only thing left to do is to define SciMLBase.solve!(cache::LinearCache, alg::MyLUFactorization). Many algorithms may use a lazy matrix-free representation of the operator A. Thus, if the algorithm requires a concrete matrix, like LU-factorization does, the algorithm should convert(AbstractMatrix,cache.A). The flag cache.isfresh states whether A has changed since the last solve. Since we only need to factorize when A is new, the factorization part of the algorithm is done in a if cache.isfresh. cache.cacheval = fact; cache.isfresh = false puts the new factorization into the cache, so it's updated for future solves. Then y = ldiv!(cache.u, cache.cacheval, cache.b) performs the solve and a linear solution is returned via SciMLBase.build_linear_solution(alg,y,nothing,cache).","category":"page"},{"location":"basics/Preconditioners/#prec","page":"Preconditioners","title":"Preconditioners","text":"","category":"section"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"Many linear solvers can be accelerated by using what is known as a preconditioner, an approximation to the matrix inverse action which is cheap to evaluate. These can improve the numerical conditioning of the solver process and in turn improve the performance. LinearSolve.jl provides an interface for the definition of preconditioners which works with the wrapped iterative solver packages.","category":"page"},{"location":"basics/Preconditioners/#Using-Preconditioners","page":"Preconditioners","title":"Using Preconditioners","text":"","category":"section"},{"location":"basics/Preconditioners/#Mathematical-Definition","page":"Preconditioners","title":"Mathematical Definition","text":"","category":"section"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"A right preconditioner, P_r transforms the linear system Au = b into the form:","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"AP_r^-1(P_r u) = AP_r^-1y = b","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"which is solved for y, and then P_r u = y is solved for u. The left preconditioner, P_l, transforms the linear system into the form:","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"P_l^-1Au = P_l^-1b","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"A two-sided preconditioned system is of the form:","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"P_l^-1A P_r^-1 (P_r u) = P_l^-1b","category":"page"},{"location":"basics/Preconditioners/#Specifying-Preconditioners","page":"Preconditioners","title":"Specifying  Preconditioners","text":"","category":"section"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"One way to specify preconditioners uses the Pl and Pr  keyword arguments to init or solve: Pl for left and Pr for right preconditioner, respectively. By default, if no preconditioner is given, the preconditioner is assumed to be the identity I.","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"In the following, we will use a left sided diagonal (Jacobi) preconditioner.","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"import LinearSolve as LS\nimport LinearAlgebra as LA\nn = 4\n\nA = rand(n, n)\nb = rand(n)\n\nPl = LA.Diagonal(A)\n\nprob = LS.LinearProblem(A, b)\nsol = LS.solve(prob, LS.KrylovJL_GMRES(), Pl = Pl)\nsol.u","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"Alternatively, preconditioners can be specified via the  precs  argument to the constructor of an iterative solver specification. This argument shall deliver a factory method mapping A and a parameter p to a tuple (Pl,Pr) consisting a left and a right preconditioner.","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"import LinearSolve as LS\nimport LinearAlgebra as LA\nn = 4\n\nA = rand(n, n)\nb = rand(n)\n\nprob = LS.LinearProblem(A, b)\nsol = LS.solve(prob, LS.KrylovJL_GMRES(precs = (A, p) -> (LA.Diagonal(A), LA.I)))\nsol.u","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"This approach has the advantage that the specification of the preconditioner is possible without the knowledge of a concrete matrix A. It also allows to specify the preconditioner via a callable object and to  pass parameters to the constructor of the preconditioner instances. The example below also shows how to reuse the preconditioner once constructed for the subsequent solution of a modified problem.","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"import LinearSolve as LS\nimport LinearAlgebra as LA\n\nBase.@kwdef struct WeightedDiagonalPreconBuilder\n    w::Float64\nend\n\n(builder::WeightedDiagonalPreconBuilder)(A, p) = (builder.w * LA.Diagonal(A), LA.I)\n\nn = 4\nA = n * LA.I - rand(n, n)\nb = rand(n)\n\nprob = LS.LinearProblem(A, b)\nsol = LS.solve(prob, LS.KrylovJL_GMRES(precs = WeightedDiagonalPreconBuilder(w = 0.9)))\nsol.u\n\nB = A .+ 0.1\ncache = sol.cache\nLS.reinit!(cache, A = B, reuse_precs = true)\nsol = LS.solve!(cache, LS.KrylovJL_GMRES(precs = WeightedDiagonalPreconBuilder(w = 0.9)))\nsol.u","category":"page"},{"location":"basics/Preconditioners/#Preconditioner-Interface","page":"Preconditioners","title":"Preconditioner Interface","text":"","category":"section"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"To define a new preconditioner you define a Julia type which satisfies the following interface:","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"Base.eltype(::Preconditioner) (Required only for Krylov.jl)\nLinearAlgebra.ldiv!(::AbstractVector,::Preconditioner,::AbstractVector) and LinearAlgebra.ldiv!(::Preconditioner,::AbstractVector)","category":"page"},{"location":"basics/Preconditioners/#Curated-List-of-Pre-Defined-Preconditioners","page":"Preconditioners","title":"Curated List of Pre-Defined Preconditioners","text":"","category":"section"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"The following preconditioners match the interface of LinearSolve.jl.","category":"page"},{"location":"basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"LinearSolve.ComposePreconditioner(prec1,prec2): composes the preconditioners to apply prec1 before prec2.\nLinearSolve.InvPreconditioner(prec): inverts mul! and ldiv! in a preconditioner definition as a lazy inverse.\nLinearAlgera.Diagonal(s::Union{Number,AbstractVector}): the lazy Diagonal matrix type of Base.LinearAlgebra. Used for efficient construction of a diagonal preconditioner.\nOther Base.LinearAlgera types: all define the full Preconditioner interface.\nIncompleteLU.ilu: an implementation of the incomplete LU-factorization preconditioner. This requires A as a SparseMatrixCSC.\nPreconditioners.CholeskyPreconditioner(A, i): An incomplete Cholesky preconditioner with cut-off level i. Requires A as a AbstractMatrix and positive semi-definite.\nAlgebraicMultigrid: Implementations of the algebraic multigrid method. Must be converted to a preconditioner via AlgebraicMultigrid.aspreconditioner(AlgebraicMultigrid.precmethod(A)). Requires A as a AbstractMatrix. Provides the following methods:\nAlgebraicMultigrid.ruge_stuben(A)\nAlgebraicMultigrid.smoothed_aggregation(A)\nPyAMG: Implementations of the algebraic multigrid method. Must be converted to a preconditioner via PyAMG.aspreconditioner(PyAMG.precmethod(A)). Requires A as a AbstractMatrix. Provides the following methods:\nPyAMG.RugeStubenSolver(A)\nPyAMG.SmoothedAggregationSolver(A)\nILUZero.ILU0Precon(A::SparseMatrixCSC{T,N}, b_type = T): An incomplete LU implementation. Requires A as a SparseMatrixCSC.\nLimitedLDLFactorizations.lldl: A limited-memory LDLᵀ factorization for symmetric matrices. Requires A as a SparseMatrixCSC. Applying F = lldl(A); F.D .= abs.(F.D) before usage as a preconditioner makes the preconditioner symmetric positive definite and thus is required for Krylov methods which are specialized for symmetric linear systems.\nRandomizedPreconditioners.NystromPreconditioner A randomized sketching method for positive semidefinite matrices A. Builds a preconditioner P  A + μ*I for the system (A + μ*I)x = b.\nHYPRE.jl A set of solvers with preconditioners which supports distributed computing via MPI. These can be written using the LinearSolve.jl interface choosing algorithms like HYPRE.ILU and HYPRE.BoomerAMG.\nKrylovPreconditioners.jl: Provides GPU-ready preconditioners via KernelAbstractions.jl. At the time of writing the package provides the following methods:\nIncomplete Cholesky decomposition KrylovPreconditioners.kp_ic0(A)\nIncomplete LU decomposition KrylovPreconditioners.kp_ilu0(A)\nBlock Jacobi KrylovPreconditioners.kp_block_jacobi(A)","category":"page"},{"location":"tutorials/autotune/#Automatic-Algorithm-Selection-with-LinearSolveAutotune","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"LinearSolve.jl includes an automatic tuning system that benchmarks all available linear algebra algorithms on your specific hardware and automatically selects optimal algorithms for different problem sizes and data types. This tutorial will show you how to use the LinearSolveAutotune sublibrary to optimize your linear solve performance.","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"The autotuning system provides comprehensive benchmarking and automatic algorithm selection optimization for your specific hardware.","category":"page"},{"location":"tutorials/autotune/#Quick-Start","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Quick Start","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"The simplest way to use the autotuner is to run it with default settings:","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"using LinearSolve\nusing LinearSolveAutotune\n\n# Run autotune with default settings\nresults = autotune_setup()\n\n# View the results\ndisplay(results)\n\n# Generate performance plots\nplot(results)\n\n# Share results with the community (optional, requires GitHub authentication)\nshare_results(results)","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"This will:","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"Benchmark algorithms for Float64 matrices by default\nTest matrix sizes from tiny (5×5) through large (1000×1000) \nDisplay a summary of algorithm performance\nReturn an AutotuneResults object containing all benchmark data","category":"page"},{"location":"tutorials/autotune/#Understanding-the-Results","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Understanding the Results","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"The autotune_setup() function returns an AutotuneResults object containing:","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"results_df: A DataFrame with detailed benchmark results\nsysinfo: System information dictionary","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"You can explore the results in several ways:","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"# Get the results\nresults = autotune_setup()\n\n# Display a formatted summary\ndisplay(results)\n\n# Access the raw benchmark data\ndf = results.results_df\n\n# View system information\nsysinfo = results.sysinfo\n\n# Generate performance plots\nplot(results)\n\n# Filter to see successful benchmarks only\nusing DataFrames\nsuccessful = filter(row -> row.success, df)","category":"page"},{"location":"tutorials/autotune/#Customizing-the-Autotune-Process","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Customizing the Autotune Process","text":"","category":"section"},{"location":"tutorials/autotune/#Size-Categories","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Size Categories","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"Control which matrix size ranges to test:","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"# Available size categories:\n# :tiny   - 5×5 to 20×20 (very small problems)\n# :small  - 20×20 to 100×100 (small problems)  \n# :medium - 100×100 to 300×300 (typical problems)\n# :large  - 300×300 to 1000×1000 (larger problems)\n# :big    - 1000×1000 to 15000×15000 (GPU/HPC scale, capped at 15000 for stability)\n\n# Default: test tiny through large\nresults = autotune_setup()  # uses [:tiny, :small, :medium, :large]\n\n# Test only medium and large sizes\nresults = autotune_setup(sizes = [:medium, :large])\n\n# Include huge matrices (for GPU systems)\nresults = autotune_setup(sizes = [:large, :big])\n\n# Test all size categories\nresults = autotune_setup(sizes = [:tiny, :small, :medium, :large, :big])","category":"page"},{"location":"tutorials/autotune/#Element-Types","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Element Types","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"Specify which numeric types to benchmark:","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"# Default: Float64 only\nresults = autotune_setup()  # equivalent to eltypes = (Float64,)\n\n# Test standard floating point types\nresults = autotune_setup(eltypes = (Float32, Float64))\n\n# Include complex numbers\nresults = autotune_setup(eltypes = (Float64, ComplexF64))\n\n# Test all standard BLAS types\nresults = autotune_setup(eltypes = (Float32, Float64, ComplexF32, ComplexF64))\n\n# Test arbitrary precision (excludes some BLAS algorithms)\nresults = autotune_setup(eltypes = (BigFloat,), skip_missing_algs = true)","category":"page"},{"location":"tutorials/autotune/#Benchmark-Quality-vs-Speed","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Benchmark Quality vs Speed","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"Adjust the thoroughness of benchmarking:","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"# Quick benchmark (fewer samples, less time per test)\nresults = autotune_setup(samples = 1, seconds = 0.1)\n\n# Default benchmark (balanced)\nresults = autotune_setup(samples = 5, seconds = 0.5)\n\n# Thorough benchmark (more samples, more time per test)\nresults = autotune_setup(samples = 10, seconds = 2.0)\n\n# Production-quality benchmark for final tuning\nresults = autotune_setup(\n    samples = 20,\n    seconds = 5.0,\n    sizes = [:small, :medium, :large],\n    eltypes = (Float32, Float64, ComplexF32, ComplexF64)\n)","category":"page"},{"location":"tutorials/autotune/#Time-Limits-for-Algorithm-Tests","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Time Limits for Algorithm Tests","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"Control the maximum time allowed for each algorithm test (including accuracy check):","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"# Default: 100 seconds maximum per algorithm test\nresults = autotune_setup()  # maxtime = 100.0\n\n# Quick timeout for fast exploration\nresults = autotune_setup(maxtime = 10.0)\n\n# Extended timeout for slow algorithms or large matrices\nresults = autotune_setup(\n    maxtime = 300.0,  # 5 minutes per test\n    sizes = [:large, :big]\n)\n\n# Conservative timeout for production benchmarking\nresults = autotune_setup(\n    maxtime = 200.0,\n    samples = 10,\n    seconds = 2.0\n)","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"When an algorithm exceeds the maxtime limit:","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"The test is skipped to prevent hanging\nThe result is recorded as NaN in the benchmark data\nA warning is displayed indicating the timeout\nThe algorithm is automatically excluded from all larger matrix sizes to save time\nThe benchmark continues with the next algorithm","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"This intelligent timeout handling ensures that slow algorithms don't waste time on progressively larger matrices once they've proven too slow on smaller ones.","category":"page"},{"location":"tutorials/autotune/#Missing-Algorithm-Handling","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Missing Algorithm Handling","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"By default, autotune expects all algorithms to be available to ensure complete benchmarking. You can relax this requirement:","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"# Default: error if expected algorithms are missing\nresults = autotune_setup()  # Will error if RFLUFactorization is missing\n\n# Allow missing algorithms (useful for incomplete setups)\nresults = autotune_setup(skip_missing_algs = true)  # Will warn instead of error","category":"page"},{"location":"tutorials/autotune/#Preferences-Setting","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Preferences Setting","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"Control whether the autotuner updates LinearSolve preferences:","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"# Default: set preferences based on benchmark results\nresults = autotune_setup(set_preferences = true)\n\n# Benchmark only, don't change preferences\nresults = autotune_setup(set_preferences = false)","category":"page"},{"location":"tutorials/autotune/#GPU-Systems","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"GPU Systems","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"On systems with CUDA or Metal GPU support, the autotuner will automatically detect and benchmark GPU algorithms:","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"# Enable large matrix testing for GPUs\nresults = autotune_setup(\n    sizes = [:large, :big],\n    samples = 3,\n    seconds = 1.0\n)","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"GPU algorithms tested (when available):","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"CudaOffloadFactorization: CUDA GPU acceleration\nMetalLUFactorization: Apple Metal GPU acceleration","category":"page"},{"location":"tutorials/autotune/#Sharing-Results-with-the-Community","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Sharing Results with the Community","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"The autotuner includes a telemetry feature that allows you to share your benchmark results with the LinearSolve.jl community. This helps improve algorithm selection across different hardware configurations.","category":"page"},{"location":"tutorials/autotune/#Automatic-Authentication","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Authentication","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"New in v2.0+: LinearSolveAutotune now includes automatic authentication support! If you're not already authenticated, the system will offer to help you set up GitHub authentication when you run share_results().","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"# Run benchmarks\nresults = autotune_setup()\n\n# Share with the community - will prompt for authentication if needed\nshare_results(results)","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"If you're not authenticated, you'll see:","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"🔐 GitHub authentication not found.\n   To share results with the community, authentication is required.\n\nWould you like to authenticate with GitHub now? (y/n)\n> ","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"Simply type y and follow the prompts to authenticate directly from Julia!","category":"page"},{"location":"tutorials/autotune/#Manual-Authentication-Setup","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Manual Authentication Setup","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"You can also set up authentication manually before sharing:","category":"page"},{"location":"tutorials/autotune/#Method-1:-GitHub-CLI-(Recommended)","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Method 1: GitHub CLI (Recommended)","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"The GitHub CLI is the easiest way to authenticate. LinearSolveAutotune will automatically use the GitHub CLI if it's installed, or fall back to a bundled version if not.","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"Install GitHub CLI (Optional)\nmacOS: brew install gh\nWindows: winget install --id GitHub.cli\nLinux: See cli.github.com\nNote: If you don't have gh installed, LinearSolveAutotune includes a bundled version via gh_cli_jll that will be used automatically!\nAuthenticate\ngh auth login\nFollow the prompts to authenticate with your GitHub account.\nVerify authentication\ngh auth status","category":"page"},{"location":"tutorials/autotune/#Method-2:-GitHub-Personal-Access-Token","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Method 2: GitHub Personal Access Token","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"Go to GitHub Settings > Tokens\nAdd description: \"LinearSolve.jl Telemetry\"\nSelect scope: public_repo (for commenting on issues)\nClick \"Generate token\" and copy it\nIn Julia:\nENV[\"GITHUB_TOKEN\"] = \"your_token_here\"","category":"page"},{"location":"tutorials/autotune/#Sharing-Your-Results","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Sharing Your Results","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"Once authenticated (either automatically or manually), sharing is simple:","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"# Run benchmarks\nresults = autotune_setup()\n\n# Share with the community (with automatic authentication prompt)\nshare_results(results)\n\n# Or skip the authentication prompt if not authenticated\nshare_results(results; auto_login = false)","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"This will:","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"Check for existing GitHub authentication\nOffer to set up authentication if needed (unless auto_login = false)\nFormat your benchmark results as a markdown report\nPost the results as a comment to the community benchmark collection issue\nSave results locally if authentication is unavailable","category":"page"},{"location":"tutorials/autotune/#No-GitHub-CLI-Required!","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"No GitHub CLI Required!","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"LinearSolveAutotune now includes gh_cli_jll, which provides a bundled version of the GitHub CLI. This means:","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"You don't need to install gh separately\nAuthentication works on all platforms\nThe system automatically uses your existing gh installation if available, or falls back to the bundled version","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"info: Privacy Note\nSharing is completely optional\nOnly benchmark performance data and system specifications are shared\nNo personal information is collected\nAll shared data is publicly visible on GitHub\nIf authentication fails or is skipped, results are saved locally for manual sharing","category":"page"},{"location":"tutorials/autotune/#Working-with-Results","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Working with Results","text":"","category":"section"},{"location":"tutorials/autotune/#Examining-Performance-Data","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Examining Performance Data","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"using DataFrames\nusing Statistics\n\nresults = autotune_setup()\n\n# Access the raw DataFrame\ndf = results.results_df\n\n# Filter successful results\nsuccessful = filter(row -> row.success, df)\n\n# Summary by algorithm\nsummary = combine(groupby(successful, [:algorithm, :eltype]), \n                 :gflops => mean => :avg_gflops,\n                 :gflops => maximum => :max_gflops)\nsort!(summary, :avg_gflops, rev=true)\nprintln(summary)\n\n# Best algorithm for each size category\nby_size = combine(groupby(successful, [:size_category, :eltype])) do group\n    best_row = argmax(group.gflops)\n    return (algorithm = group.algorithm[best_row],\n            gflops = group.gflops[best_row])\nend\nprintln(by_size)","category":"page"},{"location":"tutorials/autotune/#Performance-Visualization","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Performance Visualization","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"Generate and save performance plots:","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"results = autotune_setup()\n\n# Generate plots (returns a combined plot)\np = plot(results)\ndisplay(p)\n\n# Save the plot\nusing Plots\nsavefig(p, \"benchmark_results.png\")","category":"page"},{"location":"tutorials/autotune/#Accessing-System-Information","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Accessing System Information","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"results = autotune_setup()\n\n# System information is stored in the results\nsysinfo = results.sysinfo\nprintln(\"CPU: \", sysinfo[\"cpu_name\"])\nprintln(\"Cores: \", sysinfo[\"num_cores\"])\nprintln(\"Julia: \", sysinfo[\"julia_version\"])\nprintln(\"OS: \", sysinfo[\"os\"])","category":"page"},{"location":"tutorials/autotune/#Advanced-Usage","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Advanced Usage","text":"","category":"section"},{"location":"tutorials/autotune/#Custom-Benchmark-Pipeline","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Custom Benchmark Pipeline","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"For complete control over the benchmarking process:","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"# Step 1: Run benchmarks without plotting or sharing\nresults = autotune_setup(\n    sizes = [:medium, :large],\n    eltypes = (Float64, ComplexF64),\n    set_preferences = false,  # Don't change preferences yet\n    samples = 10,\n    seconds = 1.0\n)\n\n# Step 2: Analyze results\ndf = results.results_df\n# ... perform custom analysis ...\n\n# Step 3: Generate plots\np = plot(results)\nsavefig(p, \"my_benchmarks.png\")\n\n# Step 4: Optionally share results\nshare_results(results)","category":"page"},{"location":"tutorials/autotune/#Batch-Testing-Multiple-Configurations","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Batch Testing Multiple Configurations","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"# Test different element types separately\nconfigs = [\n    (eltypes = (Float32,), name = \"float32\"),\n    (eltypes = (Float64,), name = \"float64\"),\n    (eltypes = (ComplexF64,), name = \"complex64\")\n]\n\nall_results = Dict()\nfor config in configs\n    println(\"Testing $(config.name)...\")\n    results = autotune_setup(\n        eltypes = config.eltypes,\n        sizes = [:small, :medium],\n        samples = 3\n    )\n    all_results[config.name] = results\nend","category":"page"},{"location":"tutorials/autotune/#Algorithm-Selection-Analysis","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Algorithm Selection Analysis","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"You can analyze what algorithms are currently being chosen for different matrix sizes:","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"using LinearSolve\n\n# Show current algorithm choices and preferences\nshow_algorithm_choices()","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"This displays:","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"Current autotune preferences for all element types (if any are set)\nAlgorithm choices for all element types across representative sizes in each category  \nComprehensive element type behavior (Float32, Float64, ComplexF32, ComplexF64)\nSystem information (MKL, Apple Accelerate, RecursiveFactorization status)","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"The output shows a clear table format:","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"📊 Default Algorithm Choices:\nSize       Category    Float32            Float64            ComplexF32         ComplexF64\n8×8        tiny        GenericLUFactorization GenericLUFactorization GenericLUFactorization GenericLUFactorization\n200×200    medium      MKLLUFactorization MKLLUFactorization MKLLUFactorization MKLLUFactorization","category":"page"},{"location":"tutorials/autotune/#Preferences-Integration","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Preferences Integration","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"The autotuner sets preferences that LinearSolve.jl uses for automatic algorithm selection:","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"using LinearSolveAutotune\n\n# Run autotune and set preferences\nresults = autotune_setup(set_preferences = true)\n\n# View what algorithms are now being chosen\nusing LinearSolve\nshow_algorithm_choices()\n\n# View current preferences\nLinearSolveAutotune.show_current_preferences()\n\n# Clear all autotune preferences if needed\nLinearSolveAutotune.clear_algorithm_preferences()","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"After running autotune with set_preferences = true, LinearSolve.jl will automatically use the fastest algorithms found for each matrix size and element type, with intelligent fallbacks when extensions are not available.","category":"page"},{"location":"tutorials/autotune/#Troubleshooting","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Troubleshooting","text":"","category":"section"},{"location":"tutorials/autotune/#Common-Issues","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Common Issues","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"Missing algorithms error\n# If you get errors about missing algorithms:\nresults = autotune_setup(skip_missing_algs = true)\nGitHub authentication fails\nEnsure gh CLI is installed and authenticated: gh auth status\nOr set a valid GitHub token: ENV[\"GITHUB_TOKEN\"] = \"your_token\"\nResults will be saved locally if authentication fails\nOut of memory on large matrices\n# Use smaller size categories\nresults = autotune_setup(sizes = [:tiny, :small, :medium])\nBenchmarks taking too long\n# Reduce samples and time per benchmark\nresults = autotune_setup(samples = 1, seconds = 0.1)","category":"page"},{"location":"tutorials/autotune/#Summary","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Summary","text":"","category":"section"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"LinearSolveAutotune provides a comprehensive system for benchmarking and optimizing LinearSolve.jl performance on your specific hardware. Key features include:","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"Flexible size categories from tiny to GPU-scale matrices\nSupport for all standard numeric types\nAutomatic GPU algorithm detection\nCommunity result sharing via GitHub\nPerformance visualization\nPreference setting for automatic algorithm selection (in development)","category":"page"},{"location":"tutorials/autotune/","page":"Automatic Algorithm Selection with LinearSolveAutotune","title":"Automatic Algorithm Selection with LinearSolveAutotune","text":"By running autotune and optionally sharing your results, you help improve LinearSolve.jl's performance for everyone in the Julia community.","category":"page"},{"location":"basics/algorithm_selection/#Algorithm-Selection-Guide","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"","category":"section"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"LinearSolve.jl automatically selects appropriate algorithms based on your problem characteristics, but understanding how this works can help you make better choices for your specific use case.","category":"page"},{"location":"basics/algorithm_selection/#Automatic-Algorithm-Selection","page":"Algorithm Selection Guide","title":"Automatic Algorithm Selection","text":"","category":"section"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"When you call solve(prob) without specifying an algorithm, LinearSolve.jl uses intelligent heuristics to choose the best solver:","category":"page"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"using LinearSolve\n\n# LinearSolve.jl automatically chooses the best algorithm\nA = rand(100, 100)\nb = rand(100)\nprob = LinearProblem(A, b)\nsol = solve(prob)  # Automatic algorithm selection","category":"page"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"The selection process considers:","category":"page"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"Matrix type: Dense vs. sparse vs. structured matrices\nMatrix properties: Square vs. rectangular, symmetric, positive definite\nSize: Small vs. large matrices for performance optimization  \nHardware: CPU vs. GPU arrays\nConditioning: Well-conditioned vs. ill-conditioned systems","category":"page"},{"location":"basics/algorithm_selection/#Algorithm-Categories","page":"Algorithm Selection Guide","title":"Algorithm Categories","text":"","category":"section"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"LinearSolve.jl organizes algorithms into several categories:","category":"page"},{"location":"basics/algorithm_selection/#Factorization-Methods","page":"Algorithm Selection Guide","title":"Factorization Methods","text":"","category":"section"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"These algorithms decompose your matrix into simpler components:","category":"page"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"Dense factorizations: Best for matrices without special sparsity structure\nLUFactorization(): General-purpose, good balance of speed and stability\nQRFactorization(): More stable for ill-conditioned problems\nCholeskyFactorization(): Fastest for symmetric positive definite matrices\nSparse factorizations: Optimized for matrices with many zeros\nUMFPACKFactorization(): General sparse LU with good fill-in control\nKLUFactorization(): Optimized for circuit simulation problems","category":"page"},{"location":"basics/algorithm_selection/#Iterative-Methods","page":"Algorithm Selection Guide","title":"Iterative Methods","text":"","category":"section"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"These solve the system iteratively without explicit factorization:","category":"page"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"Krylov methods: Memory-efficient for large sparse systems\nKrylovJL_GMRES(): General-purpose iterative solver\nKrylovJL_CG(): For symmetric positive definite systems","category":"page"},{"location":"basics/algorithm_selection/#Direct-Methods","page":"Algorithm Selection Guide","title":"Direct Methods","text":"","category":"section"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"Simple direct approaches:","category":"page"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"DirectLdiv!(): Uses Julia's built-in \\ operator\nDiagonalFactorization(): Optimized for diagonal matrices","category":"page"},{"location":"basics/algorithm_selection/#Performance-Characteristics","page":"Algorithm Selection Guide","title":"Performance Characteristics","text":"","category":"section"},{"location":"basics/algorithm_selection/#Dense-Matrices","page":"Algorithm Selection Guide","title":"Dense Matrices","text":"","category":"section"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"For dense matrices, algorithm choice depends on size and conditioning:","category":"page"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"# Small matrices (< 100×100): SimpleLUFactorization often fastest\nA_small = rand(50, 50)\nsol = solve(LinearProblem(A_small, rand(50)), SimpleLUFactorization())\n\n# Medium matrices (100×500): RFLUFactorization often optimal  \nA_medium = rand(200, 200)\nsol = solve(LinearProblem(A_medium, rand(200)), RFLUFactorization())\n\n# Large matrices (> 500×500): MKLLUFactorization, OpenBLASLUFactorization, or AppleAccelerate\nA_large = rand(1000, 1000) \nsol = solve(LinearProblem(A_large, rand(1000)), MKLLUFactorization())\n# Alternative: OpenBLASLUFactorization() for direct OpenBLAS calls","category":"page"},{"location":"basics/algorithm_selection/#Sparse-Matrices","page":"Algorithm Selection Guide","title":"Sparse Matrices","text":"","category":"section"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"For sparse matrices, structure matters:","category":"page"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"using SparseArrays\n\n# General sparse matrices\nA_sparse = sprand(1000, 1000, 0.01)\nsol = solve(LinearProblem(A_sparse, rand(1000)), UMFPACKFactorization())\n\n# Structured sparse (e.g., from discretized PDEs)\n# KLUFactorization often better for circuit-like problems","category":"page"},{"location":"basics/algorithm_selection/#GPU-Acceleration","page":"Algorithm Selection Guide","title":"GPU Acceleration","text":"","category":"section"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"For very large problems, GPU offloading can be beneficial:","category":"page"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"# Requires CUDA.jl\n# A_gpu = CuArray(rand(Float32, 2000, 2000))\n# sol = solve(LinearProblem(A_gpu, CuArray(rand(Float32, 2000))), \n#            CudaOffloadLUFactorization())","category":"page"},{"location":"basics/algorithm_selection/#When-to-Override-Automatic-Selection","page":"Algorithm Selection Guide","title":"When to Override Automatic Selection","text":"","category":"section"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"You might want to manually specify an algorithm when:","category":"page"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"You know your problem structure: E.g., you know your matrix is positive definite\nsol = solve(prob, CholeskyFactorization())  # Faster for SPD matrices\nYou need maximum stability: For ill-conditioned problems\nsol = solve(prob, QRFactorization())  # More numerically stable\nYou're doing many solves: Factorization methods amortize cost over multiple solves\ncache = init(prob, LUFactorization())\nfor i in 1:1000\n    cache.b = new_rhs[i]\n    sol = solve!(cache)\nend\nMemory constraints: Iterative methods use less memory\nsol = solve(prob, KrylovJL_GMRES())  # Lower memory usage","category":"page"},{"location":"basics/algorithm_selection/#Algorithm-Selection-Flowchart","page":"Algorithm Selection Guide","title":"Algorithm Selection Flowchart","text":"","category":"section"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"The automatic selection roughly follows this logic:","category":"page"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"Is A diagonal? → DiagonalFactorization\nIs A tridiagonal/bidiagonal? → DirectLdiv! (Julia 1.11+) or LUFactorization  \nIs A symmetric positive definite? → CholeskyFactorization\nIs A symmetric indefinite? → BunchKaufmanFactorization\nIs A sparse? → UMFPACKFactorization or KLUFactorization\nIs A small dense? → RFLUFactorization or SimpleLUFactorization\nIs A large dense? → MKLLUFactorization, OpenBLASLUFactorization, or AppleAccelerateLUFactorization\nIs A GPU array? → QRFactorization or LUFactorization\nIs A an operator/function? → KrylovJL_GMRES\nIs the system overdetermined? → QRFactorization or KrylovJL_LSMR","category":"page"},{"location":"basics/algorithm_selection/#Custom-Functions","page":"Algorithm Selection Guide","title":"Custom Functions","text":"","category":"section"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"For specialized algorithms not covered by the built-in solvers:","category":"page"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"function my_custom_solver(A, b, u, p, isfresh, Pl, Pr, cacheval; kwargs...)\n    # Your custom solving logic here\n    return A \\ b  # Simple example\nend\n\nsol = solve(prob, LinearSolveFunction(my_custom_solver))","category":"page"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"See the Custom Linear Solvers section for more details.","category":"page"},{"location":"basics/algorithm_selection/#Tuned-Algorithm-Selection","page":"Algorithm Selection Guide","title":"Tuned Algorithm Selection","text":"","category":"section"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"LinearSolve.jl includes a sophisticated preference system that can be tuned using LinearSolveAutotune for optimal performance on your specific hardware:","category":"page"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"using LinearSolve\nusing LinearSolveAutotune\n\n# Run autotune to benchmark algorithms and set preferences\nresults = autotune_setup(set_preferences = true)\n\n# View what algorithms are now being chosen\nshow_algorithm_choices()","category":"page"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"The system automatically sets preferences for:","category":"page"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"Different matrix sizes: tiny (≤20), small (21-100), medium (101-300), large (301-1000), big (>1000)\nDifferent element types: Float32, Float64, ComplexF32, ComplexF64\nDual preferences: Best overall algorithm + best always-available fallback","category":"page"},{"location":"basics/algorithm_selection/#Viewing-Algorithm-Choices","page":"Algorithm Selection Guide","title":"Viewing Algorithm Choices","text":"","category":"section"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"Use show_algorithm_choices() to see what algorithms are currently being selected:","category":"page"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"using LinearSolve\nshow_algorithm_choices()","category":"page"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"This shows a comprehensive analysis:","category":"page"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"Current autotune preferences for all element types (if set)\nAlgorithm choices for all element types across all size categories\nSide-by-side comparison showing Float32, Float64, ComplexF32, ComplexF64 behavior\nSystem information (available extensions: MKL, Apple Accelerate, RecursiveFactorization)","category":"page"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"Example output:","category":"page"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"📊 Default Algorithm Choices:\nSize       Category    Float32            Float64            ComplexF32         ComplexF64\n8×8        tiny        GenericLUFactorization GenericLUFactorization GenericLUFactorization GenericLUFactorization\n50×50      small       MKLLUFactorization MKLLUFactorization MKLLUFactorization MKLLUFactorization\n200×200    medium      MKLLUFactorization GenericLUFactorization MKLLUFactorization MKLLUFactorization","category":"page"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"When preferences are set, you can see exactly how they affect algorithm choice across different element types.","category":"page"},{"location":"basics/algorithm_selection/#Preference-System-Benefits","page":"Algorithm Selection Guide","title":"Preference System Benefits","text":"","category":"section"},{"location":"basics/algorithm_selection/","page":"Algorithm Selection Guide","title":"Algorithm Selection Guide","text":"Automatic optimization: Uses the fastest algorithms found by benchmarking\nIntelligent fallbacks: Falls back to always-available algorithms when extensions aren't loaded\nSize-specific tuning: Different algorithms optimized for different matrix sizes\nType-specific tuning: Optimized algorithm selection for different numeric types","category":"page"},{"location":"advanced/internal_api/#Internal-API-Documentation","page":"Internal API Documentation","title":"Internal API Documentation","text":"","category":"section"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"This page documents LinearSolve.jl's internal API, which is useful for developers who want to understand the package's architecture, contribute to the codebase, or develop custom linear solver algorithms.","category":"page"},{"location":"advanced/internal_api/#Abstract-Type-Hierarchy","page":"Internal API Documentation","title":"Abstract Type Hierarchy","text":"","category":"section"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"LinearSolve.jl uses a well-structured type hierarchy to organize different classes of linear solver algorithms:","category":"page"},{"location":"advanced/internal_api/#LinearSolve.SciMLLinearSolveAlgorithm","page":"Internal API Documentation","title":"LinearSolve.SciMLLinearSolveAlgorithm","text":"SciMLLinearSolveAlgorithm <: SciMLBase.AbstractLinearAlgorithm\n\nThe root abstract type for all linear solver algorithms in LinearSolve.jl. All concrete linear solver implementations should inherit from one of the specialized subtypes rather than directly from this type.\n\nThis type integrates with the SciMLBase ecosystem, providing a consistent interface for linear algebra operations across the Julia scientific computing ecosystem.\n\n\n\n\n\n","category":"type"},{"location":"advanced/internal_api/#LinearSolve.AbstractFactorization","page":"Internal API Documentation","title":"LinearSolve.AbstractFactorization","text":"AbstractFactorization <: SciMLLinearSolveAlgorithm\n\nAbstract type for linear solvers that work by computing a matrix factorization. These algorithms typically decompose the matrix A into a product of simpler matrices (e.g., A = LU, A = QR, A = LDL') and then solve the system using forward/backward substitution.\n\nCharacteristics\n\nRequires concrete matrix representation (needs_concrete_A() = true)\nTypically efficient for multiple solves with the same matrix\nGenerally provides high accuracy for well-conditioned problems\nMemory requirements depend on the specific factorization type\n\nSubtypes\n\nAbstractDenseFactorization: For dense matrix factorizations\nAbstractSparseFactorization: For sparse matrix factorizations\n\nExamples of concrete subtypes\n\nLUFactorization, QRFactorization, CholeskyFactorization\nUMFPACKFactorization, KLUFactorization\n\n\n\n\n\n","category":"type"},{"location":"advanced/internal_api/#LinearSolve.AbstractDenseFactorization","page":"Internal API Documentation","title":"LinearSolve.AbstractDenseFactorization","text":"AbstractDenseFactorization <: AbstractFactorization\n\nAbstract type for factorization-based linear solvers optimized for dense matrices. These algorithms assume the matrix has no particular sparsity structure and use dense linear algebra routines (typically from BLAS/LAPACK) for optimal performance.\n\nCharacteristics\n\nOptimized for matrices with few zeros or no sparsity structure\nLeverage highly optimized BLAS/LAPACK routines when available\nGenerally provide excellent performance for moderately-sized dense problems\nMemory usage scales as O(n²) with matrix size\n\nExamples of concrete subtypes\n\nLUFactorization: Dense LU with partial pivoting (via LAPACK)\nQRFactorization: Dense QR factorization for overdetermined systems\nCholeskyFactorization: Dense Cholesky for symmetric positive definite matrices\nBunchKaufmanFactorization: For symmetric indefinite matrices\n\n\n\n\n\n","category":"type"},{"location":"advanced/internal_api/#LinearSolve.AbstractSparseFactorization","page":"Internal API Documentation","title":"LinearSolve.AbstractSparseFactorization","text":"AbstractSparseFactorization <: AbstractFactorization\n\nAbstract type for factorization-based linear solvers optimized for sparse matrices. These algorithms take advantage of sparsity patterns to reduce memory usage and computational cost compared to dense factorizations.\n\nCharacteristics\n\nOptimized for matrices with many zero entries\nOften use specialized pivoting strategies to preserve sparsity\nMay reorder rows/columns to minimize fill-in during factorization\nTypically more memory-efficient than dense methods for sparse problems\n\nExamples of concrete subtypes\n\nUMFPACKFactorization: General sparse LU with partial pivoting\nKLUFactorization: Sparse LU optimized for circuit simulation\nCHOLMODFactorization: Sparse Cholesky for positive definite systems\nSparspakFactorization: Envelope/profile method for sparse systems\n\n\n\n\n\n","category":"type"},{"location":"advanced/internal_api/#LinearSolve.AbstractKrylovSubspaceMethod","page":"Internal API Documentation","title":"LinearSolve.AbstractKrylovSubspaceMethod","text":"AbstractKrylovSubspaceMethod <: SciMLLinearSolveAlgorithm\n\nAbstract type for iterative linear solvers based on Krylov subspace methods. These algorithms solve linear systems by iteratively building an approximation from a sequence of Krylov subspaces, without requiring explicit matrix factorization.\n\nCharacteristics\n\nDoes not require concrete matrix representation (needs_concrete_A() = false)\nOnly needs matrix-vector products A*v (can work with operators/functions)\nMemory usage typically O(n) or O(kn) where k << n\nConvergence depends on matrix properties (condition number, eigenvalue distribution)\nOften benefits significantly from preconditioning\n\nAdvantages\n\nLow memory requirements for large sparse systems\nCan handle matrix-free operators (functions that compute A*v)\nOften the only feasible approach for very large systems\nCan exploit matrix structure through specialized operators\n\nExamples of concrete subtypes\n\nGMRESIteration: Generalized Minimal Residual method\nCGIteration: Conjugate Gradient (for symmetric positive definite systems)\nBiCGStabLIteration: Bi-Conjugate Gradient Stabilized\nWrapped external iterative solvers (KrylovKit.jl, IterativeSolvers.jl)\n\n\n\n\n\n","category":"type"},{"location":"advanced/internal_api/#LinearSolve.AbstractSolveFunction","page":"Internal API Documentation","title":"LinearSolve.AbstractSolveFunction","text":"AbstractSolveFunction <: SciMLLinearSolveAlgorithm\n\nAbstract type for linear solvers that wrap custom solving functions or provide direct interfaces to specific solve methods. These provide flexibility for integrating custom algorithms or simple solve strategies.\n\nCharacteristics\n\nDoes not require concrete matrix representation (needs_concrete_A() = false)\nProvides maximum flexibility for custom solving strategies\nCan wrap external solver libraries or implement specialized algorithms\nPerformance and stability depend entirely on the wrapped implementation\n\nExamples of concrete subtypes\n\nLinearSolveFunction: Wraps arbitrary user-defined solve functions\nDirectLdiv!: Direct application of the \\ operator\n\n\n\n\n\n","category":"type"},{"location":"advanced/internal_api/#Core-Cache-System","page":"Internal API Documentation","title":"Core Cache System","text":"","category":"section"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"The caching system is central to LinearSolve.jl's performance and functionality:","category":"page"},{"location":"advanced/internal_api/#LinearSolve.LinearCache","page":"Internal API Documentation","title":"LinearSolve.LinearCache","text":"LinearCache{TA, Tb, Tu, Tp, Talg, Tc, Tl, Tr, Ttol, issq, S}\n\nThe core cache structure used by LinearSolve for storing and managing the state of linear solver computations. This mutable struct acts as the primary interface for iterative  solving and caching of factorizations and intermediate results.\n\nFields\n\nA::TA: The matrix operator of the linear system.\nb::Tb: The right-hand side vector of the linear system.\nu::Tu: The solution vector (preallocated storage for the result).\np::Tp: Parameters passed to the linear solver algorithm.\nalg::Talg: The linear solver algorithm instance.\ncacheval::Tc: Algorithm-specific cache storage for factorizations and intermediate computations.\nisfresh::Bool: Cache validity flag for the matrix A. false means cacheval is up-to-date  with respect to A, true means cacheval needs to be updated.\nprecsisfresh::Bool: Cache validity flag for preconditioners. false means Pl and Pr  are up-to-date with respect to A, true means they need to be updated.\nPl::Tl: Left preconditioner operator.\nPr::Tr: Right preconditioner operator.\nabstol::Ttol: Absolute tolerance for iterative solvers.\nreltol::Ttol: Relative tolerance for iterative solvers.\nmaxiters::Int: Maximum number of iterations for iterative solvers.\nverbose::LinearVerbosity: Whether to print verbose output during solving.\nassumptions::OperatorAssumptions{issq}: Assumptions about the operator properties.\nsensealg::S: Sensitivity analysis algorithm for automatic differentiation.\n\nUsage\n\nThe LinearCache is typically created via init(::LinearProblem, ::SciMLLinearSolveAlgorithm)  and then used with solve!(cache) for efficient repeated solves with the same matrix structure but potentially different right-hand sides or parameter values.\n\nCache Management\n\nThe cache automatically tracks when matrix A or parameters p change by setting the  appropriate freshness flags. When solve! is called, stale cache entries are automatically recomputed as needed.\n\n\n\n\n\n","category":"type"},{"location":"advanced/internal_api/#LinearSolve.init_cacheval","page":"Internal API Documentation","title":"LinearSolve.init_cacheval","text":"init_cacheval(alg::SciMLLinearSolveAlgorithm, args...)\n\nInitialize algorithm-specific cache values for the given linear solver algorithm. This function returns nothing by default and is intended to be overloaded by  specific algorithm implementations that need to store intermediate computations or factorizations.\n\nArguments\n\nalg: The linear solver algorithm instance\nargs...: Additional arguments passed to the cache initialization\n\nReturns\n\nAlgorithm-specific cache value or nothing for algorithms that don't require caching.\n\n\n\n\n\ncache.cacheval = NamedTuple(LUFactorization = cache of LUFactorization, ...)\n\n\n\n\n\n","category":"function"},{"location":"advanced/internal_api/#Algorithm-Selection","page":"Internal API Documentation","title":"Algorithm Selection","text":"","category":"section"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"The automatic algorithm selection is one of LinearSolve.jl's key features:","category":"page"},{"location":"advanced/internal_api/#LinearSolve.get_tuned_algorithm","page":"Internal API Documentation","title":"LinearSolve.get_tuned_algorithm","text":"get_tuned_algorithm(::Type{eltype_A}, ::Type{eltype_b}, matrix_size) where {eltype_A, eltype_b}\n\nGet the tuned algorithm preference for the given element type and matrix size. Returns nothing if no preference exists. Uses preloaded constants for efficiency. Fast path when no preferences are set.\n\n\n\n\n\n","category":"function"},{"location":"advanced/internal_api/#LinearSolve.is_algorithm_available","page":"Internal API Documentation","title":"LinearSolve.is_algorithm_available","text":"is_algorithm_available(alg::DefaultAlgorithmChoice.T)\n\nCheck if the given algorithm is currently available (extensions loaded, etc.).\n\n\n\n\n\n","category":"function"},{"location":"advanced/internal_api/#LinearSolve.show_algorithm_choices","page":"Internal API Documentation","title":"LinearSolve.show_algorithm_choices","text":"show_algorithm_choices()\n\nDisplay what algorithm choices are actually made by the default solver for  representative matrix sizes. Shows current preferences and system information.\n\n\n\n\n\n","category":"function"},{"location":"advanced/internal_api/#LinearSolve.make_preferences_dynamic!","page":"Internal API Documentation","title":"LinearSolve.make_preferences_dynamic!","text":"make_preferences_dynamic!()\n\nInternal function for testing only. Makes preferences dynamic by redefining gettunedalgorithm to check preferences at runtime instead of using compile-time constants. This allows tests to verify that the preference system works correctly.\n\nwarning: Testing Only\nThis function is only intended for internal testing purposes. It modifies global state and should never be used in production code.\n\n\n\n\n\n","category":"function"},{"location":"advanced/internal_api/#Preference-System-Architecture","page":"Internal API Documentation","title":"Preference System Architecture","text":"","category":"section"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"The dual preference system provides intelligent algorithm selection with comprehensive fallbacks:","category":"page"},{"location":"advanced/internal_api/#**Core-Functions**","page":"Internal API Documentation","title":"Core Functions","text":"","category":"section"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"get_tuned_algorithm: Retrieves tuned algorithm preferences based on matrix size and element type\nis_algorithm_available: Checks if a specific algorithm is currently available (extensions loaded)  \nshow_algorithm_choices: Analysis function displaying algorithm choices for all element types\nmake_preferences_dynamic!: Testing function that enables runtime preference checking","category":"page"},{"location":"advanced/internal_api/#**Size-Categorization**","page":"Internal API Documentation","title":"Size Categorization","text":"","category":"section"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"The system categorizes matrix sizes to match LinearSolveAutotune benchmarking:","category":"page"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"tiny: ≤20 elements (matrices ≤10 always override to GenericLU)\nsmall: 21-100 elements  \nmedium: 101-300 elements\nlarge: 301-1000 elements\nbig: >1000 elements","category":"page"},{"location":"advanced/internal_api/#**Dual-Preference-Structure**","page":"Internal API Documentation","title":"Dual Preference Structure","text":"","category":"section"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"For each category and element type (Float32, Float64, ComplexF32, ComplexF64):","category":"page"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"best_algorithm_{type}_{size}: Overall fastest algorithm from autotune\nbest_always_loaded_{type}_{size}: Fastest always-available algorithm (fallback)","category":"page"},{"location":"advanced/internal_api/#**Preference-File-Organization**","page":"Internal API Documentation","title":"Preference File Organization","text":"","category":"section"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"All preference-related functionality is consolidated in src/preferences.jl:","category":"page"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"Compile-Time Constants:","category":"page"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"AUTOTUNE_PREFS: Preference structure loaded at package import\nAUTOTUNE_PREFS_SET: Fast path check for whether any preferences are set\n_string_to_algorithm_choice: Mapping from preference strings to algorithm enums","category":"page"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"Runtime Functions:","category":"page"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"_get_tuned_algorithm_runtime: Dynamic preference checking for testing\n_choose_available_algorithm: Algorithm availability and fallback logic\nshow_algorithm_choices: Comprehensive analysis and display function","category":"page"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"Testing Infrastructure:","category":"page"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"make_preferences_dynamic!: Eval-based function redefinition for testing\nEnables runtime preference verification without affecting production performance","category":"page"},{"location":"advanced/internal_api/#**Testing-Mode-Operation**","page":"Internal API Documentation","title":"Testing Mode Operation","text":"","category":"section"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"The testing system uses an elegant eval-based approach:","category":"page"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"# Production: Uses compile-time constants (maximum performance)\nget_tuned_algorithm(Float64, Float64, 200)  # → Uses AUTOTUNE_PREFS constants\n\n# Testing: Redefines function to use runtime checking\nmake_preferences_dynamic!()\nget_tuned_algorithm(Float64, Float64, 200)  # → Uses runtime preference loading","category":"page"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"This approach maintains type stability and inference while enabling comprehensive testing.","category":"page"},{"location":"advanced/internal_api/#**Algorithm-Support-Scope**","page":"Internal API Documentation","title":"Algorithm Support Scope","text":"","category":"section"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"The preference system focuses exclusively on LU algorithms for dense matrices:","category":"page"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"Supported LU Algorithms:","category":"page"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"LUFactorization, GenericLUFactorization, RFLUFactorization\nMKLLUFactorization, AppleAccelerateLUFactorization\nSimpleLUFactorization, FastLUFactorization (both map to LU)\nGPU LU variants (CUDA, Metal, AMDGPU - all map to LU)","category":"page"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"Non-LU algorithms (QR, Cholesky, SVD, etc.) are not included in the preference system as they serve different use cases and are not typically the focus of dense matrix autotune optimization.","category":"page"},{"location":"advanced/internal_api/#Trait-Functions","page":"Internal API Documentation","title":"Trait Functions","text":"","category":"section"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"These trait functions help determine algorithm capabilities and requirements:","category":"page"},{"location":"advanced/internal_api/#LinearSolve.needs_concrete_A","page":"Internal API Documentation","title":"LinearSolve.needs_concrete_A","text":"needs_concrete_A(alg) -> Bool\n\nTrait function that determines whether a linear solver algorithm requires a concrete matrix representation or can work with abstract operators.\n\nArguments\n\nalg: A linear solver algorithm instance\n\nReturns\n\ntrue: Algorithm requires a concrete matrix (e.g., for factorization)\nfalse: Algorithm can work with abstract operators (e.g., matrix-free methods)\n\nUsage\n\nThis trait is used internally by LinearSolve.jl to optimize algorithm dispatch and determine when matrix operators need to be converted to concrete arrays.\n\nAlgorithm-Specific Behavior\n\nAbstractFactorization: true (needs explicit matrix entries for factorization)\nAbstractKrylovSubspaceMethod: false (only needs matrix-vector products)\nAbstractSolveFunction: false (depends on the wrapped function's requirements)\n\nExample\n\nneeds_concrete_A(LUFactorization())  # true\nneeds_concrete_A(GMRESIteration())   # false\n\n\n\n\n\n","category":"function"},{"location":"advanced/internal_api/#Utility-Functions","page":"Internal API Documentation","title":"Utility Functions","text":"","category":"section"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"Various utility functions support the core functionality:","category":"page"},{"location":"advanced/internal_api/#LinearSolve.default_tol","page":"Internal API Documentation","title":"LinearSolve.default_tol","text":"default_tol(T)\n\nCompute the default tolerance for iterative linear solvers based on the element type. The tolerance is typically set as the square root of the machine epsilon for the  given floating point type, ensuring numerical accuracy appropriate for that precision.\n\nArguments\n\nT: The element type of the linear system\n\nReturns\n\nFor floating point types: √(eps(T))\nFor exact types (Rational, Integer): 0 (exact arithmetic)\nFor Any type: 0 (conservative default)\n\n\n\n\n\n","category":"function"},{"location":"advanced/internal_api/#LinearSolve.default_alias_A","page":"Internal API Documentation","title":"LinearSolve.default_alias_A","text":"default_alias_A(alg, A, b) -> Bool\n\nDetermine the default aliasing behavior for the matrix A given the algorithm type. Aliasing allows the algorithm to modify the original matrix in-place for efficiency, but this may not be desirable or safe for all algorithm types.\n\nArguments\n\nalg: The linear solver algorithm\nA: The matrix operator  \nb: The right-hand side vector\n\nReturns\n\nfalse: Safe default, algorithm will not modify the original matrix A\ntrue: Algorithm may modify A in-place for efficiency\n\nAlgorithm-Specific Behavior\n\nDense factorizations: false (destructive, need to preserve original)\nKrylov methods: true (non-destructive, safe to alias)\nSparse factorizations: true (typically preserve sparsity structure)\n\n\n\n\n\n","category":"function"},{"location":"advanced/internal_api/#LinearSolve.default_alias_b","page":"Internal API Documentation","title":"LinearSolve.default_alias_b","text":"default_alias_b(alg, A, b) -> Bool\n\nDetermine the default aliasing behavior for the right-hand side vector b given the  algorithm type. Similar to default_alias_A but for the RHS vector.\n\nReturns\n\nfalse: Safe default, algorithm will not modify the original vector b\ntrue: Algorithm may modify b in-place for efficiency\n\n\n\n\n\n","category":"function"},{"location":"advanced/internal_api/#LinearSolve.__init_u0_from_Ab","page":"Internal API Documentation","title":"LinearSolve.__init_u0_from_Ab","text":"__init_u0_from_Ab(A, b)\n\nInitialize the solution vector u0 with appropriate size and type based on the  matrix A and right-hand side b. The solution vector is allocated with the  same element type as b and sized to match the number of columns in A.\n\nArguments\n\nA: The matrix operator (determines solution vector size)\nb: The right-hand side vector (determines element type)\n\nReturns\n\nA zero-initialized vector of size (size(A, 2),) with element type matching b.\n\nSpecializations\n\nFor static matrices (SMatrix): Returns a static vector (SVector)\nFor regular matrices: Returns a similar vector to b with appropriate size\n\n\n\n\n\n","category":"function"},{"location":"advanced/internal_api/#Solve-Functions","page":"Internal API Documentation","title":"Solve Functions","text":"","category":"section"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"For custom solving strategies:","category":"page"},{"location":"advanced/internal_api/#LinearSolve.LinearSolveFunction","page":"Internal API Documentation","title":"LinearSolve.LinearSolveFunction","text":"LinearSolveFunction{F} <: AbstractSolveFunction\n\nA flexible wrapper that allows using custom functions as linear solver algorithms. This provides a way to integrate user-defined solving strategies into the LinearSolve.jl framework while maintaining compatibility with the caching and interface systems.\n\nFields\n\nsolve_func::F: A callable that implements the custom linear solving logic\n\nFunction Signature\n\nThe wrapped function should have the signature:\n\nsolve_func(A, b, u, p, isfresh, Pl, Pr, cacheval; kwargs...)\n\nArguments to wrapped function\n\nA: The matrix operator of the linear system  \nb: The right-hand side vector\nu: Pre-allocated solution vector (can be used as working space)\np: Parameters passed to the solver\nisfresh: Boolean indicating if the matrix A has changed since last solve\nPl: Left preconditioner operator\nPr: Right preconditioner operator  \ncacheval: Algorithm-specific cache storage\nkwargs...: Additional keyword arguments\n\nReturns\n\nThe wrapped function should return a solution vector.\n\nExample\n\nfunction my_custom_solver(A, b, u, p, isfresh, Pl, Pr, cacheval; kwargs...)\n    # Custom solving logic here\n    return A \\ b  # Simple example\nend\n\nalg = LinearSolveFunction(my_custom_solver)\nsol = solve(prob, alg)\n\n\n\n\n\n","category":"type"},{"location":"advanced/internal_api/#LinearSolve.DirectLdiv!","page":"Internal API Documentation","title":"LinearSolve.DirectLdiv!","text":"DirectLdiv! <: AbstractSolveFunction\n\nA simple linear solver that directly applies the left-division operator (\\)  to solve the linear system. This algorithm calls ldiv!(u, A, b) which computes u = A \\ b in-place.\n\nUsage\n\nalg = DirectLdiv!()\nsol = solve(prob, alg)\n\nNotes\n\nThis is essentially a direct wrapper around Julia's built-in ldiv! function\nSuitable for cases where the matrix A has a natural inverse or factorization\nPerformance depends on the specific matrix type and its ldiv! implementation\nNo preconditioners or advanced numerical techniques are applied\nBest used for small to medium problems or when A has special structure\n\n\n\n\n\n","category":"type"},{"location":"advanced/internal_api/#Preconditioner-Infrastructure","page":"Internal API Documentation","title":"Preconditioner Infrastructure","text":"","category":"section"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"The preconditioner system allows for flexible preconditioning strategies:","category":"page"},{"location":"advanced/internal_api/#LinearSolve.ComposePreconditioner","page":"Internal API Documentation","title":"LinearSolve.ComposePreconditioner","text":"ComposePreconditioner{Ti, To}\n\nA preconditioner that composes two preconditioners by applying them sequentially. The inner preconditioner is applied first, followed by the outer preconditioner. This allows for building complex preconditioning strategies by combining simpler ones.\n\nFields\n\ninner::Ti: The inner (first) preconditioner to apply\nouter::To: The outer (second) preconditioner to apply\n\nUsage\n\n# Compose a diagonal preconditioner with an ILU preconditioner\ninner_prec = DiagonalPreconditioner(diag(A))\nouter_prec = ILUFactorization()  \ncomposed = ComposePreconditioner(inner_prec, outer_prec)\n\nThe composed preconditioner applies: outer(inner(x)) for any vector x.\n\nMathematical Interpretation\n\nFor a linear system Ax = b, if P₁ is the inner and P₂ is the outer preconditioner, then the composed preconditioner effectively applies P₂P₁ as the combined preconditioner.\n\n\n\n\n\n","category":"type"},{"location":"advanced/internal_api/#LinearSolve.InvPreconditioner","page":"Internal API Documentation","title":"LinearSolve.InvPreconditioner","text":"InvPreconditioner{T}\n\nA preconditioner wrapper that treats a matrix or operator as if it represents the inverse of the actual preconditioner. Instead of solving Px = y, it  computes P*y where P is stored as the \"inverse\" preconditioner matrix.\n\nFields\n\nP::T: The stored preconditioner matrix/operator (representing P⁻¹)\n\nUsage\n\nThis is useful when you have a matrix that approximates the inverse of your desired preconditioner. For example, if you have computed an approximate  inverse matrix Ainv ≈ A⁻¹, you can use:\n\nprec = InvPreconditioner(Ainv)\n\nMathematical Interpretation\n\nFor a linear system Ax = b with preconditioner M, normally we solve M⁻¹Ax = M⁻¹b. With InvPreconditioner, the stored matrix P represents M⁻¹ directly, so applying the preconditioner becomes a matrix-vector multiplication rather than a linear solve.\n\nMethods\n\nldiv!(A::InvPreconditioner, x): Computes x ← P*x (in-place)\nldiv!(y, A::InvPreconditioner, x): Computes y ← P*x  \nmul!(y, A::InvPreconditioner, x): Computes y ← P⁻¹*x (inverse operation)\n\n\n\n\n\n","category":"type"},{"location":"advanced/internal_api/#Internal-Algorithm-Types","page":"Internal API Documentation","title":"Internal Algorithm Types","text":"","category":"section"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"These are internal algorithm implementations:","category":"page"},{"location":"advanced/internal_api/#LinearSolve.SimpleLUFactorization","page":"Internal API Documentation","title":"LinearSolve.SimpleLUFactorization","text":"SimpleLUFactorization(pivot::Bool = true)\n\nA pure Julia LU factorization implementation without BLAS dependencies. This solver is optimized for small matrices and situations where BLAS  is not available or desirable.\n\nConstructor Arguments\n\npivot::Bool = true: Whether to perform partial pivoting for numerical stability. Set to false for slightly better performance at the cost of stability.\n\nFeatures\n\nPure Julia implementation (no BLAS dependencies)\nPartial pivoting support for numerical stability\nIn-place matrix modification for memory efficiency  \nFast for small matrices (typically < 100×100)\nEducational value for understanding LU factorization\n\nPerformance Characteristics\n\nOptimal for small dense matrices\nNo overhead from BLAS calls\nLinear scaling with problem size (O(n³) operations)\nMemory efficient due to in-place operations\n\nUse Cases\n\nSmall matrices where BLAS overhead is significant\nSystems without optimized BLAS libraries\nEducational and prototyping purposes\nEmbedded systems with memory constraints\n\nExample\n\n# Stable version with pivoting (default)\nalg1 = SimpleLUFactorization()\n# Faster version without pivoting\nalg2 = SimpleLUFactorization(false)\n\nprob = LinearProblem(A, b)\nsol = solve(prob, alg1)\n\nNotes\n\nFor larger matrices (> 100×100), consider using BLAS-based factorizations  like LUFactorization() for better performance.\n\n\n\n\n\n","category":"type"},{"location":"advanced/internal_api/#LinearSolve.LUSolver","page":"Internal API Documentation","title":"LinearSolve.LUSolver","text":"LUSolver{T}\n\nA mutable workspace for performing LU factorization and solving linear systems. This struct maintains all necessary arrays and state information for the  factorization and solve phases, allowing for efficient reuse when solving multiple systems with the same matrix structure.\n\nFields\n\nn::Int: Dimension of the square matrix\nA::Matrix{T}: Working copy of the matrix to be factorized (modified in-place)\nb::Vector{T}: Right-hand side vector storage\nx::Vector{T}: Solution vector storage  \npivots::Vector{Int}: Pivot indices from the factorization\nperms::Vector{Int}: Permutation vector tracking row exchanges\ninfo::Int: Status information (0 = success, >0 indicates singularity)\n\nConstructor\n\nLUSolver{T}(n)  # Create solver for n×n matrix with element type T\n\nUsage\n\nThe solver is typically created from a matrix using the convenience constructors:\n\nsolver = LUSolver(A)        # From matrix A\nsolver = LUSolver(A, b)     # From matrix A and RHS b\n\nThen factorized and solved:\n\nsimplelu_factorize!(solver)    # Perform LU factorization\nsimplelu_solve!(solver)        # Solve for the stored RHS\n\nNotes\n\nThis is a pure Julia implementation primarily for educational purposes and small matrices. For production use, prefer optimized LAPACK-based factorizations.\n\n\n\n\n\n","category":"type"},{"location":"advanced/internal_api/#Developer-Notes","page":"Internal API Documentation","title":"Developer Notes","text":"","category":"section"},{"location":"advanced/internal_api/#Adding-New-Algorithms","page":"Internal API Documentation","title":"Adding New Algorithms","text":"","category":"section"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"When adding a new linear solver algorithm to LinearSolve.jl:","category":"page"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"Choose the appropriate abstract type: Inherit from the most specific abstract type that fits your algorithm\nImplement required methods: At minimum, implement solve! and possibly init_cacheval\nConsider trait functions: Override trait functions like needs_concrete_A if needed\nDocument thoroughly: Add comprehensive docstrings following the patterns shown here","category":"page"},{"location":"advanced/internal_api/#Performance-Considerations","page":"Internal API Documentation","title":"Performance Considerations","text":"","category":"section"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"The LinearCache system is designed for efficient repeated solves\nUse cache.isfresh to avoid redundant computations when the matrix hasn't changed\nConsider implementing specialized init_cacheval for algorithms that need setup\nLeverage trait functions to optimize dispatch and memory usage","category":"page"},{"location":"advanced/internal_api/#Testing-Guidelines","page":"Internal API Documentation","title":"Testing Guidelines","text":"","category":"section"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"When adding new functionality:","category":"page"},{"location":"advanced/internal_api/","page":"Internal API Documentation","title":"Internal API Documentation","text":"Test with various matrix types (dense, sparse, GPU arrays)\nVerify caching behavior works correctly\nEnsure trait functions return appropriate values\nTest integration with the automatic algorithm selection system","category":"page"},{"location":"tutorials/linear/#Getting-Started-with-Solving-Linear-Systems-in-Julia","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"","category":"section"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"A linear system Au=b is specified by defining an AbstractMatrix or AbstractSciMLOperator. For the sake of simplicity, this tutorial will start by only showcasing concrete matrices. And specifically, we will start by using the basic Julia Matrix type.","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"The following defines a Matrix and a LinearProblem which is subsequently solved by the default linear solver.","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"import LinearSolve as LS\n\nA = rand(4, 4)\nb = rand(4)\nprob = LS.LinearProblem(A, b)\nsol = LS.solve(prob)\nsol.u","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"Note that LS.solve(prob) is equivalent to LS.solve(prob,nothing) where nothing denotes the choice of the default linear solver. This is equivalent to the Julia built-in A\\b, where the solution is recovered via sol.u. The power of this package comes into play when changing the algorithms. For example, Krylov.jl has some nice methods like GMRES which can be faster in some cases. With LinearSolve.jl, there is one interface and changing linear solvers is simply the switch of the algorithm choice:","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"sol = LS.solve(prob, LS.KrylovJL_GMRES())\nsol.u","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"Thus, a package which uses LinearSolve.jl simply needs to allow the user to pass in an algorithm struct and all wrapped linear solvers are immediately available as tweaks to the general algorithm. For more information on the available solvers, see the solvers page","category":"page"},{"location":"tutorials/linear/#Sparse-and-Structured-Matrices","page":"Getting Started with Solving Linear Systems in Julia","title":"Sparse and Structured Matrices","text":"","category":"section"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"There is no difference in the interface for LinearSolve.jl on sparse and structured matrices. For example, the following now uses Julia's built-in SparseArrays.jl to define a sparse matrix (SparseMatrixCSC) and solve the system with LinearSolve.jl. Note that sprand is a shorthand for quickly creating a sparse random matrix (see SparseArrays.jl for more details on defining sparse matrices).","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"import LinearSolve as LS\nimport SparseArrays as SA\n\nA = SA.sprand(4, 4, 0.75)\nb = rand(4)\nprob = LS.LinearProblem(A, b)\nsol = LS.solve(prob)\nsol.u\n\nsol = LS.solve(prob, LS.KrylovJL_GMRES()) # Choosing algorithms is done the same way\nsol.u","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"Similarly structure matrix types, like banded matrices, can be provided using special matrix types. While any AbstractMatrix type should be compatible via the general Julia interfaces, LinearSolve.jl specifically tests with the following cases:","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"LinearAlgebra.jl\nSymmetric\nHermitian\nUpperTriangular\nUnitUpperTriangular\nLowerTriangular\nUnitLowerTriangular\nSymTridiagonal\nTridiagonal\nBidiagonal\nDiagonal\nBandedMatrices.jl BandedMatrix\nBlockDiagonals.jl BlockDiagonal\nCUDA.jl (CUDA GPU-based dense and sparse matrices) CuArray (GPUArray)\nFastAlmostBandedMatrices.jl FastAlmostBandedMatrix\nMetal.jl (Apple M-series GPU-based dense matrices) MetalArray","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"note: Note\nChoosing the most specific matrix structure that matches your specific system will give you the most performance. Thus if your matrix is symmetric, specifically building with Symmetric(A) will be faster than simply using A, and will generally lead to better automatic linear solver choices. Note that you can also choose the type for b, but generally a dense vector will be the fastest here and many solvers will not support a sparse b.","category":"page"},{"location":"tutorials/linear/#Using-Matrix-Free-Operators-via-SciMLOperators.jl","page":"Getting Started with Solving Linear Systems in Julia","title":"Using Matrix-Free Operators via SciMLOperators.jl","text":"","category":"section"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"In many cases where a sparse matrix gets really large, even the sparse representation cannot be stored in memory. However, in many such cases, such as with PDE discretizations, you may be able to write down a function that directly computes A*x. These \"matrix-free\" operators allow the user to define the Ax=b problem to be solved giving only the definition of A*x and allowing specific solvers (Krylov methods) to act without ever constructing the full matrix.","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"The Matrix-Free operators are provided by the SciMLOperators.jl interface. For example, for the matrix A defined via:","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"A = [-2.0 1 0 0 0\n     1 -2 1 0 0\n     0 1 -2 1 0\n     0 0 1 -2 1\n     0 0 0 1 -2]","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"We can define the FunctionOperator that does the A*v operations, without using the matrix A. This is done by defining a function func(w,v,u,p,t) which calculates w = A(u,p,t)*v (for the purposes of this tutorial, A is just a constant operator. See the SciMLOperators.jl documentation for more details on defining non-constant operators, operator algebras, and many more features). This is done by:","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"function Afunc!(w, v, u, p, t)\n    w[1] = -2v[1] + v[2]\n    for i in 2:4\n        w[i] = v[i - 1] - 2v[i] + v[i + 1]\n    end\n    w[5] = v[4] - 2v[5]\n    nothing\nend\n\nfunction Afunc!(v, u, p, t)\n    w = zeros(5)\n    Afunc!(w, v, u, p, t)\n    w\nend\n\nimport SciMLOperators as SMO\nmfopA = SMO.FunctionOperator(Afunc!, zeros(5), zeros(5))","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"Let's check these are the same:","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"v = rand(5)\nmfopA*v - A*v","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"Notice mfopA does this without having to have A because it just uses the equivalent Afunc! instead. Now, even though we don't have a matrix, we can still solve linear systems defined by this operator. For example:","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"b = rand(5)\nprob = LS.LinearProblem(mfopA, b)\nsol = LS.solve(prob)\nsol.u","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"And we can check this is successful:","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"mfopA * sol.u - b","category":"page"},{"location":"tutorials/linear/","page":"Getting Started with Solving Linear Systems in Julia","title":"Getting Started with Solving Linear Systems in Julia","text":"note: Note\nNote that not all methods can use a matrix-free operator. For example, LS.LUFactorization() requires a matrix. If you use an invalid method, you will get an error. The methods particularly from KrylovJL are the ones preferred for these cases (and are defaulted to).","category":"page"},{"location":"release_notes/#Release-Notes","page":"Release Notes","title":"Release Notes","text":"","category":"section"},{"location":"release_notes/#Upcoming-Changes","page":"Release Notes","title":"Upcoming Changes","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"CudaOffloadFactorization has been split into two algorithms:\nCudaOffloadLUFactorization - Uses LU factorization for better performance\nCudaOffloadQRFactorization - Uses QR factorization for better numerical stability\nCudaOffloadFactorization is now deprecated and will show a warning suggesting to use one of the new algorithms","category":"page"},{"location":"release_notes/#v2.0","page":"Release Notes","title":"v2.0","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"LinearCache changed from immutable to mutable. With this, the out of place interfaces like set_A were deprecated for simply mutating the cache, cache.A = .... This fixes some correctness checks and makes the package more robust while improving performance.\nThe default algorithm is now type-stable and does not rely on a dynamic dispatch for the choice.\nIterativeSolvers.jl and KrylovKit.jl were made into extension packages.\nDocumentation of the solvers has changed to docstrings","category":"page"},{"location":"basics/common_solver_opts/#Common-Solver-Options-(Keyword-Arguments-for-Solve)","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"While many algorithms have specific arguments within their constructor, the keyword arguments for solve are common across all the algorithms in order to give composability. These are also the options taken at init time. The following are the options these algorithms take, along with their defaults.","category":"page"},{"location":"basics/common_solver_opts/#General-Controls","page":"Common Solver Options (Keyword Arguments for Solve)","title":"General Controls","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"alias::LinearAliasSpecifier: Holds the fields alias_A and alias_b which specify whether to alias the matrices A and b respectively. When these fields are true, A and b can be written to and changed by the solver algorithm. When fields are nothing the default behavior is used, which is to default to true when the algorithm is known not to modify the matrices, and false otherwise.\nverbose: Whether to print extra information. Defaults to false.\nassumptions: Sets the assumptions of the operator in order to effect the default choice algorithm. See the Operator Assumptions page for more details.","category":"page"},{"location":"basics/common_solver_opts/#Iterative-Solver-Controls","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Iterative Solver Controls","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"Error controls are not used by all algorithms. Specifically, direct solves always solve completely. Error controls only apply to iterative solvers.","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"abstol: The absolute tolerance. Defaults to √(eps(eltype(A)))\nreltol: The relative tolerance. Defaults to √(eps(eltype(A)))\nmaxiters: The number of iterations allowed. Defaults to length(prob.b)\nPl,Pr: The left and right preconditioners, respectively. For more information, see the Preconditioners page.","category":"page"},{"location":"basics/common_solver_opts/#Verbosity-Controls","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Verbosity Controls","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"The verbosity system in LinearSolve.jl provides fine-grained control over the diagnostic messages, warnings, and errors that are displayed during the solution of linear systems. To use this system, a keyword argument verbose is provided to solve. ","category":"page"},{"location":"basics/common_solver_opts/#LinearSolve.LinearVerbosity","page":"Common Solver Options (Keyword Arguments for Solve)","title":"LinearSolve.LinearVerbosity","text":"LinearVerbosity <: AbstractVerbositySpecifier\n\nVerbosity configuration for LinearSolve.jl solvers, providing fine-grained control over diagnostic messages, warnings, and errors during linear system solution.\n\nFields\n\nError Control Group\n\ndefault_lu_fallback: Messages when falling back to LU factorization from other methods\nblas_errors: Critical BLAS errors that stop computation\nblas_invalid_args: BLAS errors due to invalid arguments\n\nPerformance Group\n\nno_right_preconditioning: Messages when right preconditioning is not used\n\nNumerical Group\n\nusing_IterativeSolvers: Messages when using the IterativeSolvers.jl package\nIterativeSolvers_iterations: Iteration count messages from IterativeSolvers.jl\nKrylovKit_verbosity: Verbosity level passed to KrylovKit.jl solvers\nKrylovJL_verbosity: Verbosity level passed to Krylov.jl solvers\nHYPRE_verbosity: Verbosity level passed to HYPRE solvers\npardiso_verbosity: Verbosity level passed to Pardiso solvers\nblas_info: Informational messages from BLAS operations\nblas_success: Success messages from BLAS operations\ncondition_number: Messages related to condition number calculations\nconvergence_failure: Messages when iterative solvers fail to converge\nsolver_failure: Messages when solvers fail for reasons other than convergence\nmax_iters: Messages when iterative solvers reach maximum iterations\n\nConstructors\n\nLinearVerbosity(preset::AbstractVerbosityPreset)\n\nCreate a LinearVerbosity using a preset configuration:\n\nSciMLLogging.None(): All messages disabled\nSciMLLogging.Minimal(): Only critical errors and fatal issues\nSciMLLogging.Standard(): Balanced verbosity (default)\nSciMLLogging.Detailed(): Comprehensive debugging information\nSciMLLogging.All(): Maximum verbosity\nLinearVerbosity(; error_control=nothing, performance=nothing, numerical=nothing, kwargs...)\n\nCreate a LinearVerbosity with group-level or individual field control.\n\nExamples\n\n# Use a preset\nverbose = LinearVerbosity(SciMLLogging.Standard())\n\n# Set entire groups\nverbose = LinearVerbosity(\n    error_control = SciMLLogging.WarnLevel(),\n    numerical = SciMLLogging.InfoLevel()\n)\n\n# Set individual fields\nverbose = LinearVerbosity(\n    default_lu_fallback = SciMLLogging.InfoLevel(),\n    KrylovJL_verbosity = SciMLLogging.CustomLevel(1),\n    blas_errors = SciMLLogging.ErrorLevel()\n)\n\n# Mix group and individual settings\nverbose = LinearVerbosity(\n    numerical = SciMLLogging.InfoLevel(),  # Set all numerical to InfoLevel\n    blas_errors = SciMLLogging.ErrorLevel()  # Override specific field\n)\n\n\n\n\n\n","category":"type"},{"location":"basics/common_solver_opts/#Basic-Usage","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Basic Usage","text":"","category":"section"},{"location":"basics/common_solver_opts/#Global-Verbosity-Control","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Global Verbosity Control","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"using LinearSolve\n\n# Suppress all messages\nverbose = LinearVerbosity(SciMLLogging.None())\nprob = LinearProblem(A, b)\nsol = solve(prob; verbose=verbose)\n\n# Show only essential messages (critical errors and fatal issues)\nverbose = LinearVerbosity(SciMLLogging.Minimal())\nsol = solve(prob; verbose=verbose)\n\n# Use default settings (balanced verbosity for typical usage)\nverbose = LinearVerbosity(SciMLLogging.Standard())\nsol = solve(prob; verbose=verbose)\n\n# Show comprehensive debugging information\nverbose = LinearVerbosity(SciMLLogging.Detailed())\nsol = solve(prob; verbose=verbose)\n\n# Show all messages (maximum verbosity)\nverbose = LinearVerbosity(SciMLLogging.All())\nsol = solve(prob; verbose=verbose)","category":"page"},{"location":"basics/common_solver_opts/#Group-Level-Control","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Group Level Control","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"# Customize by category\nverbose = LinearVerbosity(\n    error_control = SciMLLogging.Warn(),   # Show warnings for error control related issues\n    performance = SciMLLogging.Silent(),     # Suppress performance messages\n    numerical = SciMLLogging.Info()        # Show all numerical related log messages at info level\n)\n\nsol = solve(prob; verbose=verbose)","category":"page"},{"location":"basics/common_solver_opts/#Fine-grained-Control","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Fine-grained Control","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"The constructor for LinearVerbosity allows you to set verbosity for each specific message toggle, giving you fine-grained control.  The verbosity settings for the toggles are automatically passed to the group objects. ","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"# Set specific message types\nverbose = LinearVerbosity(\n    default_lu_fallback = SciMLLogging.InfoLevel(),                     # Show info when LU fallback is used\n    KrylovJL_verbosity = SciMLLogging.WarnLevel(),                      # Show warnings from KrylovJL\n    no_right_preconditioning = SciMLLogging.Silent(),                # Suppress right preconditioning messages\n    KrylovKit_verbosity = SciMLLogging.Level(KrylovKit.WARN_LEVEL) # Set KrylovKit verbosity level using KrylovKit's own verbosity levels\n)\n\nsol = solve(prob; verbose=verbose)\n","category":"page"},{"location":"solvers/solvers/#linearsystemsolvers","page":"Linear System Solvers","title":"Linear System Solvers","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"LS.solve(prob::LS.LinearProblem,alg;kwargs)","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Solves for Au=b in the problem defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.","category":"page"},{"location":"solvers/solvers/#Recommended-Methods","page":"Linear System Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"solvers/solvers/#Dense-Matrices","page":"Linear System Solvers","title":"Dense Matrices","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"The default algorithm nothing is good for picking an algorithm that will work, but one may need to change this to receive more performance or precision. If more precision is necessary, LS.QRFactorization() and LS.SVDFactorization() are the best choices, with SVD being the slowest but most precise.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"For efficiency, RFLUFactorization is the fastest for dense LU-factorizations until around 150x150 matrices, though this can be dependent on the exact details of the hardware. After this point, MKLLUFactorization is usually faster on most hardware. Note that on Mac computers that AppleAccelerateLUFactorization is generally always the fastest. OpenBLASLUFactorization  provides direct OpenBLAS calls without going through libblastrampoline and can be faster than  LUFactorization in some configurations. LUFactorization will use your base system BLAS which  can be fast or slow depending on the hardware configuration. SimpleLUFactorization will be fast  only on very small matrices but can cut down on compile times.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"For very large dense factorizations, offloading to the GPU can be preferred. Metal.jl can be used on Mac hardware to offload, and has a cutoff point of being faster at around size 20,000 x 20,000 matrices (and only supports Float32). CudaOffloadLUFactorization and CudaOffloadQRFactorization  can be more efficient at a much smaller cutoff, possibly around size 1,000 x 1,000 matrices, though  this is highly dependent on the chosen GPU hardware. These algorithms require a CUDA-compatible NVIDIA GPU. CUDA offload supports Float64 but most consumer GPU hardware will be much faster on Float32 (many are >32x faster for Float32 operations than Float64 operations) and thus for most hardware this is only recommended for Float32 matrices. Choose CudaOffloadLUFactorization for better  performance on well-conditioned problems, or CudaOffloadQRFactorization for better numerical  stability on ill-conditioned problems.","category":"page"},{"location":"solvers/solvers/#Mixed-Precision-Methods","page":"Linear System Solvers","title":"Mixed Precision Methods","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"For large well-conditioned problems where memory bandwidth is the bottleneck, mixed precision  methods can provide significant speedups (up to 2x) by performing the factorization in Float32  while maintaining Float64 interfaces. These methods are particularly effective for:","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Large dense matrices (> 1000x1000)\nWell-conditioned problems (condition number < 10^4)\nHardware with good Float32 performance","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Available mixed precision solvers:","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"MKL32MixedLUFactorization - CPUs with MKL\nAppleAccelerate32MixedLUFactorization - Apple CPUs with Accelerate\nCUDAOffload32MixedLUFactorization - NVIDIA GPUs with CUDA\nMetalOffload32MixedLUFactorization - Apple GPUs with Metal","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"These methods automatically handle the precision conversion, making them easy drop-in replacements when reduced precision is acceptable for the factorization step.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nPerformance details for dense LU-factorizations can be highly dependent on the hardware configuration. For details see this issue. If one is looking to best optimize their system, we suggest running the performance tuning benchmark.","category":"page"},{"location":"solvers/solvers/#Sparse-Matrices","page":"Linear System Solvers","title":"Sparse Matrices","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"For sparse LU-factorizations, KLUFactorization if there is less structure to the sparsity pattern and UMFPACKFactorization if there is more structure. Pardiso.jl's methods are also known to be very efficient sparse linear solvers.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"For GPU-accelerated sparse LU-factorizations, there are two high-performance options. When using CuSparseMatrixCSR arrays with CUDSS.jl loaded, LUFactorization() will automatically use NVIDIA's cuDSS library. Alternatively, CUSOLVERRFFactorization provides access to NVIDIA's cusolverRF library. Both offer significant performance improvements for sparse systems on CUDA-capable GPUs and are particularly effective for large sparse matrices that can benefit from GPU parallelization. CUDSS is more for Float32 while CUSOLVERRFFactorization is for Float64.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"While these sparse factorizations are based on implementations in other languages, and therefore constrained to standard number types (Float64,  Float32 and their complex counterparts),  SparspakFactorization is able to handle general number types, e.g. defined by ForwardDiff.jl, MultiFloats.jl, or IntervalArithmetics.jl.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"As sparse matrices get larger, iterative solvers tend to get more efficient than factorization methods if a lower tolerance of the solution is required.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Krylov.jl generally outperforms IterativeSolvers.jl and KrylovKit.jl, and is compatible with CPUs and GPUs, and thus is the generally preferred form for Krylov methods. The choice of Krylov method should be the one most constrained to the type of operator one has, for example if positive definite then KrylovJL_CG(), but if no good properties then use KrylovJL_GMRES().","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Finally, a user can pass a custom function for handling the linear solve using LS.LinearSolveFunction() if existing solvers are not optimally suited for their application. The interface is detailed here.","category":"page"},{"location":"solvers/solvers/#Lazy-SciMLOperators","page":"Linear System Solvers","title":"Lazy SciMLOperators","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"If the linear operator is given as a lazy non-concrete operator, such as a FunctionOperator, then using a Krylov method is preferred in order to not concretize the matrix. Krylov.jl generally outperforms IterativeSolvers.jl and KrylovKit.jl, and is compatible with CPUs and GPUs, and thus is the generally preferred form for Krylov methods. The choice of Krylov method should be the one most constrained to the type of operator one has, for example if positive definite then KrylovJL_CG(), but if no good properties then use KrylovJL_GMRES().","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"tip: Tip\nIf your materialized operator is a uniform block diagonal matrix, then you can use SimpleGMRES(; blocksize = <known block size>) to further improve performance. This often shows up in Neural Networks where the Jacobian wrt the Inputs (almost always) is a Uniform Block Diagonal matrix of Block Size = size of the input divided by the batch size.","category":"page"},{"location":"solvers/solvers/#Full-List-of-Methods","page":"Linear System Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"solvers/solvers/#Polyalgorithms","page":"Linear System Solvers","title":"Polyalgorithms","text":"","category":"section"},{"location":"solvers/solvers/#LinearSolve.DefaultLinearSolver","page":"Linear System Solvers","title":"LinearSolve.DefaultLinearSolver","text":"DefaultLinearSolver(;safetyfallback=true)\n\nThe default linear solver. This is the algorithm chosen when solve(prob) is called. It's a polyalgorithm that detects the optimal method for a given A, b and hardware (Intel, AMD, GPU, etc.).\n\nKeyword Arguments\n\nsafetyfallback: determines whether to fallback to a column-pivoted QR factorization when an LU factorization fails. This can be required if A is rank-deficient. Defaults to true.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#RecursiveFactorization.jl","page":"Linear System Solvers","title":"RecursiveFactorization.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing this solver requires adding the package RecursiveFactorization.jl, i.e. using RecursiveFactorization","category":"page"},{"location":"solvers/solvers/#LinearSolve.RFLUFactorization","page":"Linear System Solvers","title":"LinearSolve.RFLUFactorization","text":"RFLUFactorization{P, T}(; pivot = Val(true), thread = Val(true))\n\nA fast pure Julia LU-factorization implementation using RecursiveFactorization.jl.  This is by far the fastest LU-factorization implementation, usually outperforming  OpenBLAS and MKL for smaller matrices (<500x500), but currently optimized only for  Base Array with Float32 or Float64. Additional optimization for complex matrices  is in the works.\n\nType Parameters\n\nP: Pivoting strategy as Val{Bool}. Val{true} enables partial pivoting for stability.\nT: Threading strategy as Val{Bool}. Val{true} enables multi-threading for performance.\n\nConstructor Arguments\n\npivot = Val(true): Enable partial pivoting. Set to Val{false} to disable for speed  at the cost of numerical stability.\nthread = Val(true): Enable multi-threading. Set to Val{false} for single-threaded  execution.\nthrowerror = true: Whether to throw an error if RecursiveFactorization.jl is not loaded.\n\nPerformance Notes\n\nFastest for dense matrices with dimensions roughly < 500×500\nOptimized specifically for Float32 and Float64 element types\nRecursive blocking strategy provides excellent cache performance\nMulti-threading can provide significant speedups on multi-core systems\n\nRequirements\n\nUsing this solver requires that RecursiveFactorization.jl is loaded: using RecursiveFactorization\n\nExample\n\nusing RecursiveFactorization\n# Fast, stable (with pivoting)\nalg1 = RFLUFactorization()\n# Fastest (no pivoting), less stable\nalg2 = RFLUFactorization(pivot=Val(false))  \n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#Base.LinearAlgebra","page":"Linear System Solvers","title":"Base.LinearAlgebra","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"These overloads tend to work for many array types, such as CuArrays for GPU-accelerated solving, using the overloads provided by the respective packages. Given that this can be customized per-package, details given below describe a subset of important arrays (Matrix, SparseMatrixCSC, CuMatrix, etc.)","category":"page"},{"location":"solvers/solvers/#LinearSolve.LUFactorization","page":"Linear System Solvers","title":"LinearSolve.LUFactorization","text":"LUFactorization(pivot=LinearAlgebra.RowMaximum())\n\nJulia's built in lu. Equivalent to calling lu!(A)\n\nOn dense matrices, this uses the current BLAS implementation of the user's computer, which by default is OpenBLAS but will use MKL if the user does using MKL in their system.\nOn sparse matrices, this will use UMFPACK from SparseArrays. Note that this will not cache the symbolic factorization.\nOn CuMatrix, it will use a CUDA-accelerated LU from CuSolver.\nOn BandedMatrix and BlockBandedMatrix, it will use a banded LU.\n\nPositional Arguments\n\npivot: The choice of pivoting. Defaults to LinearAlgebra.RowMaximum(). The other choice is LinearAlgebra.NoPivot().\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.GenericLUFactorization","page":"Linear System Solvers","title":"LinearSolve.GenericLUFactorization","text":"GenericLUFactorization(pivot=LinearAlgebra.RowMaximum())\n\nJulia's built in generic LU factorization. Equivalent to calling LinearAlgebra.generic_lufact!. Supports arbitrary number types but does not achieve as good scaling as BLAS-based LU implementations. Has low overhead and is good for small matrices.\n\nPositional Arguments\n\npivot: The choice of pivoting. Defaults to LinearAlgebra.RowMaximum(). The other choice is LinearAlgebra.NoPivot().\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.QRFactorization","page":"Linear System Solvers","title":"LinearSolve.QRFactorization","text":"QRFactorization(pivot=LinearAlgebra.NoPivot(),blocksize=16)\n\nJulia's built in qr. Equivalent to calling qr!(A).\n\nOn dense matrices, this uses the current BLAS implementation of the user's computer which by default is OpenBLAS but will use MKL if the user does using MKL in their system.\nOn sparse matrices, this will use SPQR from SparseArrays\nOn CuMatrix, it will use a CUDA-accelerated QR from CuSolver.\nOn BandedMatrix and BlockBandedMatrix, it will use a banded QR.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.SVDFactorization","page":"Linear System Solvers","title":"LinearSolve.SVDFactorization","text":"SVDFactorization(full=false,alg=LinearAlgebra.DivideAndConquer())\n\nJulia's built in svd. Equivalent to svd!(A).\n\nOn dense matrices, this uses the current BLAS implementation of the user's computer which by default is OpenBLAS but will use MKL if the user does using MKL in their system.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.CholeskyFactorization","page":"Linear System Solvers","title":"LinearSolve.CholeskyFactorization","text":"CholeskyFactorization(; pivot = nothing, tol = 0.0, shift = 0.0, perm = nothing)\n\nJulia's built in cholesky. Equivalent to calling cholesky!(A).\n\nKeyword Arguments\n\npivot: defaluts to NoPivot, can also be RowMaximum.\ntol: the tol argument in CHOLMOD. Only used for sparse matrices.\nshift: the shift argument in CHOLMOD. Only used for sparse matrices.\nperm: the perm argument in CHOLMOD. Only used for sparse matrices.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.BunchKaufmanFactorization","page":"Linear System Solvers","title":"LinearSolve.BunchKaufmanFactorization","text":"BunchKaufmanFactorization(; rook = false)\n\nJulia's built in bunchkaufman. Equivalent to calling bunchkaufman(A). Only for Symmetric matrices.\n\nKeyword Arguments\n\nrook: whether to perform rook pivoting. Defaults to false.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.CHOLMODFactorization","page":"Linear System Solvers","title":"LinearSolve.CHOLMODFactorization","text":"CHOLMODFactorization(; shift = 0.0, perm = nothing)\n\nA wrapper of CHOLMOD's polyalgorithm, mixing Cholesky factorization and ldlt. Tries cholesky for performance and retries ldlt if conditioning causes Cholesky to fail.\n\nOnly supports sparse matrices.\n\nKeyword Arguments\n\nshift: the shift argument in CHOLMOD.\nperm: the perm argument in CHOLMOD\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.NormalCholeskyFactorization","page":"Linear System Solvers","title":"LinearSolve.NormalCholeskyFactorization","text":"NormalCholeskyFactorization(pivot = RowMaximum())\n\nA fast factorization which uses a Cholesky factorization on A * A'. Can be much faster than LU factorization, but is not as numerically stable and thus should only be applied to well-conditioned matrices.\n\nwarn: Warn\nNormalCholeskyFactorization should only be applied to well-conditioned matrices. As a method it is not able to easily identify possible numerical issues. As a check it is recommended that the user checks A*u-b is approximately zero, as this may be untrue even if sol.retcode === ReturnCode.Success due to numerical stability issues.\n\nPositional Arguments\n\npivot: Defaults to RowMaximum(), but can be NoPivot()\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.NormalBunchKaufmanFactorization","page":"Linear System Solvers","title":"LinearSolve.NormalBunchKaufmanFactorization","text":"NormalBunchKaufmanFactorization(rook = false)\n\nA fast factorization which uses a BunchKaufman factorization on A * A'. Can be much faster than LU factorization, but is not as numerically stable and thus should only be applied to well-conditioned matrices.\n\nPositional Arguments\n\nrook: whether to perform rook pivoting. Defaults to false.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.jl","page":"Linear System Solvers","title":"LinearSolve.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"LinearSolve.jl contains some linear solvers built in for specialized cases.","category":"page"},{"location":"solvers/solvers/#LinearSolve.DiagonalFactorization","page":"Linear System Solvers","title":"LinearSolve.DiagonalFactorization","text":"DiagonalFactorization()\n\nA special implementation only for solving Diagonal matrices fast.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.SimpleGMRES","page":"Linear System Solvers","title":"LinearSolve.SimpleGMRES","text":"SimpleGMRES(; restart::Bool = true, blocksize::Int = 0, warm_start::Bool = false,\n    memory::Int = 20)\n\nA simple GMRES implementation for square non-Hermitian linear systems.\n\nThis implementation handles Block Diagonal Matrices with Uniformly Sized Square Blocks with specialized dispatches.\n\nArguments\n\nrestart::Bool: If true, then the solver will restart after memory iterations.\nmemory::Int = 20: The number of iterations before restarting. If restart is false, this value is used to allocate memory and later expanded if more memory is required.\nblocksize::Int = 0: If blocksize is > 0, the solver assumes that the matrix has a uniformly sized block diagonal structure with square blocks of size blocksize. Misusing this option will lead to incorrect results.\nIf this is set ≤ 0 and during runtime we get a Block Diagonal Matrix, then we will check if the specialized dispatch can be used.\n\nwarning: Warning\nMost users should be using the KrylovJL_GMRES solver instead of this implementation.\n\ntip: Tip\nWe can automatically detect if the matrix is a Block Diagonal Matrix with Uniformly Sized Square Blocks. If this is the case, then we can use a specialized dispatch. However, on most modern systems performing a single matrix-vector multiplication is faster than performing multiple smaller matrix-vector multiplications (as in the case of Block Diagonal Matrix). We recommend making the matrix dense (if size permits) and specifying the blocksize argument.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#FastLapackInterface.jl","page":"Linear System Solvers","title":"FastLapackInterface.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"FastLapackInterface.jl is a package that allows for a lower-level interface to the LAPACK calls to allow for preallocating workspaces to decrease the overhead of the wrappers. LinearSolve.jl provides a wrapper to these routines in a way where an initialized solver has a non-allocating LU factorization. In theory, this post-initialized solve should always be faster than the Base.LinearAlgebra version. In practice, with the way we wrap the solvers, we do not see a performance benefit and in fact benchmarks tend to show this inhibits performance.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing this solver requires adding the package FastLapackInterface.jl, i.e. using FastLapackInterface","category":"page"},{"location":"solvers/solvers/#LinearSolve.FastLUFactorization","page":"Linear System Solvers","title":"LinearSolve.FastLUFactorization","text":"FastLUFactorization()\n\nA high-performance LU factorization using the FastLapackInterface.jl package. This provides an optimized interface to LAPACK routines with reduced overhead compared to the standard LinearAlgebra LAPACK wrappers.\n\nFeatures\n\nReduced function call overhead compared to standard LAPACK wrappers\nOptimized for performance-critical applications\nUses partial pivoting (no choice of pivoting method available)\nSuitable for dense matrices where maximum performance is required\n\nLimitations\n\nDoes not allow customization of pivoting strategy (always uses partial pivoting)\nRequires FastLapackInterface.jl to be loaded\nLimited to dense matrix types supported by LAPACK\n\nRequirements\n\nUsing this solver requires that FastLapackInterface.jl is loaded: using FastLapackInterface\n\nPerformance Notes\n\nThis factorization is optimized for cases where the overhead of standard LAPACK function calls becomes significant, typically for moderate-sized dense matrices or when performing many factorizations.\n\nExample\n\nusing FastLapackInterface\nalg = FastLUFactorization()\nsol = solve(prob, alg)\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.FastQRFactorization","page":"Linear System Solvers","title":"LinearSolve.FastQRFactorization","text":"FastQRFactorization{P}(; pivot = ColumnNorm(), blocksize = 36)\n\nA high-performance QR factorization using the FastLapackInterface.jl package. This provides an optimized interface to LAPACK QR routines with reduced overhead compared to the standard LinearAlgebra LAPACK wrappers.\n\nType Parameters\n\nP: The type of pivoting strategy used\n\nFields\n\npivot::P: Pivoting strategy (e.g., ColumnNorm() for column pivoting, nothing for no pivoting)\nblocksize::Int: Block size for the blocked QR algorithm (default: 36)\n\nFeatures\n\nReduced function call overhead compared to standard LAPACK wrappers\nSupports various pivoting strategies for numerical stability\nConfigurable block size for optimal performance\nSuitable for dense matrices, especially overdetermined systems\n\nPerformance Notes\n\nThe block size can be tuned for optimal performance depending on matrix size and architecture. The default value of 36 is generally good for most cases, but experimentation may be beneficial for specific applications.\n\nRequirements\n\nUsing this solver requires that FastLapackInterface.jl is loaded: using FastLapackInterface\n\nExample\n\nusing FastLapackInterface\n# QR with column pivoting\nalg1 = FastQRFactorization()  \n# QR without pivoting for speed\nalg2 = FastQRFactorization(pivot=nothing)\n# Custom block size\nalg3 = FastQRFactorization(blocksize=64)\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#SuiteSparse.jl","page":"Linear System Solvers","title":"SuiteSparse.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing this solver requires adding the package SparseArrays.jl, i.e. using SparseArrays","category":"page"},{"location":"solvers/solvers/#LinearSolve.KLUFactorization","page":"Linear System Solvers","title":"LinearSolve.KLUFactorization","text":"KLUFactorization(;reuse_symbolic=true, check_pattern=true)\n\nA fast sparse LU-factorization which specializes on sparsity patterns with “less structure”.\n\nnote: Note\nBy default, the SparseArrays.jl are implemented for efficiency by caching the symbolic factorization. If the sparsity pattern of A may change between solves, set reuse_symbolic=false. If the pattern is assumed or known to be constant, set reuse_symbolic=true to avoid unnecessary recomputation. To further reduce computational overhead, you can disable pattern checks entirely by setting check_pattern = false. Note that this may error if the sparsity pattern does change unexpectedly.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.UMFPACKFactorization","page":"Linear System Solvers","title":"LinearSolve.UMFPACKFactorization","text":"UMFPACKFactorization(;reuse_symbolic=true, check_pattern=true)\n\nA fast sparse multithreaded LU-factorization which specializes on sparsity patterns with “more structure”.\n\nnote: Note\nBy default, the SparseArrays.jl are implemented for efficiency by caching the symbolic factorization. If the sparsity pattern of A may change between solves, set reuse_symbolic=false. If the pattern is assumed or known to be constant, set reuse_symbolic=true to avoid unnecessary recomputation. To further reduce computational overhead, you can disable pattern checks entirely by setting check_pattern = false. Note that this may error if the sparsity pattern does change unexpectedly.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#Sparspak.jl","page":"Linear System Solvers","title":"Sparspak.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing this solver requires adding the package Sparspak.jl, i.e. using Sparspak","category":"page"},{"location":"solvers/solvers/#LinearSolve.SparspakFactorization","page":"Linear System Solvers","title":"LinearSolve.SparspakFactorization","text":"SparspakFactorization(reuse_symbolic = true)\n\nThis is the translation of the well-known sparse matrix software Sparspak (Waterloo Sparse Matrix Package), solving large sparse systems of linear algebraic equations. Sparspak is composed of the subroutines from the book \"Computer Solution of Large Sparse Positive Definite Systems\" by Alan George and Joseph Liu. Originally written in Fortran 77, later rewritten in Fortran 90. Here is the software translated into Julia.\n\nThe Julia rewrite is released  under the MIT license with an express permission from the authors of the Fortran package. The package uses multiple dispatch to route around standard BLAS routines in the case e.g. of arbitrary-precision floating point numbers or ForwardDiff.Dual. This e.g. allows for Automatic Differentiation (AD) of a sparse-matrix solve.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#CliqueTrees.jl","page":"Linear System Solvers","title":"CliqueTrees.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing this solver requires adding the package CliqueTrees.jl, i.e. using CliqueTrees","category":"page"},{"location":"solvers/solvers/#LinearSolve.CliqueTreesFactorization","page":"Linear System Solvers","title":"LinearSolve.CliqueTreesFactorization","text":"CliqueTreesFactorization(\n    alg = nothing,\n    snd = nothing,\n    reuse_symbolic = true,\n)\n\nThe sparse Cholesky factorization algorithm implemented in CliqueTrees.jl. The implementation is pure-Julia and accepts arbitrary numeric types. It is somewhat slower than CHOLMOD.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#Krylov.jl","page":"Linear System Solvers","title":"Krylov.jl","text":"","category":"section"},{"location":"solvers/solvers/#LinearSolve.KrylovJL_CG","page":"Linear System Solvers","title":"LinearSolve.KrylovJL_CG","text":"KrylovJL_CG(args...; kwargs...)\n\nA generic CG implementation for Hermitian and positive definite linear systems\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.KrylovJL_MINRES","page":"Linear System Solvers","title":"LinearSolve.KrylovJL_MINRES","text":"KrylovJL_MINRES(args...; kwargs...)\n\nA generic MINRES implementation for Hermitian linear systems\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.KrylovJL_GMRES","page":"Linear System Solvers","title":"LinearSolve.KrylovJL_GMRES","text":"KrylovJL_GMRES(args...; gmres_restart = 0, window = 0, kwargs...)\n\nA generic GMRES implementation for square non-Hermitian linear systems\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.KrylovJL_BICGSTAB","page":"Linear System Solvers","title":"LinearSolve.KrylovJL_BICGSTAB","text":"KrylovJL_BICGSTAB(args...; kwargs...)\n\nA generic BICGSTAB implementation for square non-Hermitian linear systems\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.KrylovJL_LSMR","page":"Linear System Solvers","title":"LinearSolve.KrylovJL_LSMR","text":"KrylovJL_LSMR(args...; kwargs...)\n\nA generic LSMR implementation for least-squares problems\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.KrylovJL_CRAIGMR","page":"Linear System Solvers","title":"LinearSolve.KrylovJL_CRAIGMR","text":"KrylovJL_CRAIGMR(args...; kwargs...)\n\nA generic CRAIGMR implementation for least-norm problems\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.KrylovJL","page":"Linear System Solvers","title":"LinearSolve.KrylovJL","text":"KrylovJL(args...; KrylovAlg = Krylov.gmres!,\n    Pl = nothing, Pr = nothing,\n    gmres_restart = 0, window = 0,\n    kwargs...)\n\nA generic wrapper over the Krylov.jl krylov-subspace iterative solvers.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#MKL.jl","page":"Linear System Solvers","title":"MKL.jl","text":"","category":"section"},{"location":"solvers/solvers/#LinearSolve.MKLLUFactorization","page":"Linear System Solvers","title":"LinearSolve.MKLLUFactorization","text":"MKLLUFactorization()\n\nA wrapper over Intel's Math Kernel Library (MKL). Direct calls to MKL in a way that pre-allocates workspace to avoid allocations and does not require libblastrampoline.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.MKL32MixedLUFactorization","page":"Linear System Solvers","title":"LinearSolve.MKL32MixedLUFactorization","text":"MKL32MixedLUFactorization()\n\nA mixed precision LU factorization using Intel MKL that performs factorization in Float32 precision while maintaining Float64 interface. This can provide significant speedups for large matrices when reduced precision is acceptable.\n\nPerformance Notes\n\nConverts Float64 matrices to Float32 for factorization\nUses optimized MKL routines for the factorization\nCan be 2x faster than full precision for memory-bandwidth limited problems\nMay have reduced accuracy compared to full Float64 precision\n\nRequirements\n\nThis solver requires MKL to be available through MKL_jll.\n\nExample\n\nalg = MKL32MixedLUFactorization()\nsol = solve(prob, alg)\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#OpenBLAS","page":"Linear System Solvers","title":"OpenBLAS","text":"","category":"section"},{"location":"solvers/solvers/#LinearSolve.OpenBLASLUFactorization","page":"Linear System Solvers","title":"LinearSolve.OpenBLASLUFactorization","text":"OpenBLASLUFactorization()\n\nA direct wrapper over OpenBLAS's LU factorization (getrf! and getrs!). This solver makes direct calls to OpenBLAS_jll without going through Julia's libblastrampoline, which can provide performance benefits in certain configurations.\n\nPerformance Characteristics\n\nPre-allocates workspace to avoid allocations during solving\nMakes direct ccalls to OpenBLAS routines\nCan be faster than LUFactorization when OpenBLAS is well-optimized for the hardware\nSupports Float32, Float64, ComplexF32, and ComplexF64 element types\n\nWhen to Use\n\nWhen you want to ensure OpenBLAS is used regardless of the system BLAS configuration\nWhen benchmarking shows better performance than LUFactorization on your specific hardware\nWhen you need consistent behavior across different systems (always uses OpenBLAS)\n\nExample\n\nusing LinearSolve, LinearAlgebra\n\nA = rand(100, 100)\nb = rand(100)\nprob = LinearProblem(A, b)\nsol = solve(prob, OpenBLASLUFactorization())\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#AppleAccelerate.jl","page":"Linear System Solvers","title":"AppleAccelerate.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing this solver requires a Mac with Apple Accelerate. This should come standard in most \"modern\" Mac computers.","category":"page"},{"location":"solvers/solvers/#LinearSolve.AppleAccelerateLUFactorization","page":"Linear System Solvers","title":"LinearSolve.AppleAccelerateLUFactorization","text":"AppleAccelerateLUFactorization()\n\nA wrapper over Apple's Accelerate Library. Direct calls to Acceelrate in a way that pre-allocates workspace to avoid allocations and does not require libblastrampoline.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.AppleAccelerate32MixedLUFactorization","page":"Linear System Solvers","title":"LinearSolve.AppleAccelerate32MixedLUFactorization","text":"AppleAccelerate32MixedLUFactorization()\n\nA mixed precision LU factorization using Apple's Accelerate framework that performs factorization in Float32 precision while maintaining Float64 interface. This can provide significant speedups on Apple hardware when reduced precision is acceptable.\n\nPerformance Notes\n\nConverts Float64 matrices to Float32 for factorization\nUses optimized Accelerate routines for the factorization\nParticularly effective on Apple Silicon with unified memory\nMay have reduced accuracy compared to full Float64 precision\n\nRequirements\n\nThis solver is only available on Apple platforms and requires the Accelerate framework.\n\nExample\n\nalg = AppleAccelerate32MixedLUFactorization()\nsol = solve(prob, alg)\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#Metal.jl","page":"Linear System Solvers","title":"Metal.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing this solver requires adding the package Metal.jl, i.e. using Metal. This package is only compatible with Mac M-Series computers with a Metal-compatible GPU.","category":"page"},{"location":"solvers/solvers/#LinearSolve.MetalLUFactorization","page":"Linear System Solvers","title":"LinearSolve.MetalLUFactorization","text":"MetalLUFactorization()\n\nA wrapper over Apple's Metal GPU library for LU factorization. Direct calls to Metal  in a way that pre-allocates workspace to avoid allocations and automatically offloads  to the GPU. This solver is optimized for Metal-capable Apple Silicon Macs.\n\nRequirements\n\nUsing this solver requires that Metal.jl is loaded: using Metal\n\nPerformance Notes\n\nMost efficient for large dense matrices where GPU acceleration benefits outweigh transfer costs\nAutomatically manages GPU memory and transfers\nParticularly effective on Apple Silicon Macs with unified memory\n\nExample\n\nusing Metal\nalg = MetalLUFactorization()\nsol = solve(prob, alg)\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.MetalOffload32MixedLUFactorization","page":"Linear System Solvers","title":"LinearSolve.MetalOffload32MixedLUFactorization","text":"MetalOffload32MixedLUFactorization()\n\nA mixed precision Metal GPU-accelerated LU factorization that converts matrices to Float32 before offloading to Metal GPU for factorization, then converts back for the solve. This can provide speedups on Apple Silicon when reduced precision is acceptable.\n\nPerformance Notes\n\nConverts Float64 matrices to Float32 for GPU factorization\nCan be significantly faster for large matrices where memory bandwidth is limiting\nParticularly effective on Apple Silicon Macs with unified memory architecture\nMay have reduced accuracy compared to full precision methods\n\nRequirements\n\nUsing this solver requires that Metal.jl is loaded: using Metal\n\nExample\n\nusing Metal\nalg = MetalOffload32MixedLUFactorization()\nsol = solve(prob, alg)\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#Pardiso.jl","page":"Linear System Solvers","title":"Pardiso.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing this solver requires adding the package Pardiso.jl, i.e. using Pardiso","category":"page"},{"location":"solvers/solvers/#LinearSolve.MKLPardisoFactorize","page":"Linear System Solvers","title":"LinearSolve.MKLPardisoFactorize","text":"MKLPardisoFactorize(; nprocs::Union{Int, Nothing} = nothing,\n    matrix_type = nothing,\n    cache_analysis = false,\n    iparm::Union{Vector{Tuple{Int, Int}}, Nothing} = nothing,\n    dparm::Union{Vector{Tuple{Int, Int}}, Nothing} = nothing)\n\nA sparse factorization method using MKL Pardiso.\n\nnote: Note\nUsing this solver requires adding the package Pardiso.jl, i.e. using Pardiso\n\nKeyword Arguments\n\nSetting cache_analysis = true disables Pardiso's scaling and matching defaults and caches the result of the initial analysis phase for all further computations with this solver.\n\nFor the definition of the other keyword arguments, see the Pardiso.jl documentation. All values default to nothing and the solver internally determines the values given the input types, and these keyword arguments are only for overriding the default handling process. This should not be required by most users.\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.MKLPardisoIterate","page":"Linear System Solvers","title":"LinearSolve.MKLPardisoIterate","text":"MKLPardisoIterate(; nprocs::Union{Int, Nothing} = nothing,\n    matrix_type = nothing,\n    cache_analysis = false,\n    iparm::Union{Vector{Tuple{Int, Int}}, Nothing} = nothing,\n    dparm::Union{Vector{Tuple{Int, Int}}, Nothing} = nothing)\n\nA mixed factorization+iterative method using MKL Pardiso.\n\nnote: Note\nUsing this solver requires adding the package Pardiso.jl, i.e. using Pardiso\n\nKeyword Arguments\n\nSetting cache_analysis = true disables Pardiso's scaling and matching defaults and caches the result of the initial analysis phase for all further computations with this solver.\n\nFor the definition of the other keyword arguments, see the Pardiso.jl documentation. All values default to nothing and the solver internally determines the values given the input types, and these keyword arguments are only for overriding the default handling process. This should not be required by most users.\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.PardisoJL","page":"Linear System Solvers","title":"LinearSolve.PardisoJL","text":"PardisoJL(; nprocs::Union{Int, Nothing} = nothing,\n    solver_type = nothing,\n    matrix_type = nothing,\n    iparm::Union{Vector{Tuple{Int, Int}}, Nothing} = nothing,\n    dparm::Union{Vector{Tuple{Int, Int}}, Nothing} = nothing,\n    vendor::Union{Symbol, Nothing} = nothing\n)\n\nA generic method using  Pardiso. Specifying solver_type is required.\n\nnote: Note\nUsing this solver requires adding the package Pardiso.jl, i.e. using Pardiso\n\nKeyword Arguments\n\nThe vendor keyword allows to choose between Panua pardiso  (former pardiso-project.org; vendor=:Panua) and  MKL Pardiso (vendor=:MKL). If vendor==nothing, Panua pardiso is preferred over MKL Pardiso.\n\nFor the definition of the other keyword arguments, see the Pardiso.jl documentation. All values default to nothing and the solver internally determines the values given the input types, and these keyword arguments are only for overriding the default handling process. This should not be required by most users.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#CUDA.jl","page":"Linear System Solvers","title":"CUDA.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Note that CuArrays are supported by GenericFactorization in the \"normal\" way. The following are non-standard GPU factorization routines.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing these solvers requires adding the package CUDA.jl, i.e. using CUDA","category":"page"},{"location":"solvers/solvers/#LinearSolve.CudaOffloadLUFactorization","page":"Linear System Solvers","title":"LinearSolve.CudaOffloadLUFactorization","text":"CudaOffloadLUFactorization()\n\nAn offloading technique used to GPU-accelerate CPU-based computations using LU factorization. Requires a sufficiently large A to overcome the data transfer costs.\n\nnote: Note\nUsing this solver requires adding the package CUDA.jl, i.e. using CUDA\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.CudaOffloadQRFactorization","page":"Linear System Solvers","title":"LinearSolve.CudaOffloadQRFactorization","text":"CudaOffloadQRFactorization()\n\nAn offloading technique used to GPU-accelerate CPU-based computations using QR factorization. Requires a sufficiently large A to overcome the data transfer costs.\n\nnote: Note\nUsing this solver requires adding the package CUDA.jl, i.e. using CUDA\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.CUDAOffload32MixedLUFactorization","page":"Linear System Solvers","title":"LinearSolve.CUDAOffload32MixedLUFactorization","text":"CUDAOffload32MixedLUFactorization()\n\nA mixed precision GPU-accelerated LU factorization that converts matrices to Float32  before offloading to CUDA GPU for factorization, then converts back for the solve. This can provide speedups when the reduced precision is acceptable and memory  bandwidth is a bottleneck.\n\nPerformance Notes\n\nConverts Float64 matrices to Float32 for GPU factorization\nCan be significantly faster for large matrices where memory bandwidth is limiting\nMay have reduced accuracy compared to full precision methods\nMost beneficial when the condition number of the matrix is moderate\n\nnote: Note\nUsing this solver requires adding the package CUDA.jl, i.e. using CUDA\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#AMDGPU.jl","page":"Linear System Solvers","title":"AMDGPU.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"The following are GPU factorization routines for AMD GPUs using the ROCm stack.","category":"page"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing these solvers requires adding the package AMDGPU.jl, i.e. using AMDGPU","category":"page"},{"location":"solvers/solvers/#LinearSolve.AMDGPUOffloadLUFactorization","page":"Linear System Solvers","title":"LinearSolve.AMDGPUOffloadLUFactorization","text":"AMDGPUOffloadLUFactorization()\n\nAn offloading technique using LU factorization to GPU-accelerate CPU-based computations on AMD GPUs. Requires a sufficiently large A to overcome the data transfer costs.\n\nnote: Note\nUsing this solver requires adding the package AMDGPU.jl, i.e. using AMDGPU\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#LinearSolve.AMDGPUOffloadQRFactorization","page":"Linear System Solvers","title":"LinearSolve.AMDGPUOffloadQRFactorization","text":"AMDGPUOffloadQRFactorization()\n\nAn offloading technique using QR factorization to GPU-accelerate CPU-based computations on AMD GPUs. Requires a sufficiently large A to overcome the data transfer costs.\n\nnote: Note\nUsing this solver requires adding the package AMDGPU.jl, i.e. using AMDGPU\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#CUSOLVERRF.jl","page":"Linear System Solvers","title":"CUSOLVERRF.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing this solver requires adding the package CUSOLVERRF.jl, i.e. using CUSOLVERRF","category":"page"},{"location":"solvers/solvers/#LinearSolve.CUSOLVERRFFactorization","page":"Linear System Solvers","title":"LinearSolve.CUSOLVERRFFactorization","text":"CUSOLVERRFFactorization(; symbolic = :RF, reuse_symbolic = true)\n\nA GPU-accelerated sparse LU factorization using NVIDIA's cusolverRF library. This solver is specifically designed for sparse matrices on CUDA GPUs and  provides high-performance factorization and solve capabilities.\n\nKeyword Arguments\n\nsymbolic: The symbolic factorization method to use. Options are:\n:RF (default): Use cusolverRF's built-in symbolic analysis\n:KLU: Use KLU for symbolic analysis\nreuse_symbolic: Whether to reuse the symbolic factorization when the  sparsity pattern doesn't change (default: true)\n\nnote: Note\nThis solver requires CUSOLVERRF.jl to be loaded and only supports  Float64 element types with Int32 indices.\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#IterativeSolvers.jl","page":"Linear System Solvers","title":"IterativeSolvers.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing these solvers requires adding the package IterativeSolvers.jl, i.e. using IterativeSolvers","category":"page"},{"location":"solvers/solvers/#LinearSolve.IterativeSolversJL_CG","page":"Linear System Solvers","title":"LinearSolve.IterativeSolversJL_CG","text":"IterativeSolversJL_CG(args...; Pl = nothing, Pr = nothing, kwargs...)\n\nA wrapper over the IterativeSolvers.jl CG.\n\nnote: Note\nUsing this solver requires adding the package IterativeSolvers.jl, i.e. using IterativeSolvers\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.IterativeSolversJL_GMRES","page":"Linear System Solvers","title":"LinearSolve.IterativeSolversJL_GMRES","text":"IterativeSolversJL_GMRES(args...; Pl = nothing, Pr = nothing, gmres_restart = 0, kwargs...)\n\nA wrapper over the IterativeSolvers.jl GMRES.\n\nnote: Note\nUsing this solver requires adding the package IterativeSolvers.jl, i.e. using IterativeSolvers\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.IterativeSolversJL_BICGSTAB","page":"Linear System Solvers","title":"LinearSolve.IterativeSolversJL_BICGSTAB","text":"IterativeSolversJL_BICGSTAB(args...; Pl = nothing, Pr = nothing, kwargs...)\n\nA wrapper over the IterativeSolvers.jl BICGSTAB.\n\nnote: Note\nUsing this solver requires adding the package IterativeSolvers.jl, i.e. using IterativeSolvers\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.IterativeSolversJL_MINRES","page":"Linear System Solvers","title":"LinearSolve.IterativeSolversJL_MINRES","text":"IterativeSolversJL_MINRES(args...; Pl = nothing, Pr = nothing, kwargs...)\n\nA wrapper over the IterativeSolvers.jl MINRES.\n\nnote: Note\nUsing this solver requires adding the package IterativeSolvers.jl, i.e. using IterativeSolvers\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.IterativeSolversJL_IDRS","page":"Linear System Solvers","title":"LinearSolve.IterativeSolversJL_IDRS","text":"IterativeSolversJL_IDRS(args...; Pl = nothing, kwargs...)\n\nA wrapper over the IterativeSolvers.jl IDR(S).\n\nnote: Note\nUsing this solver requires adding the package IterativeSolvers.jl, i.e. using IterativeSolvers\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.IterativeSolversJL","page":"Linear System Solvers","title":"LinearSolve.IterativeSolversJL","text":"IterativeSolversJL(args...;\n    generate_iterator = IterativeSolvers.gmres_iterable!,\n    Pl = nothing, Pr = nothing,\n    gmres_restart = 0, kwargs...)\n\nA generic wrapper over the IterativeSolvers.jl solvers.\n\nnote: Note\nUsing this solver requires adding the package IterativeSolvers.jl, i.e. using IterativeSolvers\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#KrylovKit.jl","page":"Linear System Solvers","title":"KrylovKit.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing these solvers requires adding the package KrylovKit.jl, i.e. using KrylovKit","category":"page"},{"location":"solvers/solvers/#LinearSolve.KrylovKitJL_CG","page":"Linear System Solvers","title":"LinearSolve.KrylovKitJL_CG","text":"KrylovKitJL_CG(args...; Pl = nothing, Pr = nothing, kwargs...)\n\nA generic CG implementation for Hermitian and positive definite linear systems\n\nnote: Note\nUsing this solver requires adding the package KrylovKit.jl, i.e. using KrylovKit\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.KrylovKitJL_GMRES","page":"Linear System Solvers","title":"LinearSolve.KrylovKitJL_GMRES","text":"KrylovKitJL_GMRES(args...; Pl = nothing, Pr = nothing, gmres_restart = 0, kwargs...)\n\nA generic GMRES implementation.\n\nnote: Note\nUsing this solver requires adding the package KrylovKit.jl, i.e. using KrylovKit\n\n\n\n\n\n","category":"function"},{"location":"solvers/solvers/#LinearSolve.KrylovKitJL","page":"Linear System Solvers","title":"LinearSolve.KrylovKitJL","text":"KrylovKitJL(args...; KrylovAlg = Krylov.gmres!, kwargs...)\n\nA generic iterative solver implementation allowing the choice of KrylovKit.jl solvers.\n\nnote: Note\nUsing this solver requires adding the package KrylovKit.jl, i.e. using KrylovKit\n\n\n\n\n\n","category":"type"},{"location":"solvers/solvers/#HYPRE.jl","page":"Linear System Solvers","title":"HYPRE.jl","text":"","category":"section"},{"location":"solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing HYPRE solvers requires Julia version 1.9 or higher, and that the package HYPRE.jl is installed.","category":"page"},{"location":"solvers/solvers/#LinearSolve.HYPREAlgorithm","page":"Linear System Solvers","title":"LinearSolve.HYPREAlgorithm","text":"HYPREAlgorithm(solver; Pl = nothing)\n\nHYPRE.jl is an interface to hypre and provide iterative solvers and preconditioners for sparse linear systems. It is mainly developed for large multi-process distributed problems (using MPI), but can also be used for single-process problems with Julias standard sparse matrices.\n\nIf you need more fine-grained control over the solver/preconditioner options you can alternatively pass an already created solver to HYPREAlgorithm (and to the Pl keyword argument). See HYPRE.jl docs for how to set up solvers with specific options.\n\nnote: Note\nUsing HYPRE solvers requires Julia version 1.9 or higher, and that the package HYPRE.jl is installed.\n\nPositional Arguments\n\nThe single positional argument solver has the following choices:\n\nHYPRE.BiCGSTAB\nHYPRE.BoomerAMG\nHYPRE.FlexGMRES\nHYPRE.GMRES\nHYPRE.Hybrid\nHYPRE.ILU\nHYPRE.ParaSails (as preconditioner only)\nHYPRE.PCG\n\nKeyword Arguments\n\nPl: A choice of left preconditioner.\n\nExample\n\nFor example, to use HYPRE.PCG as the solver, with HYPRE.BoomerAMG as the preconditioner, the algorithm should be defined as follows:\n\nA, b = setup_system(...)\nprob = LinearProblem(A, b)\nalg = HYPREAlgorithm(HYPRE.PCG)\nprec = HYPRE.BoomerAMG\nsol = solve(prob, alg; Pl = prec)\n\n\n\n\n\n","category":"type"},{"location":"#LinearSolve.jl:-High-Performance-Unified-Linear-Solvers","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"","category":"section"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"LinearSolve.jl is a unified interface for the linear solving packages of Julia. It interfaces with other packages of the Julia ecosystem to make it easy to test alternative solver packages and pass small types to control algorithm swapping. It also interfaces with the ModelingToolkit.jl world of symbolic modeling to allow for automatically generating high-performance code.","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"Performance is key: the current methods are made to be highly performant on scalar and statically sized small problems, with options for large-scale systems. If you run into any performance issues, please file an issue.","category":"page"},{"location":"#Installation","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"Installation","text":"","category":"section"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"To install LinearSolve.jl, use the Julia package manager:","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"using Pkg\nPkg.add(\"LinearSolve\")","category":"page"},{"location":"#Contributing","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"Contributing","text":"","category":"section"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nSee the SciML Style Guide for common coding practices and other style decisions.\nThere are a few community forums:\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Slack\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Zulip\nOn the Julia Discourse forums\nSee also SciML Community page","category":"page"},{"location":"#Roadmap","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"Roadmap","text":"","category":"section"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"Wrappers for every linear solver in the Julia language is on the roadmap. If there are any important ones that are missing that you would like to see added, please open an issue. The current algorithms should support automatic differentiation. Pre-defined preconditioners would be a welcome addition.","category":"page"},{"location":"#Reproducibility","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"</details>","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"</details>","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"using Pkg # hide\nPkg.status(; mode = PKGMODE_MANIFEST) # hide","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"</details>","category":"page"},{"location":"","page":"LinearSolve.jl: High-Performance Unified Linear Solvers","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"using TOML\nusing Markdown\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink_manifest = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n                \"/assets/Manifest.toml\"\nlink_project = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n               \"/assets/Project.toml\"\nMarkdown.parse(\"\"\"You can also download the\n[manifest]($link_manifest)\nfile and the\n[project]($link_project)\nfile.\n\"\"\")","category":"page"},{"location":"tutorials/caching_interface/#Linear-Solve-with-Caching-Interface","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"","category":"section"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"Often, one may want to cache information that is reused between different linear solves. For example, if one is going to perform:","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"A \\ b1\nA \\ b2","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"then it would be more efficient to LU-factorize one time and reuse the factorization:","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"LA.lu!(A)\nA \\ b1\nA \\ b2","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"LinearSolve.jl's caching interface automates this process to use the most efficient means of solving and resolving linear systems. To do this with LinearSolve.jl, you simply init a cache, solve, replace b, and solve again. This looks like:","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"import LinearSolve as LS\nimport LinearAlgebra as LA\n\nn = 4\nA = rand(n, n)\nb1 = rand(n);\nb2 = rand(n);\nprob = LS.LinearProblem(A, b1)\n\nlinsolve = LS.init(prob)\nsol1 = LS.solve!(linsolve)","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"linsolve.b = b2\nsol2 = LS.solve!(linsolve)\n\nsol2.u","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"Then refactorization will occur when a new A is given:","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"A2 = rand(n, n)\nlinsolve.A = A2\nsol3 = LS.solve!(linsolve)\n\nsol3.u","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"The factorization occurs on the first solve, and it stores the factorization in the cache. You can retrieve this cache via sol.cache, which is the same object as the init, but updated to know not to re-solve the factorization.","category":"page"},{"location":"tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"The advantage of course with import LinearSolve.jl in this form is that it is efficient while being agnostic to the linear solver. One can easily swap in iterative solvers, sparse solvers, etc. and it will do all the tricks like caching the symbolic factorization if the sparsity pattern is unchanged.","category":"page"}]
}
