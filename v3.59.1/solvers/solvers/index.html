<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Linear System Solvers · LinearSolve.jl</title><meta name="title" content="Linear System Solvers · LinearSolve.jl"/><meta property="og:title" content="Linear System Solvers · LinearSolve.jl"/><meta property="twitter:title" content="Linear System Solvers · LinearSolve.jl"/><meta name="description" content="Documentation for LinearSolve.jl."/><meta property="og:description" content="Documentation for LinearSolve.jl."/><meta property="twitter:description" content="Documentation for LinearSolve.jl."/><meta property="og:url" content="https://docs.sciml.ai/LinearSolve/stable/solvers/solvers/"/><meta property="twitter:url" content="https://docs.sciml.ai/LinearSolve/stable/solvers/solvers/"/><link rel="canonical" href="https://docs.sciml.ai/LinearSolve/stable/solvers/solvers/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="LinearSolve.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">LinearSolve.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">LinearSolve.jl: High-Performance Unified Linear Solvers</a></li><li><a class="tocitem" href="../../tutorials/linear/">Getting Started with Solving Linear Systems in Julia</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/caching_interface/">Linear Solve with Caching Interface</a></li><li><a class="tocitem" href="../../tutorials/accelerating_choices/">Accelerating your Linear Solves</a></li><li><a class="tocitem" href="../../tutorials/gpu/">GPU-Accelerated Linear Solving in Julia</a></li><li><a class="tocitem" href="../../tutorials/autotune/">Automatic Algorithm Selection with LinearSolveAutotune</a></li></ul></li><li><span class="tocitem">Basics</span><ul><li><a class="tocitem" href="../../basics/LinearProblem/">Linear Problems</a></li><li><a class="tocitem" href="../../basics/algorithm_selection/">Algorithm Selection Guide</a></li><li><a class="tocitem" href="../../basics/common_solver_opts/">Common Solver Options (Keyword Arguments for Solve)</a></li><li><a class="tocitem" href="../../basics/OperatorAssumptions/">Linear Solve Operator Assumptions</a></li><li><a class="tocitem" href="../../basics/Preconditioners/">Preconditioners</a></li><li><a class="tocitem" href="../../basics/FAQ/">Frequently Asked Questions</a></li></ul></li><li><span class="tocitem">Solvers</span><ul><li class="is-active"><a class="tocitem" href>Linear System Solvers</a><ul class="internal"><li><a class="tocitem" href="#Recommended-Methods"><span>Recommended Methods</span></a></li><li><a class="tocitem" href="#Full-List-of-Methods"><span>Full List of Methods</span></a></li></ul></li></ul></li><li><span class="tocitem">Advanced</span><ul><li><a class="tocitem" href="../../advanced/developing/">Developing New Linear Solvers</a></li><li><a class="tocitem" href="../../advanced/custom/">Passing in a Custom Linear Solver</a></li><li><a class="tocitem" href="../../advanced/internal_api/">Internal API Documentation</a></li></ul></li><li><a class="tocitem" href="../../release_notes/">Release Notes</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Solvers</a></li><li class="is-active"><a href>Linear System Solvers</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Linear System Solvers</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/SciML/LinearSolve.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/SciML/LinearSolve.jl/blob/main/docs/src/solvers/solvers.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="linearsystemsolvers"><a class="docs-heading-anchor" href="#linearsystemsolvers">Linear System Solvers</a><a id="linearsystemsolvers-1"></a><a class="docs-heading-anchor-permalink" href="#linearsystemsolvers" title="Permalink"></a></h1><p><code>LS.solve(prob::LS.LinearProblem,alg;kwargs)</code></p><p>Solves for <span>$Au=b$</span> in the problem defined by <code>prob</code> using the algorithm <code>alg</code>. If no algorithm is given, a default algorithm will be chosen.</p><h2 id="Recommended-Methods"><a class="docs-heading-anchor" href="#Recommended-Methods">Recommended Methods</a><a id="Recommended-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Recommended-Methods" title="Permalink"></a></h2><h3 id="Dense-Matrices"><a class="docs-heading-anchor" href="#Dense-Matrices">Dense Matrices</a><a id="Dense-Matrices-1"></a><a class="docs-heading-anchor-permalink" href="#Dense-Matrices" title="Permalink"></a></h3><p>The default algorithm <code>nothing</code> is good for picking an algorithm that will work, but one may need to change this to receive more performance or precision. If more precision is necessary, <code>LS.QRFactorization()</code> and <code>LS.SVDFactorization()</code> are the best choices, with SVD being the slowest but most precise.</p><p>For efficiency, <code>RFLUFactorization</code> is the fastest for dense LU-factorizations until around 150x150 matrices, though this can be dependent on the exact details of the hardware. After this point, <code>MKLLUFactorization</code> is usually faster on most hardware. Note that on Mac computers that <code>AppleAccelerateLUFactorization</code> is generally always the fastest. <code>OpenBLASLUFactorization</code>  provides direct OpenBLAS calls without going through libblastrampoline and can be faster than  <code>LUFactorization</code> in some configurations. <code>LUFactorization</code> will use your base system BLAS which  can be fast or slow depending on the hardware configuration. <code>SimpleLUFactorization</code> will be fast  only on very small matrices but can cut down on compile times.</p><p>For very large dense factorizations, offloading to the GPU can be preferred. Metal.jl can be used on Mac hardware to offload, and has a cutoff point of being faster at around size 20,000 x 20,000 matrices (and only supports Float32). <code>CudaOffloadLUFactorization</code> and <code>CudaOffloadQRFactorization</code>  can be more efficient at a much smaller cutoff, possibly around size 1,000 x 1,000 matrices, though  this is highly dependent on the chosen GPU hardware. These algorithms require a CUDA-compatible NVIDIA GPU. CUDA offload supports Float64 but most consumer GPU hardware will be much faster on Float32 (many are &gt;32x faster for Float32 operations than Float64 operations) and thus for most hardware this is only recommended for Float32 matrices. Choose <code>CudaOffloadLUFactorization</code> for better  performance on well-conditioned problems, or <code>CudaOffloadQRFactorization</code> for better numerical  stability on ill-conditioned problems.</p><h4 id="Mixed-Precision-Methods"><a class="docs-heading-anchor" href="#Mixed-Precision-Methods">Mixed Precision Methods</a><a id="Mixed-Precision-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Mixed-Precision-Methods" title="Permalink"></a></h4><p>For large well-conditioned problems where memory bandwidth is the bottleneck, mixed precision  methods can provide significant speedups (up to 2x) by performing the factorization in Float32  while maintaining Float64 interfaces. These methods are particularly effective for:</p><ul><li>Large dense matrices (&gt; 1000x1000)</li><li>Well-conditioned problems (condition number &lt; 10^4)</li><li>Hardware with good Float32 performance</li></ul><p>Available mixed precision solvers:</p><ul><li><code>MKL32MixedLUFactorization</code> - CPUs with MKL</li><li><code>AppleAccelerate32MixedLUFactorization</code> - Apple CPUs with Accelerate</li><li><code>CUDAOffload32MixedLUFactorization</code> - NVIDIA GPUs with CUDA</li><li><code>MetalOffload32MixedLUFactorization</code> - Apple GPUs with Metal</li></ul><p>These methods automatically handle the precision conversion, making them easy drop-in replacements when reduced precision is acceptable for the factorization step.</p><div class="admonition is-info" id="Note-2d180c44e4a19f9c"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-2d180c44e4a19f9c" title="Permalink"></a></header><div class="admonition-body"><p>Performance details for dense LU-factorizations can be highly dependent on the hardware configuration. For details see <a href="https://github.com/SciML/LinearSolve.jl/issues/357">this issue</a>. If one is looking to best optimize their system, we suggest running the performance tuning benchmark.</p></div></div><h3 id="Sparse-Matrices"><a class="docs-heading-anchor" href="#Sparse-Matrices">Sparse Matrices</a><a id="Sparse-Matrices-1"></a><a class="docs-heading-anchor-permalink" href="#Sparse-Matrices" title="Permalink"></a></h3><p>For sparse LU-factorizations, <code>KLUFactorization</code> if there is less structure to the sparsity pattern and <code>UMFPACKFactorization</code> if there is more structure. Pardiso.jl&#39;s methods are also known to be very efficient sparse linear solvers.</p><p>For GPU-accelerated sparse LU-factorizations, there are two high-performance options. When using CuSparseMatrixCSR arrays with CUDSS.jl loaded, <code>LUFactorization()</code> will automatically use NVIDIA&#39;s cuDSS library. Alternatively, <code>CUSOLVERRFFactorization</code> provides access to NVIDIA&#39;s cusolverRF library. Both offer significant performance improvements for sparse systems on CUDA-capable GPUs and are particularly effective for large sparse matrices that can benefit from GPU parallelization. <code>CUDSS</code> is more for <code>Float32</code> while <code>CUSOLVERRFFactorization</code> is for <code>Float64</code>.</p><p>While these sparse factorizations are based on implementations in other languages, and therefore constrained to standard number types (<code>Float64</code>,  <code>Float32</code> and their complex counterparts),  <code>SparspakFactorization</code> is able to handle general number types, e.g. defined by <code>ForwardDiff.jl</code>, <code>MultiFloats.jl</code>, or <code>IntervalArithmetics.jl</code>.</p><p>As sparse matrices get larger, iterative solvers tend to get more efficient than factorization methods if a lower tolerance of the solution is required.</p><p>Krylov.jl generally outperforms IterativeSolvers.jl and KrylovKit.jl, and is compatible with CPUs and GPUs, and thus is the generally preferred form for Krylov methods. The choice of Krylov method should be the one most constrained to the type of operator one has, for example if positive definite then <code>KrylovJL_CG()</code>, but if no good properties then use <code>KrylovJL_GMRES()</code>.</p><p>Finally, a user can pass a custom function for handling the linear solve using <code>LS.LinearSolveFunction()</code> if existing solvers are not optimally suited for their application. The interface is detailed <a href="../../advanced/custom/#custom">here</a>.</p><h3 id="Lazy-SciMLOperators"><a class="docs-heading-anchor" href="#Lazy-SciMLOperators">Lazy SciMLOperators</a><a id="Lazy-SciMLOperators-1"></a><a class="docs-heading-anchor-permalink" href="#Lazy-SciMLOperators" title="Permalink"></a></h3><p>If the linear operator is given as a lazy non-concrete operator, such as a <code>FunctionOperator</code>, then using a Krylov method is preferred in order to not concretize the matrix. Krylov.jl generally outperforms IterativeSolvers.jl and KrylovKit.jl, and is compatible with CPUs and GPUs, and thus is the generally preferred form for Krylov methods. The choice of Krylov method should be the one most constrained to the type of operator one has, for example if positive definite then <code>KrylovJL_CG()</code>, but if no good properties then use <code>KrylovJL_GMRES()</code>.</p><div class="admonition is-success" id="Tip-f22756d9a8278c24"><header class="admonition-header">Tip<a class="admonition-anchor" href="#Tip-f22756d9a8278c24" title="Permalink"></a></header><div class="admonition-body"><p>If your materialized operator is a uniform block diagonal matrix, then you can use <code>SimpleGMRES(; blocksize = &lt;known block size&gt;)</code> to further improve performance. This often shows up in Neural Networks where the Jacobian wrt the Inputs (almost always) is a Uniform Block Diagonal matrix of Block Size = size of the input divided by the batch size.</p></div></div><h2 id="Full-List-of-Methods"><a class="docs-heading-anchor" href="#Full-List-of-Methods">Full List of Methods</a><a id="Full-List-of-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Full-List-of-Methods" title="Permalink"></a></h2><h3 id="Polyalgorithms"><a class="docs-heading-anchor" href="#Polyalgorithms">Polyalgorithms</a><a id="Polyalgorithms-1"></a><a class="docs-heading-anchor-permalink" href="#Polyalgorithms" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearSolve.DefaultLinearSolver"><a class="docstring-binding" href="#LinearSolve.DefaultLinearSolver"><code>LinearSolve.DefaultLinearSolver</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">DefaultLinearSolver(;safetyfallback=true)</code></pre><p>The default linear solver. This is the algorithm chosen when <code>solve(prob)</code> is called. It&#39;s a polyalgorithm that detects the optimal method for a given <code>A, b</code> and hardware (Intel, AMD, GPU, etc.).</p><p><strong>Keyword Arguments</strong></p><ul><li><code>safetyfallback</code>: determines whether to fallback to a column-pivoted QR factorization when an LU factorization fails. This can be required if <code>A</code> is rank-deficient. Defaults to true.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/LinearSolve.jl#L369-L381">source</a></section></details></article><h3 id="RecursiveFactorization.jl"><a class="docs-heading-anchor" href="#RecursiveFactorization.jl">RecursiveFactorization.jl</a><a id="RecursiveFactorization.jl-1"></a><a class="docs-heading-anchor-permalink" href="#RecursiveFactorization.jl" title="Permalink"></a></h3><div class="admonition is-info" id="Note-7c188927234540a9"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-7c188927234540a9" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires adding the package RecursiveFactorization.jl, i.e. <code>using RecursiveFactorization</code></p></div></div><article><details class="docstring" open="true"><summary id="LinearSolve.RFLUFactorization"><a class="docstring-binding" href="#LinearSolve.RFLUFactorization"><code>LinearSolve.RFLUFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">RFLUFactorization{P, T}(; pivot = Val(true), thread = Val(true))</code></pre><p>A fast pure Julia LU-factorization implementation using RecursiveFactorization.jl.  This is by far the fastest LU-factorization implementation, usually outperforming  OpenBLAS and MKL for smaller matrices (&lt;500x500), but currently optimized only for  Base <code>Array</code> with <code>Float32</code> or <code>Float64</code>. Additional optimization for complex matrices  is in the works.</p><p><strong>Type Parameters</strong></p><ul><li><code>P</code>: Pivoting strategy as <code>Val{Bool}</code>. <code>Val{true}</code> enables partial pivoting for stability.</li><li><code>T</code>: Threading strategy as <code>Val{Bool}</code>. <code>Val{true}</code> enables multi-threading for performance.</li></ul><p><strong>Constructor Arguments</strong></p><ul><li><code>pivot = Val(true)</code>: Enable partial pivoting. Set to <code>Val{false}</code> to disable for speed  at the cost of numerical stability.</li><li><code>thread = Val(true)</code>: Enable multi-threading. Set to <code>Val{false}</code> for single-threaded  execution.</li><li><code>throwerror = true</code>: Whether to throw an error if RecursiveFactorization.jl is not loaded.</li></ul><p><strong>Performance Notes</strong></p><ul><li>Fastest for dense matrices with dimensions roughly &lt; 500×500</li><li>Optimized specifically for Float32 and Float64 element types</li><li>Recursive blocking strategy provides excellent cache performance</li><li>Multi-threading can provide significant speedups on multi-core systems</li></ul><p><strong>Requirements</strong></p><p>Using this solver requires that RecursiveFactorization.jl is loaded: <code>using RecursiveFactorization</code></p><p><strong>Example</strong></p><pre><code class="language-julia hljs">using RecursiveFactorization
# Fast, stable (with pivoting)
alg1 = RFLUFactorization()
# Fastest (no pivoting), less stable
alg2 = RFLUFactorization(pivot=Val(false))  </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L262-L299">source</a></section></details></article><h3 id="Base.LinearAlgebra"><a class="docs-heading-anchor" href="#Base.LinearAlgebra">Base.LinearAlgebra</a><a id="Base.LinearAlgebra-1"></a><a class="docs-heading-anchor-permalink" href="#Base.LinearAlgebra" title="Permalink"></a></h3><p>These overloads tend to work for many array types, such as <code>CuArrays</code> for GPU-accelerated solving, using the overloads provided by the respective packages. Given that this can be customized per-package, details given below describe a subset of important arrays (<code>Matrix</code>, <code>SparseMatrixCSC</code>, <code>CuMatrix</code>, etc.)</p><article><details class="docstring" open="true"><summary id="LinearSolve.LUFactorization"><a class="docstring-binding" href="#LinearSolve.LUFactorization"><code>LinearSolve.LUFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><p><code>LUFactorization(pivot=LinearAlgebra.RowMaximum())</code></p><p>Julia&#39;s built in <code>lu</code>. Equivalent to calling <code>lu!(A)</code></p><ul><li>On dense matrices, this uses the current BLAS implementation of the user&#39;s computer, which by default is OpenBLAS but will use MKL if the user does <code>using MKL</code> in their system.</li><li>On sparse matrices, this will use UMFPACK from SparseArrays. Note that this will not cache the symbolic factorization.</li><li>On CuMatrix, it will use a CUDA-accelerated LU from CuSolver.</li><li>On BandedMatrix and BlockBandedMatrix, it will use a banded LU.</li></ul><p><strong>Positional Arguments</strong></p><ul><li>pivot: The choice of pivoting. Defaults to <code>LinearAlgebra.RowMaximum()</code>. The other choice is <code>LinearAlgebra.NoPivot()</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/factorization.jl#L104-L121">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.GenericLUFactorization"><a class="docstring-binding" href="#LinearSolve.GenericLUFactorization"><code>LinearSolve.GenericLUFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><p><code>GenericLUFactorization(pivot=LinearAlgebra.RowMaximum())</code></p><p>Julia&#39;s built in generic LU factorization. Equivalent to calling LinearAlgebra.generic_lufact!. Supports arbitrary number types but does not achieve as good scaling as BLAS-based LU implementations. Has low overhead and is good for small matrices.</p><p><strong>Positional Arguments</strong></p><ul><li>pivot: The choice of pivoting. Defaults to <code>LinearAlgebra.RowMaximum()</code>. The other choice is <code>LinearAlgebra.NoPivot()</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/factorization.jl#L131-L142">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.QRFactorization"><a class="docstring-binding" href="#LinearSolve.QRFactorization"><code>LinearSolve.QRFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><p><code>QRFactorization(pivot=LinearAlgebra.NoPivot(),blocksize=16)</code></p><p>Julia&#39;s built in <code>qr</code>. Equivalent to calling <code>qr!(A)</code>.</p><ul><li>On dense matrices, this uses the current BLAS implementation of the user&#39;s computer which by default is OpenBLAS but will use MKL if the user does <code>using MKL</code> in their system.</li><li>On sparse matrices, this will use SPQR from SparseArrays</li><li>On CuMatrix, it will use a CUDA-accelerated QR from CuSolver.</li><li>On BandedMatrix and BlockBandedMatrix, it will use a banded QR.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/factorization.jl#L319-L330">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.SVDFactorization"><a class="docstring-binding" href="#LinearSolve.SVDFactorization"><code>LinearSolve.SVDFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">SVDFactorization(full=false, alg=nothing)</code></pre><p>Julia&#39;s built-in <code>svd</code>. Equivalent to <code>svd!(A)</code>.</p><ul><li>On dense matrices, this uses the current BLAS implementation.</li><li>When <code>alg = nothing</code>, the backend default SVD algorithm is used (required for CUDA compatibility).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/factorization.jl#L538-L545">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.CholeskyFactorization"><a class="docstring-binding" href="#LinearSolve.CholeskyFactorization"><code>LinearSolve.CholeskyFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><p><code>CholeskyFactorization(; pivot = nothing, tol = 0.0, shift = 0.0, perm = nothing)</code></p><p>Julia&#39;s built in <code>cholesky</code>. Equivalent to calling <code>cholesky!(A)</code>.</p><p><strong>Keyword Arguments</strong></p><ul><li>pivot: defaluts to NoPivot, can also be RowMaximum.</li><li>tol: the tol argument in CHOLMOD. Only used for sparse matrices.</li><li>shift: the shift argument in CHOLMOD. Only used for sparse matrices.</li><li>perm: the perm argument in CHOLMOD. Only used for sparse matrices.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/factorization.jl#L413-L424">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.BunchKaufmanFactorization"><a class="docstring-binding" href="#LinearSolve.BunchKaufmanFactorization"><code>LinearSolve.BunchKaufmanFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><p><code>BunchKaufmanFactorization(; rook = false)</code></p><p>Julia&#39;s built in <code>bunchkaufman</code>. Equivalent to calling <code>bunchkaufman(A)</code>. Only for Symmetric matrices.</p><p><strong>Keyword Arguments</strong></p><ul><li>rook: whether to perform rook pivoting. Defaults to false.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/factorization.jl#L601-L610">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.CHOLMODFactorization"><a class="docstring-binding" href="#LinearSolve.CHOLMODFactorization"><code>LinearSolve.CHOLMODFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><p><code>CHOLMODFactorization(; shift = 0.0, perm = nothing)</code></p><p>A wrapper of CHOLMOD&#39;s polyalgorithm, mixing Cholesky factorization and ldlt. Tries cholesky for performance and retries ldlt if conditioning causes Cholesky to fail.</p><p>Only supports sparse matrices.</p><p><strong>Keyword Arguments</strong></p><ul><li>shift: the shift argument in CHOLMOD.</li><li>perm: the perm argument in CHOLMOD</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/factorization.jl#L1104-L1117">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.NormalCholeskyFactorization"><a class="docstring-binding" href="#LinearSolve.NormalCholeskyFactorization"><code>LinearSolve.NormalCholeskyFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><p><code>NormalCholeskyFactorization(pivot = RowMaximum())</code></p><p>A fast factorization which uses a Cholesky factorization on A * A&#39;. Can be much faster than LU factorization, but is not as numerically stable and thus should only be applied to well-conditioned matrices.</p><div class="admonition is-warning" id="Warning-ed12fc6a267ffd1f"><header class="admonition-header">Warning<a class="admonition-anchor" href="#Warning-ed12fc6a267ffd1f" title="Permalink"></a></header><div class="admonition-body"><p><code>NormalCholeskyFactorization</code> should only be applied to well-conditioned matrices. As a method it is not able to easily identify possible numerical issues. As a check it is recommended that the user checks <code>A*u-b</code> is approximately zero, as this may be untrue even if <code>sol.retcode === ReturnCode.Success</code> due to numerical stability issues.</p></div></div><p><strong>Positional Arguments</strong></p><ul><li>pivot: Defaults to RowMaximum(), but can be NoPivot()</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/factorization.jl#L1152-L1169">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.NormalBunchKaufmanFactorization"><a class="docstring-binding" href="#LinearSolve.NormalBunchKaufmanFactorization"><code>LinearSolve.NormalBunchKaufmanFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><p><code>NormalBunchKaufmanFactorization(rook = false)</code></p><p>A fast factorization which uses a BunchKaufman factorization on A * A&#39;. Can be much faster than LU factorization, but is not as numerically stable and thus should only be applied to well-conditioned matrices.</p><p><strong>Positional Arguments</strong></p><ul><li>rook: whether to perform rook pivoting. Defaults to false.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/factorization.jl#L1258-L1268">source</a></section></details></article><h3 id="LinearSolve.jl"><a class="docs-heading-anchor" href="#LinearSolve.jl">LinearSolve.jl</a><a id="LinearSolve.jl-1"></a><a class="docs-heading-anchor-permalink" href="#LinearSolve.jl" title="Permalink"></a></h3><p>LinearSolve.jl contains some linear solvers built in for specialized cases.</p><div class="admonition is-warning" id="Missing-docstring.-9d3496675332aec5"><header class="admonition-header">Missing docstring.<a class="admonition-anchor" href="#Missing-docstring.-9d3496675332aec5" title="Permalink"></a></header><div class="admonition-body"><p>Missing docstring for <code>SimpleLUFactorization</code>. Check Documenter&#39;s build log for details.</p></div></div><article><details class="docstring" open="true"><summary id="LinearSolve.DiagonalFactorization"><a class="docstring-binding" href="#LinearSolve.DiagonalFactorization"><code>LinearSolve.DiagonalFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><p><code>DiagonalFactorization()</code></p><p>A special implementation only for solving <code>Diagonal</code> matrices fast.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/factorization.jl#L1305-L1309">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.SimpleGMRES"><a class="docstring-binding" href="#LinearSolve.SimpleGMRES"><code>LinearSolve.SimpleGMRES</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">SimpleGMRES(; restart::Bool = true, blocksize::Int = 0, warm_start::Bool = false,
    memory::Int = 20)</code></pre><p>A simple GMRES implementation for square non-Hermitian linear systems.</p><p>This implementation handles Block Diagonal Matrices with Uniformly Sized Square Blocks with specialized dispatches.</p><p><strong>Arguments</strong></p><ul><li><p><code>restart::Bool</code>: If <code>true</code>, then the solver will restart after <code>memory</code> iterations.</p></li><li><p><code>memory::Int = 20</code>: The number of iterations before restarting. If restart is false, this value is used to allocate memory and later expanded if more memory is required.</p></li><li><p><code>blocksize::Int = 0</code>: If blocksize is <code>&gt; 0</code>, the solver assumes that the matrix has a uniformly sized block diagonal structure with square blocks of size <code>blocksize</code>. Misusing this option will lead to incorrect results.</p><ul><li>If this is set <code>≤ 0</code> and during runtime we get a Block Diagonal Matrix, then we will check if the specialized dispatch can be used.</li></ul></li></ul><div class="admonition is-warning" id="Warning-6be1baf4c2a7dba9"><header class="admonition-header">Warning<a class="admonition-anchor" href="#Warning-6be1baf4c2a7dba9" title="Permalink"></a></header><div class="admonition-body"><p>Most users should be using the <code>KrylovJL_GMRES</code> solver instead of this implementation.</p></div></div><div class="admonition is-success" id="Tip-609b37c84189f9aa"><header class="admonition-header">Tip<a class="admonition-anchor" href="#Tip-609b37c84189f9aa" title="Permalink"></a></header><div class="admonition-body"><p>We can automatically detect if the matrix is a Block Diagonal Matrix with Uniformly Sized Square Blocks. If this is the case, then we can use a specialized dispatch. However, on most modern systems performing a single matrix-vector multiplication is faster than performing multiple smaller matrix-vector multiplications (as in the case of Block Diagonal Matrix). We recommend making the matrix dense (if size permits) and specifying the <code>blocksize</code> argument.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/simplegmres.jl#L1-L35">source</a></section></details></article><div class="admonition is-warning" id="Missing-docstring.-91825a74832d3d87"><header class="admonition-header">Missing docstring.<a class="admonition-anchor" href="#Missing-docstring.-91825a74832d3d87" title="Permalink"></a></header><div class="admonition-body"><p>Missing docstring for <code>DirectLdiv!</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning" id="Missing-docstring.-4942f6526b47bfc"><header class="admonition-header">Missing docstring.<a class="admonition-anchor" href="#Missing-docstring.-4942f6526b47bfc" title="Permalink"></a></header><div class="admonition-body"><p>Missing docstring for <code>LinearSolveFunction</code>. Check Documenter&#39;s build log for details.</p></div></div><h3 id="FastLapackInterface.jl"><a class="docs-heading-anchor" href="#FastLapackInterface.jl">FastLapackInterface.jl</a><a id="FastLapackInterface.jl-1"></a><a class="docs-heading-anchor-permalink" href="#FastLapackInterface.jl" title="Permalink"></a></h3><p>FastLapackInterface.jl is a package that allows for a lower-level interface to the LAPACK calls to allow for preallocating workspaces to decrease the overhead of the wrappers. LinearSolve.jl provides a wrapper to these routines in a way where an initialized solver has a non-allocating LU factorization. In theory, this post-initialized solve should always be faster than the Base.LinearAlgebra version. In practice, with the way we wrap the solvers, we do not see a performance benefit and in fact benchmarks tend to show this inhibits performance.</p><div class="admonition is-info" id="Note-5d56ad87c74a3ae7"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-5d56ad87c74a3ae7" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires adding the package FastLapackInterface.jl, i.e. <code>using FastLapackInterface</code></p></div></div><article><details class="docstring" open="true"><summary id="LinearSolve.FastLUFactorization"><a class="docstring-binding" href="#LinearSolve.FastLUFactorization"><code>LinearSolve.FastLUFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">FastLUFactorization()</code></pre><p>A high-performance LU factorization using the FastLapackInterface.jl package. This provides an optimized interface to LAPACK routines with reduced overhead compared to the standard LinearAlgebra LAPACK wrappers.</p><p><strong>Features</strong></p><ul><li>Reduced function call overhead compared to standard LAPACK wrappers</li><li>Optimized for performance-critical applications</li><li>Uses partial pivoting (no choice of pivoting method available)</li><li>Suitable for dense matrices where maximum performance is required</li></ul><p><strong>Limitations</strong></p><ul><li>Does not allow customization of pivoting strategy (always uses partial pivoting)</li><li>Requires FastLapackInterface.jl to be loaded</li><li>Limited to dense matrix types supported by LAPACK</li></ul><p><strong>Requirements</strong></p><p>Using this solver requires that FastLapackInterface.jl is loaded: <code>using FastLapackInterface</code></p><p><strong>Performance Notes</strong></p><p>This factorization is optimized for cases where the overhead of standard LAPACK function calls becomes significant, typically for moderate-sized dense matrices or when performing many factorizations.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">using FastLapackInterface
alg = FastLUFactorization()
sol = solve(prob, alg)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L340-L372">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.FastQRFactorization"><a class="docstring-binding" href="#LinearSolve.FastQRFactorization"><code>LinearSolve.FastQRFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">FastQRFactorization{P}(; pivot = ColumnNorm(), blocksize = 36)</code></pre><p>A high-performance QR factorization using the FastLapackInterface.jl package. This provides an optimized interface to LAPACK QR routines with reduced overhead compared to the standard LinearAlgebra LAPACK wrappers.</p><p><strong>Type Parameters</strong></p><ul><li><code>P</code>: The type of pivoting strategy used</li></ul><p><strong>Fields</strong></p><ul><li><code>pivot::P</code>: Pivoting strategy (e.g., <code>ColumnNorm()</code> for column pivoting, <code>nothing</code> for no pivoting)</li><li><code>blocksize::Int</code>: Block size for the blocked QR algorithm (default: 36)</li></ul><p><strong>Features</strong></p><ul><li>Reduced function call overhead compared to standard LAPACK wrappers</li><li>Supports various pivoting strategies for numerical stability</li><li>Configurable block size for optimal performance</li><li>Suitable for dense matrices, especially overdetermined systems</li></ul><p><strong>Performance Notes</strong></p><p>The block size can be tuned for optimal performance depending on matrix size and architecture. The default value of 36 is generally good for most cases, but experimentation may be beneficial for specific applications.</p><p><strong>Requirements</strong></p><p>Using this solver requires that FastLapackInterface.jl is loaded: <code>using FastLapackInterface</code></p><p><strong>Example</strong></p><pre><code class="language-julia hljs">using FastLapackInterface
# QR with column pivoting
alg1 = FastQRFactorization()  
# QR without pivoting for speed
alg2 = FastQRFactorization(pivot=nothing)
# Custom block size
alg3 = FastQRFactorization(blocksize=64)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L375-L413">source</a></section></details></article><h3 id="SuiteSparse.jl"><a class="docs-heading-anchor" href="#SuiteSparse.jl">SuiteSparse.jl</a><a id="SuiteSparse.jl-1"></a><a class="docs-heading-anchor-permalink" href="#SuiteSparse.jl" title="Permalink"></a></h3><div class="admonition is-info" id="Note-f187f71f926060f1"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-f187f71f926060f1" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires adding the package SparseArrays.jl, i.e. <code>using SparseArrays</code></p></div></div><article><details class="docstring" open="true"><summary id="LinearSolve.KLUFactorization"><a class="docstring-binding" href="#LinearSolve.KLUFactorization"><code>LinearSolve.KLUFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><p><code>KLUFactorization(;reuse_symbolic=true, check_pattern=true)</code></p><p>A fast sparse LU-factorization which specializes on sparsity patterns with “less structure”.</p><div class="admonition is-info" id="Note-b4c903aa3513a6c8"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-b4c903aa3513a6c8" title="Permalink"></a></header><div class="admonition-body"><p>By default, the SparseArrays.jl are implemented for efficiency by caching the symbolic factorization. If the sparsity pattern of <code>A</code> may change between solves, set <code>reuse_symbolic=false</code>. If the pattern is assumed or known to be constant, set <code>reuse_symbolic=true</code> to avoid unnecessary recomputation. To further reduce computational overhead, you can disable pattern checks entirely by setting <code>check_pattern = false</code>. Note that this may error if the sparsity pattern does change unexpectedly.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/factorization.jl#L1074-L1087">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.UMFPACKFactorization"><a class="docstring-binding" href="#LinearSolve.UMFPACKFactorization"><code>LinearSolve.UMFPACKFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><p><code>UMFPACKFactorization(;reuse_symbolic=true, check_pattern=true)</code></p><p>A fast sparse multithreaded LU-factorization which specializes on sparsity patterns with “more structure”.</p><div class="admonition is-info" id="Note-b4c903aa3513a6c8"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-b4c903aa3513a6c8" title="Permalink"></a></header><div class="admonition-body"><p>By default, the SparseArrays.jl are implemented for efficiency by caching the symbolic factorization. If the sparsity pattern of <code>A</code> may change between solves, set <code>reuse_symbolic=false</code>. If the pattern is assumed or known to be constant, set <code>reuse_symbolic=true</code> to avoid unnecessary recomputation. To further reduce computational overhead, you can disable pattern checks entirely by setting <code>check_pattern = false</code>. Note that this may error if the sparsity pattern does change unexpectedly.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/factorization.jl#L1045-L1059">source</a></section></details></article><h3 id="Sparspak.jl"><a class="docs-heading-anchor" href="#Sparspak.jl">Sparspak.jl</a><a id="Sparspak.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Sparspak.jl" title="Permalink"></a></h3><div class="admonition is-info" id="Note-17e93ef6091518f"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-17e93ef6091518f" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires adding the package Sparspak.jl, i.e. <code>using Sparspak</code></p></div></div><article><details class="docstring" open="true"><summary id="LinearSolve.SparspakFactorization"><a class="docstring-binding" href="#LinearSolve.SparspakFactorization"><code>LinearSolve.SparspakFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><p><code>SparspakFactorization(reuse_symbolic = true)</code></p><p>This is the translation of the well-known sparse matrix software Sparspak (Waterloo Sparse Matrix Package), solving large sparse systems of linear algebraic equations. Sparspak is composed of the subroutines from the book &quot;Computer Solution of Large Sparse Positive Definite Systems&quot; by Alan George and Joseph Liu. Originally written in Fortran 77, later rewritten in Fortran 90. Here is the software translated into Julia.</p><p>The Julia rewrite is released  under the MIT license with an express permission from the authors of the Fortran package. The package uses multiple dispatch to route around standard BLAS routines in the case e.g. of arbitrary-precision floating point numbers or ForwardDiff.Dual. This e.g. allows for Automatic Differentiation (AD) of a sparse-matrix solve.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/factorization.jl#L1336-L1351">source</a></section></details></article><h3 id="CliqueTrees.jl"><a class="docs-heading-anchor" href="#CliqueTrees.jl">CliqueTrees.jl</a><a id="CliqueTrees.jl-1"></a><a class="docs-heading-anchor-permalink" href="#CliqueTrees.jl" title="Permalink"></a></h3><div class="admonition is-info" id="Note-fcd8a3e05a66c9fd"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-fcd8a3e05a66c9fd" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires adding the package CliqueTrees.jl, i.e. <code>using CliqueTrees</code></p></div></div><article><details class="docstring" open="true"><summary id="LinearSolve.CliqueTreesFactorization"><a class="docstring-binding" href="#LinearSolve.CliqueTreesFactorization"><code>LinearSolve.CliqueTreesFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">CliqueTreesFactorization(
    alg = nothing,
    snd = nothing,
    reuse_symbolic = true,
)</code></pre><p>The sparse Cholesky factorization algorithm implemented in CliqueTrees.jl. The implementation is pure-Julia and accepts arbitrary numeric types. It is somewhat slower than CHOLMOD.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/factorization.jl#L1383-L1393">source</a></section></details></article><h3 id="Krylov.jl"><a class="docs-heading-anchor" href="#Krylov.jl">Krylov.jl</a><a id="Krylov.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Krylov.jl" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearSolve.KrylovJL_CG"><a class="docstring-binding" href="#LinearSolve.KrylovJL_CG"><code>LinearSolve.KrylovJL_CG</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">KrylovJL_CG(args...; kwargs...)</code></pre><p>A generic CG implementation for Hermitian and positive definite linear systems</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/iterative_wrappers.jl#L37-L43">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.KrylovJL_MINRES"><a class="docstring-binding" href="#LinearSolve.KrylovJL_MINRES"><code>LinearSolve.KrylovJL_MINRES</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">KrylovJL_MINRES(args...; kwargs...)</code></pre><p>A generic MINRES implementation for Hermitian linear systems</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/iterative_wrappers.jl#L48-L54">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.KrylovJL_GMRES"><a class="docstring-binding" href="#LinearSolve.KrylovJL_GMRES"><code>LinearSolve.KrylovJL_GMRES</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">KrylovJL_GMRES(args...; gmres_restart = 0, window = 0, kwargs...)</code></pre><p>A generic GMRES implementation for square non-Hermitian linear systems</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/iterative_wrappers.jl#L59-L65">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.KrylovJL_BICGSTAB"><a class="docstring-binding" href="#LinearSolve.KrylovJL_BICGSTAB"><code>LinearSolve.KrylovJL_BICGSTAB</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">KrylovJL_BICGSTAB(args...; kwargs...)</code></pre><p>A generic BICGSTAB implementation for square non-Hermitian linear systems</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/iterative_wrappers.jl#L81-L87">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.KrylovJL_LSMR"><a class="docstring-binding" href="#LinearSolve.KrylovJL_LSMR"><code>LinearSolve.KrylovJL_LSMR</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">KrylovJL_LSMR(args...; kwargs...)</code></pre><p>A generic LSMR implementation for least-squares problems</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/iterative_wrappers.jl#L92-L98">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.KrylovJL_CRAIGMR"><a class="docstring-binding" href="#LinearSolve.KrylovJL_CRAIGMR"><code>LinearSolve.KrylovJL_CRAIGMR</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">KrylovJL_CRAIGMR(args...; kwargs...)</code></pre><p>A generic CRAIGMR implementation for least-norm problems</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/iterative_wrappers.jl#L103-L109">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.KrylovJL"><a class="docstring-binding" href="#LinearSolve.KrylovJL"><code>LinearSolve.KrylovJL</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">KrylovJL(args...; KrylovAlg = Krylov.gmres!,
    Pl = nothing, Pr = nothing,
    gmres_restart = 0, window = 0,
    kwargs...)</code></pre><p>A generic wrapper over the Krylov.jl krylov-subspace iterative solvers.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/iterative_wrappers.jl#L3-L12">source</a></section></details></article><h3 id="MKL.jl"><a class="docs-heading-anchor" href="#MKL.jl">MKL.jl</a><a id="MKL.jl-1"></a><a class="docs-heading-anchor-permalink" href="#MKL.jl" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearSolve.MKLLUFactorization"><a class="docstring-binding" href="#LinearSolve.MKLLUFactorization"><code>LinearSolve.MKLLUFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">MKLLUFactorization()</code></pre><p>A wrapper over Intel&#39;s Math Kernel Library (MKL). Direct calls to MKL in a way that pre-allocates workspace to avoid allocations and does not require libblastrampoline.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/mkl.jl#L1-L8">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.MKL32MixedLUFactorization"><a class="docstring-binding" href="#LinearSolve.MKL32MixedLUFactorization"><code>LinearSolve.MKL32MixedLUFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">MKL32MixedLUFactorization()</code></pre><p>A mixed precision LU factorization using Intel MKL that performs factorization in Float32 precision while maintaining Float64 interface. This can provide significant speedups for large matrices when reduced precision is acceptable.</p><p><strong>Performance Notes</strong></p><ul><li>Converts Float64 matrices to Float32 for factorization</li><li>Uses optimized MKL routines for the factorization</li><li>Can be 2x faster than full precision for memory-bandwidth limited problems</li><li>May have reduced accuracy compared to full Float64 precision</li></ul><p><strong>Requirements</strong></p><p>This solver requires MKL to be available through MKL_jll.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">alg = MKL32MixedLUFactorization()
sol = solve(prob, alg)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L874-L895">source</a></section></details></article><h3 id="OpenBLAS"><a class="docs-heading-anchor" href="#OpenBLAS">OpenBLAS</a><a id="OpenBLAS-1"></a><a class="docs-heading-anchor-permalink" href="#OpenBLAS" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearSolve.OpenBLASLUFactorization"><a class="docstring-binding" href="#LinearSolve.OpenBLASLUFactorization"><code>LinearSolve.OpenBLASLUFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">OpenBLASLUFactorization()</code></pre><p>A direct wrapper over OpenBLAS&#39;s LU factorization (<code>getrf!</code> and <code>getrs!</code>). This solver makes direct calls to OpenBLAS_jll without going through Julia&#39;s libblastrampoline, which can provide performance benefits in certain configurations.</p><p><strong>Performance Characteristics</strong></p><ul><li>Pre-allocates workspace to avoid allocations during solving</li><li>Makes direct <code>ccall</code>s to OpenBLAS routines</li><li>Can be faster than <code>LUFactorization</code> when OpenBLAS is well-optimized for the hardware</li><li>Supports <code>Float32</code>, <code>Float64</code>, <code>ComplexF32</code>, and <code>ComplexF64</code> element types</li></ul><p><strong>When to Use</strong></p><ul><li>When you want to ensure OpenBLAS is used regardless of the system BLAS configuration</li><li>When benchmarking shows better performance than <code>LUFactorization</code> on your specific hardware</li><li>When you need consistent behavior across different systems (always uses OpenBLAS)</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">using LinearSolve, LinearAlgebra

A = rand(100, 100)
b = rand(100)
prob = LinearProblem(A, b)
sol = solve(prob, OpenBLASLUFactorization())</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/openblas.jl#L1-L33">source</a></section></details></article><h3 id="AppleAccelerate.jl"><a class="docs-heading-anchor" href="#AppleAccelerate.jl">AppleAccelerate.jl</a><a id="AppleAccelerate.jl-1"></a><a class="docs-heading-anchor-permalink" href="#AppleAccelerate.jl" title="Permalink"></a></h3><div class="admonition is-info" id="Note-63da764332d397b7"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-63da764332d397b7" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires a Mac with Apple Accelerate. This should come standard in most &quot;modern&quot; Mac computers.</p></div></div><article><details class="docstring" open="true"><summary id="LinearSolve.AppleAccelerateLUFactorization"><a class="docstring-binding" href="#LinearSolve.AppleAccelerateLUFactorization"><code>LinearSolve.AppleAccelerateLUFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">AppleAccelerateLUFactorization()</code></pre><p>A wrapper over Apple&#39;s Accelerate Library. Direct calls to Acceelrate in a way that pre-allocates workspace to avoid allocations and does not require libblastrampoline.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/appleaccelerate.jl#L6-L13">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.AppleAccelerate32MixedLUFactorization"><a class="docstring-binding" href="#LinearSolve.AppleAccelerate32MixedLUFactorization"><code>LinearSolve.AppleAccelerate32MixedLUFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">AppleAccelerate32MixedLUFactorization()</code></pre><p>A mixed precision LU factorization using Apple&#39;s Accelerate framework that performs factorization in Float32 precision while maintaining Float64 interface. This can provide significant speedups on Apple hardware when reduced precision is acceptable.</p><p><strong>Performance Notes</strong></p><ul><li>Converts Float64 matrices to Float32 for factorization</li><li>Uses optimized Accelerate routines for the factorization</li><li>Particularly effective on Apple Silicon with unified memory</li><li>May have reduced accuracy compared to full Float64 precision</li></ul><p><strong>Requirements</strong></p><p>This solver is only available on Apple platforms and requires the Accelerate framework.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">alg = AppleAccelerate32MixedLUFactorization()
sol = solve(prob, alg)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L898-L919">source</a></section></details></article><h3 id="Metal.jl"><a class="docs-heading-anchor" href="#Metal.jl">Metal.jl</a><a id="Metal.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Metal.jl" title="Permalink"></a></h3><div class="admonition is-info" id="Note-2526338b45efe6cd"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-2526338b45efe6cd" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires adding the package Metal.jl, i.e. <code>using Metal</code>. This package is only compatible with Mac M-Series computers with a Metal-compatible GPU.</p></div></div><article><details class="docstring" open="true"><summary id="LinearSolve.MetalLUFactorization"><a class="docstring-binding" href="#LinearSolve.MetalLUFactorization"><code>LinearSolve.MetalLUFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">MetalLUFactorization()</code></pre><p>A wrapper over Apple&#39;s Metal GPU library for LU factorization. Direct calls to Metal  in a way that pre-allocates workspace to avoid allocations and automatically offloads  to the GPU. This solver is optimized for Metal-capable Apple Silicon Macs.</p><p><strong>Requirements</strong></p><p>Using this solver requires that Metal.jl is loaded: <code>using Metal</code></p><p><strong>Performance Notes</strong></p><ul><li>Most efficient for large dense matrices where GPU acceleration benefits outweigh transfer costs</li><li>Automatically manages GPU memory and transfers</li><li>Particularly effective on Apple Silicon Macs with unified memory</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">using Metal
alg = MetalLUFactorization()
sol = solve(prob, alg)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L725-L746">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.MetalOffload32MixedLUFactorization"><a class="docstring-binding" href="#LinearSolve.MetalOffload32MixedLUFactorization"><code>LinearSolve.MetalOffload32MixedLUFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">MetalOffload32MixedLUFactorization()</code></pre><p>A mixed precision Metal GPU-accelerated LU factorization that converts matrices to Float32 before offloading to Metal GPU for factorization, then converts back for the solve. This can provide speedups on Apple Silicon when reduced precision is acceptable.</p><p><strong>Performance Notes</strong></p><ul><li>Converts Float64 matrices to Float32 for GPU factorization</li><li>Can be significantly faster for large matrices where memory bandwidth is limiting</li><li>Particularly effective on Apple Silicon Macs with unified memory architecture</li><li>May have reduced accuracy compared to full precision methods</li></ul><p><strong>Requirements</strong></p><p>Using this solver requires that Metal.jl is loaded: <code>using Metal</code></p><p><strong>Example</strong></p><pre><code class="language-julia hljs">using Metal
alg = MetalOffload32MixedLUFactorization()
sol = solve(prob, alg)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L766-L788">source</a></section></details></article><h3 id="Pardiso.jl"><a class="docs-heading-anchor" href="#Pardiso.jl">Pardiso.jl</a><a id="Pardiso.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Pardiso.jl" title="Permalink"></a></h3><div class="admonition is-info" id="Note-d4b0a7f9ace7f0bf"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-d4b0a7f9ace7f0bf" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires adding the package Pardiso.jl, i.e. <code>using Pardiso</code></p></div></div><article><details class="docstring" open="true"><summary id="LinearSolve.MKLPardisoFactorize"><a class="docstring-binding" href="#LinearSolve.MKLPardisoFactorize"><code>LinearSolve.MKLPardisoFactorize</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">MKLPardisoFactorize(; nprocs::Union{Int, Nothing} = nothing,
    matrix_type = nothing,
    cache_analysis = false,
    iparm::Union{Vector{Tuple{Int, Int}}, Nothing} = nothing,
    dparm::Union{Vector{Tuple{Int, Int}}, Nothing} = nothing)</code></pre><p>A sparse factorization method using MKL Pardiso.</p><div class="admonition is-info" id="Note-d4b0a7f9ace7f0bf"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-d4b0a7f9ace7f0bf" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires adding the package Pardiso.jl, i.e. <code>using Pardiso</code></p></div></div><p><strong>Keyword Arguments</strong></p><p>Setting <code>cache_analysis = true</code> disables Pardiso&#39;s scaling and matching defaults and caches the result of the initial analysis phase for all further computations with this solver.</p><p>For the definition of the other keyword arguments, see the Pardiso.jl documentation. All values default to <code>nothing</code> and the solver internally determines the values given the input types, and these keyword arguments are only for overriding the default handling process. This should not be required by most users.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L423-L448">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.MKLPardisoIterate"><a class="docstring-binding" href="#LinearSolve.MKLPardisoIterate"><code>LinearSolve.MKLPardisoIterate</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">MKLPardisoIterate(; nprocs::Union{Int, Nothing} = nothing,
    matrix_type = nothing,
    cache_analysis = false,
    iparm::Union{Vector{Tuple{Int, Int}}, Nothing} = nothing,
    dparm::Union{Vector{Tuple{Int, Int}}, Nothing} = nothing)</code></pre><p>A mixed factorization+iterative method using MKL Pardiso.</p><div class="admonition is-info" id="Note-d4b0a7f9ace7f0bf"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-d4b0a7f9ace7f0bf" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires adding the package Pardiso.jl, i.e. <code>using Pardiso</code></p></div></div><p><strong>Keyword Arguments</strong></p><p>Setting <code>cache_analysis = true</code> disables Pardiso&#39;s scaling and matching defaults and caches the result of the initial analysis phase for all further computations with this solver.</p><p>For the definition of the other keyword arguments, see the Pardiso.jl documentation. All values default to <code>nothing</code> and the solver internally determines the values given the input types, and these keyword arguments are only for overriding the default handling process. This should not be required by most users.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L451-L476">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.PardisoJL"><a class="docstring-binding" href="#LinearSolve.PardisoJL"><code>LinearSolve.PardisoJL</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">PardisoJL(; nprocs::Union{Int, Nothing} = nothing,
    solver_type = nothing,
    matrix_type = nothing,
    iparm::Union{Vector{Tuple{Int, Int}}, Nothing} = nothing,
    dparm::Union{Vector{Tuple{Int, Int}}, Nothing} = nothing,
    vendor::Union{Symbol, Nothing} = nothing
)</code></pre><p>A generic method using  Pardiso. Specifying <code>solver_type</code> is required.</p><div class="admonition is-info" id="Note-d4b0a7f9ace7f0bf"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-d4b0a7f9ace7f0bf" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires adding the package Pardiso.jl, i.e. <code>using Pardiso</code></p></div></div><p><strong>Keyword Arguments</strong></p><p>The <code>vendor</code> keyword allows to choose between Panua pardiso  (former pardiso-project.org; <code>vendor=:Panua</code>) and  MKL Pardiso (<code>vendor=:MKL</code>). If <code>vendor==nothing</code>, Panua pardiso is preferred over MKL Pardiso.</p><p>For the definition of the other keyword arguments, see the Pardiso.jl documentation. All values default to <code>nothing</code> and the solver internally determines the values given the input types, and these keyword arguments are only for overriding the default handling process. This should not be required by most users.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L532-L558">source</a></section></details></article><h3 id="CUDA.jl"><a class="docs-heading-anchor" href="#CUDA.jl">CUDA.jl</a><a id="CUDA.jl-1"></a><a class="docs-heading-anchor-permalink" href="#CUDA.jl" title="Permalink"></a></h3><p>Note that <code>CuArrays</code> are supported by <code>GenericFactorization</code> in the &quot;normal&quot; way. The following are non-standard GPU factorization routines.</p><div class="admonition is-info" id="Note-1c388d79acb06da2"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-1c388d79acb06da2" title="Permalink"></a></header><div class="admonition-body"><p>Using these solvers requires adding the package CUDA.jl, i.e. <code>using CUDA</code></p></div></div><article><details class="docstring" open="true"><summary id="LinearSolve.CudaOffloadLUFactorization"><a class="docstring-binding" href="#LinearSolve.CudaOffloadLUFactorization"><code>LinearSolve.CudaOffloadLUFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><p><code>CudaOffloadLUFactorization()</code></p><p>An offloading technique used to GPU-accelerate CPU-based computations using LU factorization. Requires a sufficiently large <code>A</code> to overcome the data transfer costs.</p><div class="admonition is-info" id="Note-5d94e5a504a8d3bf"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-5d94e5a504a8d3bf" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires adding the package CUDA.jl, i.e. <code>using CUDA</code></p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L122-L131">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.CudaOffloadQRFactorization"><a class="docstring-binding" href="#LinearSolve.CudaOffloadQRFactorization"><code>LinearSolve.CudaOffloadQRFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><p><code>CudaOffloadQRFactorization()</code></p><p>An offloading technique used to GPU-accelerate CPU-based computations using QR factorization. Requires a sufficiently large <code>A</code> to overcome the data transfer costs.</p><div class="admonition is-info" id="Note-5d94e5a504a8d3bf"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-5d94e5a504a8d3bf" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires adding the package CUDA.jl, i.e. <code>using CUDA</code></p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L172-L181">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.CUDAOffload32MixedLUFactorization"><a class="docstring-binding" href="#LinearSolve.CUDAOffload32MixedLUFactorization"><code>LinearSolve.CUDAOffload32MixedLUFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><p><code>CUDAOffload32MixedLUFactorization()</code></p><p>A mixed precision GPU-accelerated LU factorization that converts matrices to Float32  before offloading to CUDA GPU for factorization, then converts back for the solve. This can provide speedups when the reduced precision is acceptable and memory  bandwidth is a bottleneck.</p><p><strong>Performance Notes</strong></p><ul><li>Converts Float64 matrices to Float32 for GPU factorization</li><li>Can be significantly faster for large matrices where memory bandwidth is limiting</li><li>May have reduced accuracy compared to full precision methods</li><li>Most beneficial when the condition number of the matrix is moderate</li></ul><div class="admonition is-info" id="Note-5d94e5a504a8d3bf"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-5d94e5a504a8d3bf" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires adding the package CUDA.jl, i.e. <code>using CUDA</code></p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L143-L160">source</a></section></details></article><h3 id="AMDGPU.jl"><a class="docs-heading-anchor" href="#AMDGPU.jl">AMDGPU.jl</a><a id="AMDGPU.jl-1"></a><a class="docs-heading-anchor-permalink" href="#AMDGPU.jl" title="Permalink"></a></h3><p>The following are GPU factorization routines for AMD GPUs using the ROCm stack.</p><div class="admonition is-info" id="Note-8ac91531a4850a65"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-8ac91531a4850a65" title="Permalink"></a></header><div class="admonition-body"><p>Using these solvers requires adding the package AMDGPU.jl, i.e. <code>using AMDGPU</code></p></div></div><article><details class="docstring" open="true"><summary id="LinearSolve.AMDGPUOffloadLUFactorization"><a class="docstring-binding" href="#LinearSolve.AMDGPUOffloadLUFactorization"><code>LinearSolve.AMDGPUOffloadLUFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><p><code>AMDGPUOffloadLUFactorization()</code></p><p>An offloading technique using LU factorization to GPU-accelerate CPU-based computations on AMD GPUs. Requires a sufficiently large <code>A</code> to overcome the data transfer costs.</p><div class="admonition is-info" id="Note-4bc6d392db337e5"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-4bc6d392db337e5" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires adding the package AMDGPU.jl, i.e. <code>using AMDGPU</code></p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L218-L227">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.AMDGPUOffloadQRFactorization"><a class="docstring-binding" href="#LinearSolve.AMDGPUOffloadQRFactorization"><code>LinearSolve.AMDGPUOffloadQRFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><p><code>AMDGPUOffloadQRFactorization()</code></p><p>An offloading technique using QR factorization to GPU-accelerate CPU-based computations on AMD GPUs. Requires a sufficiently large <code>A</code> to overcome the data transfer costs.</p><div class="admonition is-info" id="Note-4bc6d392db337e5"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-4bc6d392db337e5" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires adding the package AMDGPU.jl, i.e. <code>using AMDGPU</code></p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L239-L248">source</a></section></details></article><h3 id="CUSOLVERRF.jl"><a class="docs-heading-anchor" href="#CUSOLVERRF.jl">CUSOLVERRF.jl</a><a id="CUSOLVERRF.jl-1"></a><a class="docs-heading-anchor-permalink" href="#CUSOLVERRF.jl" title="Permalink"></a></h3><div class="admonition is-info" id="Note-e131d0e2ac80eca0"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-e131d0e2ac80eca0" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires adding the package CUSOLVERRF.jl, i.e. <code>using CUSOLVERRF</code></p></div></div><article><details class="docstring" open="true"><summary id="LinearSolve.CUSOLVERRFFactorization"><a class="docstring-binding" href="#LinearSolve.CUSOLVERRFFactorization"><code>LinearSolve.CUSOLVERRFFactorization</code></a> — <span class="docstring-category">Type</span></summary><section><div><p><code>CUSOLVERRFFactorization(; symbolic = :RF, reuse_symbolic = true)</code></p><p>A GPU-accelerated sparse LU factorization using NVIDIA&#39;s cusolverRF library. This solver is specifically designed for sparse matrices on CUDA GPUs and  provides high-performance factorization and solve capabilities.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>symbolic</code>: The symbolic factorization method to use. Options are:<ul><li><code>:RF</code> (default): Use cusolverRF&#39;s built-in symbolic analysis</li><li><code>:KLU</code>: Use KLU for symbolic analysis</li></ul></li><li><code>reuse_symbolic</code>: Whether to reuse the symbolic factorization when the  sparsity pattern doesn&#39;t change (default: <code>true</code>)</li></ul><div class="admonition is-info" id="Note-89d21f0dcb535a2"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-89d21f0dcb535a2" title="Permalink"></a></header><div class="admonition-body"><p>This solver requires CUSOLVERRF.jl to be loaded and only supports  <code>Float64</code> element types with <code>Int32</code> indices.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L841-L859">source</a></section></details></article><h3 id="IterativeSolvers.jl"><a class="docs-heading-anchor" href="#IterativeSolvers.jl">IterativeSolvers.jl</a><a id="IterativeSolvers.jl-1"></a><a class="docs-heading-anchor-permalink" href="#IterativeSolvers.jl" title="Permalink"></a></h3><div class="admonition is-info" id="Note-e5f742a72ca9ed78"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-e5f742a72ca9ed78" title="Permalink"></a></header><div class="admonition-body"><p>Using these solvers requires adding the package IterativeSolvers.jl, i.e. <code>using IterativeSolvers</code></p></div></div><article><details class="docstring" open="true"><summary id="LinearSolve.IterativeSolversJL_CG"><a class="docstring-binding" href="#LinearSolve.IterativeSolversJL_CG"><code>LinearSolve.IterativeSolversJL_CG</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">IterativeSolversJL_CG(args...; Pl = nothing, Pr = nothing, kwargs...)</code></pre><p>A wrapper over the IterativeSolvers.jl CG.</p><div class="admonition is-info" id="Note-da5e84f9ee430d95"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-da5e84f9ee430d95" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires adding the package IterativeSolvers.jl, i.e. <code>using IterativeSolvers</code></p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L660-L670">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.IterativeSolversJL_GMRES"><a class="docstring-binding" href="#LinearSolve.IterativeSolversJL_GMRES"><code>LinearSolve.IterativeSolversJL_GMRES</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">IterativeSolversJL_GMRES(args...; Pl = nothing, Pr = nothing, gmres_restart = 0, kwargs...)</code></pre><p>A wrapper over the IterativeSolvers.jl GMRES.</p><div class="admonition is-info" id="Note-da5e84f9ee430d95"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-da5e84f9ee430d95" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires adding the package IterativeSolvers.jl, i.e. <code>using IterativeSolvers</code></p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L673-L683">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.IterativeSolversJL_BICGSTAB"><a class="docstring-binding" href="#LinearSolve.IterativeSolversJL_BICGSTAB"><code>LinearSolve.IterativeSolversJL_BICGSTAB</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">IterativeSolversJL_BICGSTAB(args...; Pl = nothing, Pr = nothing, kwargs...)</code></pre><p>A wrapper over the IterativeSolvers.jl BICGSTAB.</p><div class="admonition is-info" id="Note-da5e84f9ee430d95"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-da5e84f9ee430d95" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires adding the package IterativeSolvers.jl, i.e. <code>using IterativeSolvers</code></p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L699-L709">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.IterativeSolversJL_MINRES"><a class="docstring-binding" href="#LinearSolve.IterativeSolversJL_MINRES"><code>LinearSolve.IterativeSolversJL_MINRES</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">IterativeSolversJL_MINRES(args...; Pl = nothing, Pr = nothing, kwargs...)</code></pre><p>A wrapper over the IterativeSolvers.jl MINRES.</p><div class="admonition is-info" id="Note-da5e84f9ee430d95"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-da5e84f9ee430d95" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires adding the package IterativeSolvers.jl, i.e. <code>using IterativeSolvers</code></p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L712-L722">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.IterativeSolversJL_IDRS"><a class="docstring-binding" href="#LinearSolve.IterativeSolversJL_IDRS"><code>LinearSolve.IterativeSolversJL_IDRS</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">IterativeSolversJL_IDRS(args...; Pl = nothing, kwargs...)</code></pre><p>A wrapper over the IterativeSolvers.jl IDR(S).</p><div class="admonition is-info" id="Note-da5e84f9ee430d95"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-da5e84f9ee430d95" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires adding the package IterativeSolvers.jl, i.e. <code>using IterativeSolvers</code></p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L686-L696">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.IterativeSolversJL"><a class="docstring-binding" href="#LinearSolve.IterativeSolversJL"><code>LinearSolve.IterativeSolversJL</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">IterativeSolversJL(args...;
    generate_iterator = IterativeSolvers.gmres_iterable!,
    Pl = nothing, Pr = nothing,
    gmres_restart = 0, kwargs...)</code></pre><p>A generic wrapper over the IterativeSolvers.jl solvers.</p><div class="admonition is-info" id="Note-da5e84f9ee430d95"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-da5e84f9ee430d95" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires adding the package IterativeSolvers.jl, i.e. <code>using IterativeSolvers</code></p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L638-L651">source</a></section></details></article><h3 id="KrylovKit.jl"><a class="docs-heading-anchor" href="#KrylovKit.jl">KrylovKit.jl</a><a id="KrylovKit.jl-1"></a><a class="docs-heading-anchor-permalink" href="#KrylovKit.jl" title="Permalink"></a></h3><div class="admonition is-info" id="Note-81181e480afe3784"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-81181e480afe3784" title="Permalink"></a></header><div class="admonition-body"><p>Using these solvers requires adding the package KrylovKit.jl, i.e. <code>using KrylovKit</code></p></div></div><article><details class="docstring" open="true"><summary id="LinearSolve.KrylovKitJL_CG"><a class="docstring-binding" href="#LinearSolve.KrylovKitJL_CG"><code>LinearSolve.KrylovKitJL_CG</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">KrylovKitJL_CG(args...; Pl = nothing, Pr = nothing, kwargs...)</code></pre><p>A generic CG implementation for Hermitian and positive definite linear systems</p><div class="admonition is-info" id="Note-f9224b1652e8e3cd"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-f9224b1652e8e3cd" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires adding the package KrylovKit.jl, i.e. <code>using KrylovKit</code></p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L612-L622">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.KrylovKitJL_GMRES"><a class="docstring-binding" href="#LinearSolve.KrylovKitJL_GMRES"><code>LinearSolve.KrylovKitJL_GMRES</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">KrylovKitJL_GMRES(args...; Pl = nothing, Pr = nothing, gmres_restart = 0, kwargs...)</code></pre><p>A generic GMRES implementation.</p><div class="admonition is-info" id="Note-f9224b1652e8e3cd"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-f9224b1652e8e3cd" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires adding the package KrylovKit.jl, i.e. <code>using KrylovKit</code></p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L625-L635">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearSolve.KrylovKitJL"><a class="docstring-binding" href="#LinearSolve.KrylovKitJL"><code>LinearSolve.KrylovKitJL</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">KrylovKitJL(args...; KrylovAlg = Krylov.gmres!, kwargs...)</code></pre><p>A generic iterative solver implementation allowing the choice of KrylovKit.jl solvers.</p><div class="admonition is-info" id="Note-f9224b1652e8e3cd"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-f9224b1652e8e3cd" title="Permalink"></a></header><div class="admonition-body"><p>Using this solver requires adding the package KrylovKit.jl, i.e. <code>using KrylovKit</code></p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L592-L603">source</a></section></details></article><h3 id="HYPRE.jl"><a class="docs-heading-anchor" href="#HYPRE.jl">HYPRE.jl</a><a id="HYPRE.jl-1"></a><a class="docs-heading-anchor-permalink" href="#HYPRE.jl" title="Permalink"></a></h3><div class="admonition is-info" id="Note-94127fba32559141"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-94127fba32559141" title="Permalink"></a></header><div class="admonition-body"><p>Using HYPRE solvers requires Julia version 1.9 or higher, and that the package HYPRE.jl is installed.</p></div></div><article><details class="docstring" open="true"><summary id="LinearSolve.HYPREAlgorithm"><a class="docstring-binding" href="#LinearSolve.HYPREAlgorithm"><code>LinearSolve.HYPREAlgorithm</code></a> — <span class="docstring-category">Type</span></summary><section><div><p><code>HYPREAlgorithm(solver; Pl = nothing)</code></p><p><a href="https://github.com/fredrikekre/HYPRE.jl">HYPRE.jl</a> is an interface to <a href="https://computing.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods"><code>hypre</code></a> and provide iterative solvers and preconditioners for sparse linear systems. It is mainly developed for large multi-process distributed problems (using MPI), but can also be used for single-process problems with Julias standard sparse matrices.</p><p>If you need more fine-grained control over the solver/preconditioner options you can alternatively pass an already created solver to <code>HYPREAlgorithm</code> (and to the <code>Pl</code> keyword argument). See HYPRE.jl docs for how to set up solvers with specific options.</p><div class="admonition is-info" id="Note-94127fba32559141"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-94127fba32559141" title="Permalink"></a></header><div class="admonition-body"><p>Using HYPRE solvers requires Julia version 1.9 or higher, and that the package HYPRE.jl is installed.</p></div></div><p><strong>Positional Arguments</strong></p><p>The single positional argument <code>solver</code> has the following choices:</p><ul><li><code>HYPRE.BiCGSTAB</code></li><li><code>HYPRE.BoomerAMG</code></li><li><code>HYPRE.FlexGMRES</code></li><li><code>HYPRE.GMRES</code></li><li><code>HYPRE.Hybrid</code></li><li><code>HYPRE.ILU</code></li><li><code>HYPRE.ParaSails</code> (as preconditioner only)</li><li><code>HYPRE.PCG</code></li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>Pl</code>: A choice of left preconditioner.</li></ul><p><strong>Example</strong></p><p>For example, to use <code>HYPRE.PCG</code> as the solver, with <code>HYPRE.BoomerAMG</code> as the preconditioner, the algorithm should be defined as follows:</p><pre><code class="language-julia hljs">A, b = setup_system(...)
prob = LinearProblem(A, b)
alg = HYPREAlgorithm(HYPRE.PCG)
prec = HYPRE.BoomerAMG
sol = solve(prob, alg; Pl = prec)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/LinearSolve.jl/blob/73c9a531bcd7870514fecbabaa50759edc8b1a06/src/extension_algs.jl#L61-L108">source</a></section></details></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../basics/FAQ/">« Frequently Asked Questions</a><a class="docs-footer-nextpage" href="../../advanced/developing/">Developing New Linear Solvers »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Thursday 19 February 2026 20:20">Thursday 19 February 2026</span>. Using Julia version 1.12.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
